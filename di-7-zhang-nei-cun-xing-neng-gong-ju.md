# 第7章：内存性能工具

各种各样的工具可用来帮助程序员了解程序的缓存和内存使用。现代处理器具有可以使用的性能监视硬件。有些事件很难准确衡量，因此也有进行模拟的空间。当涉及到更高级别的功能时，有一些特殊的工具可以监视流程的执行。我们将介绍大多数Linux系统上可用的一组常用工具。

## 7.1 内存操作分析

对内存操作进行性能分析需要硬件的协作。可以单独在软件中收集一些信息，但这要么是粗粒度的，要么仅仅是模拟。模拟示例将在第7.2节和第7.5节中显示。在这里，我们将专注于可测量的记忆效果。

[oprofile](http://oprofile.sourceforge.net/) 提供了对Linux上的性能监视硬件的访问 [。](http://oprofile.sourceforge.net/) Oprofile提供了连续分析功能，如\[continuous\]中首先描述的；它使用易于使用的界面在整个系统范围内进行统计分析。Oprofile绝不是唯一可以使用处理器性能评估功能的方法；Linux开发人员正在研究 [pfmon](http://www.hpl.hp.com/research/linux/perfmon/pfmon.php4) ，在某个时候它可能已经被广泛部署以保证在此也有描述。

oprofile提供的界面既简单又最小，即使使用了可选的GUI，也相当底层。用户必须在处理器可以记录的事件中进行选择。处理器的体系结构手册描述了事件，但是通常，它需要有关处理器本身的大量知识才能解释数据。另一个问题是所收集数据的解释。性能测量计数器是绝对值，可以任意增长。给定计数器的价格有多高？

此问题的部分答案是避免查看绝对值，而是将多个计数器彼此关联。处理器可以监视多个事件；然后可以检查所收集的绝对值的比率。这给出了不错的可比结果。除数通常是处理时间，时钟周期数或指令数的量度。作为程序性能的初步尝试，仅将这两个数字相互关联是很有用的。

> ![](https://static.lwn.net/images/cpumemory/cpumemory.45.png)
>
> **图7.1：每条指令的周期（随机）**

图7.1显示了针对各种工作集大小的简单随机“跟随”测试用例的每条指令的周期（CPI）。为大多数英特尔处理器收集该信息的事件的名称为 CPU\_CLK\_UNHALTED和INST\_RETIRED。顾名思义，前者对CPU的时钟周期进行计数，而后者则对指令数进行计数。我们看到的图片类似于我们使用的每个列表元素测量的周期。对于小尺寸的工作台，该比率为1.0甚至更低。这些测量是在Intel Core 2处理器上进行的，该处理器是多标量的，可以一次处理多个指令。对于不受内存带宽限制的程序，该比率可以显着低于1.0，但是在这种情况下，1.0非常好。

一旦L1d的大小不再足以容纳工作集，CPI就会跳到3.0以下。请注意，CPI比率是对所有指令（不只是存储器指令）的访问L2的平均惩罚。使用列表元素数据的周期，可以算出每个列表元素需要多少条指令。如果甚至L2缓存不足，CPI比率也会跳到20以上。这是预期的结果。

但是性能测量计数器应该可以提供更多有关处理器中发生的情况的见解。为此，我们需要考虑处理器的实现。在本文档中，我们关注缓存处理的详细信息，因此我们必须查看与缓存有关的事件。这些事件，它们的名称以及它们的计数是特定于处理器的。无论使用哪种简单的用户界面，oprofile都是当前很难使用的地方：用户必须自己确定性能计数器的详细信息。在第10节中，我们将看到有关某些处理器的详细信息。

对于Core 2处理器，要查找的事件是L1D\_REPL， DTLB\_MISSES和L2\_LINES\_IN。后者可以测量所有未命中和由指令引起的未命中，而不是硬件预取。随机“跟随”测试的结果如图7.2所示。

> ![](https://static.lwn.net/images/cpumemory/cpumemory.44.png)
>
> **图7.2：测得的高速缓存未命中（跟随随机）**

所有比率均使用退休指令数（INST\_RETIRED）计算。这意味着还对未触及存储器的指令进行了计数，这反过来意味着触及存储器且遭受高速缓存未命中的指令的数量甚至比图中所示的还要高。

L1d丢失比其他所有错误都高，因为对于英特尔处理器，L2d丢失意味着由于使用了包含性缓存而导致L1d丢失。处理器具有32k的L1d，因此，正如我们所预期的，L1d速率在工作集大小附近从零上升（列表数据结构旁边还有高速缓存的其他用途，这意味着增量在16k和10k之间发生）。 32k标记）。有趣的是，对于不超过64k的工作集，硬件预取可以将未命中率保持在1％。之后，L1d率猛增。

L2丢失率保持为零，直到L2耗尽为止。由于L2的其他用途而造成的一些遗漏对数字的影响不大。一旦超过L2的大小（2 21字节），未命中率就会上升。重要的是要注意，L2需求未命中率不为零。这表明硬件预取器不会稍后加载指令所需要的所有高速缓存行。这是预料之中的，访问的随机性会阻止完美的预取。将此数据与图7.3中顺序读取的数据进行比较。

> ![](https://static.lwn.net/images/cpumemory/cpumemory.46.png)
>
> **图7.3：测得的高速缓存未命中（顺序）**

在此图中，我们可以看到L2需求缺失率基本上为零（请注意，此图的比例与图7.2不同）。对于顺序访问的情况，硬件预取器可以完美地工作：几乎所有的L2高速缓存未命中都是由预取器引起的。L1d和L2丢失率相同的事实表明，所有L1d高速缓存未命中都由L2高速缓存处理，没有进一步的延迟。对于所有程序来说，这都是理想的情况，但是，这几乎是不可能实现的。

在两个图中的第四行是DTLB丢失率（Intel对代码和数据有单独的TLB，DTLB是数据TLB）。对于随机访问的情况，DTLB丢失率很高，并且会导致延迟。有趣的是，DTLB罚球在L2未命中之前设定。对于顺序访问的情况，DTLB成本基本上为零。

回到6.2.1节中的矩阵乘法示例和9.1节中的示例代码，我们可以使用另外三个计数器。该SSE\_PRE\_MISS，SSE\_PRE\_EXEC和 LOAD\_HIT\_PRE计数器可以用来查看软件预取的效果如何。如果运行第9.1节中的代码，我们将得到以下结果：

> | 描述 | 比率 |
> | :--- | :--- |
> | 有用的NTA预取 | 2.84％ |
> | 晚期NTA预取 | 2.65％ |

有用的NTA（非时间对齐）预取比率低表示已为已加载的高速缓存行执行了许多预取指令，因此不需要任何工作。这意味着处理器浪费时间解码预取指令并查找缓存。但是，人们不能太苛刻地判断代码。在很大程度上取决于所使用的处理器的缓存大小。硬件预取器也起作用。

较低的NTA预取比率很容易引起误解。该比率意味着所有预取指令的2.65％发出得太晚了。在可以将数据预取到高速缓存之前，执行需要数据的指令。必须记住，只有2.84％+ 2.65％= 5.5％的预取指令有用。在有用的NTA预取指令中，有48％未能及时完成。因此，可以进一步优化代码：

* * 不需要大多数预取指令。
* 可以调整预取指令的使用以更好地匹配硬件。

留给读者练习是确定可用硬件的最佳解决方案。确切的硬件规格起着重要作用。在Core 2处理器上，SSE算术运算的等待时间为1个周期。较旧的版本具有2个周期的延迟，这意味着硬件预取器和预取指令有更多的时间来引入数据。

要确定可能需要（或不需要）预取的位置，可以使用opannotate程序。它列出了程序的源代码或汇编代码，并显示了识别事件的指令。请注意，模糊性有两个来源：

1. 2. Oprofile执行随机分析。仅记录每个第N个事件（其中N是每个事件的阈值，具有强制的最小值），以避免过多地减慢系统的运行速度。可能有几行会导致100个事件，但它们可能不会显示在报告中。
3. 并非所有事件都被准确记录。例如，记录特定事件时的指令计数器可能不正确。处理器是多标量的，因此很难给出100％正确的答案。不过，某些处理器上的一些事件是准确的。

带注释的清单对确定预取信息不仅仅有用。每个事件都用指令指针记录。因此，还可以在程序中查明其他热点。作为许多INST\_RETIRED事件源的位置 经常执行，应该进行调整。报告许多高速缓存未命中的位置可能需要预取指令，以避免高速缓存未命中。

页面错误是一种无需硬件支持即可测量的事件。操作系统负责解决页面错误，在这种情况下，它也将其计算在内。它区分两种页面错误：

**小页面错误**这些是迄今为止尚未使用的匿名（即，没有文件支持的）页面，写时复制页面以及内容已在内存中某个位置的其他页面的页面错误。

**主要页面错误**解决它们需要访问磁盘以检索文件支持（或换出）的数据。

显然，主要页面错误要比次要页面错误贵得多。但是后者也不便宜。无论哪种情况，都必须进入内核，必须找到一个新页面，必须用适当的数据清除或填充该页面，并且必须相应地修改页面表树。最后一步需要与读取或修改页表树的其他任务同步，这可能会导致进一步的延迟。

检索有关页面错误计数信息的最简单方法是使用时间工具。注意：使用真实的工具，而不是内置的shell。输出如图7.4所示。{反斜杠会阻止使用内置命令。}

> ```text
> $ \ time ls / etc
> [...]
> 0.00user 0.00系统0：00.02经过17％CPU（0avgtext + 0avgdata 0maxresident）k
> 0inputs + 0outputs（1major + 335minor）页面错误0swaps
> ```
>
> **图7.4：时间实用程序的输出**

有趣的部分是最后一行。时间工具报告一个主要页面错误和335个次要页面错误。确切数字各不相同；特别是，立即重复运行可能会表明现在根本没有重大页面错误。如果程序执行相同的操作，而环境没有任何变化，则总的页面错误计数将保持稳定。

关于页面错误的一个特别敏感的阶段是程序启动。使用的每一页都会产生页面错误；可见的效果（尤其是对于GUI应用程序）是使用的页面越多，程序开始工作所需的时间就越长。在7.5节中，我们将看到一种专门测量此效果的工具。

在后台，时间工具使用了rusage功能。该 wait4系统调用在填充结构rusage对象时，对孩子的父母等待终止; 这正是时间工具所需要的。但是，进程也可以请求有关其自身资源使用情况（即名称 rusage的来源）或其终止子级的资源使用情况的信息。

> ```text
> #include <sys / resource.h>
> int getrusage（__ rusage_who_t who，struct rusage * usage）
> ```

该谁参数指定处理信息的请求。当前， 已定义RUSAGE\_SELF和RUSAGE\_CHILDREN。每个子进程终止时，将累积子进程的资源使用情况。它是一个总值，而不是单个子进程的使用量。存在允许请求特定于线程的信息的建议，因此在不久的将来我们可能会看到RUSAGE\_THREAD。该rusage结构被定义为包含各种指标，包括执行时间的，用于发送IPC消息和存储器的数量，和页面错误的数量。后面的信息在ru\_minflt和ru\_majflt中可用 结构的成员。

试图确定其程序由于页面错误而在哪里失去性能的程序员可以定期请求信息，然后将返回的值与以前的结果进行比较。

如果请求者具有必要的特权，则从外部也可以看到该信息。伪文件/ proc / &lt;PID&gt; / stat（其中 &lt;PID&gt;是我们感兴趣的进程的进程ID）在第十至第十四字段中包含页面错误编号。它们分别是进程及其子级累积的次要和主要页面错误的对。

## 7.2 模拟CPU缓存

尽管对缓存如何工作的技术描述相对容易理解，但要查看实际程序相对于缓存的行为却并非那么容易。程序员并不直接关心地址的值，无论它们是绝对值还是相对值。地址部分由链接器确定，部分在运行时由动态链接器和内核确定。预期生成的汇编代码将与所有可能的地址一起使用，并且在源语言中，甚至没有暗示绝对地址值的提示。因此，很难理解程序如何利用内存。{当编程接近于硬件时，这可能会有所不同，但这与普通编程无关，无论如何，它仅适用于特殊地址，例如内存映射设备。}

CPU级别的分析工具（例如oprofile）（如7.1节所述）可以帮助您了解缓存的使用。结果数据与实际硬件相对应，如果不需要细粒度的收集，则可以相对快速地收集数据。一旦需要更多细粒度的数据，oprofile就不再可用。该线程将不得不被频繁中断。此外，要查看程序在不同处理器上的存储行为，实际上必须拥有这样的机器并在它们上执行程序。有时（通常）是不可能的。一个例子是图3.8中的数据。要使用oprofile收集此类数据，必须拥有24台不同的机器，其中许多机器不存在。

该图中的数据是使用缓存模拟器收集的。该程序cachegrind使用valgrind框架，该框架最初是为检查程序中与内存处理相关的问题而开发的。valgrind框架模拟程序的执行，并且在执行此操作时，它允许各种扩展（例如cachegrind）挂接到执行框架中。cachegrind工具使用它来拦截所有对内存地址的使用。然后，它以给定的大小，高速缓存行大小和关联性来模拟L1i，L1d和L2高速缓存的操作。

要使用该工具，必须使用valgrind作为包装程序运行程序：

> ```text
> valgrind --tool = cachegrind命令arg
> ```

以这种最简单的形式，使用参数arg执行程序命令，同时使用与其运行的处理器相对应的大小和关联性来模拟三个高速缓存。程序运行时，输出的一部分被打印为标准错误。它由缓存总使用量的统计信息组成，如图7.5所示。

> ```text
> == 19645 ==我参考：152,653,497
> == 19645 == I1失误：25,833
> == 19645 == L2i未击中：2,475
> == 19645 == I1遗漏率：0.01％
> == 19645 == L2i丢失率：0.00％
> == 19645 ==
> == 19645 == D refs：56,857,129（35,838,721 rd + 21,018,408 Wr）
> == 19645 == D1失误：14,187（12,451 rd + 1,736 wr）
> == 19645 == L2d未中：7,701（6,325 rd + 1,376 wr）
> == 19645 == D1失误率：0.0％（0.0％+ 0.0％）
> == 19645 == L2d丢失率：0.0％（0.0％+ 0.0％）
> == 19645 ==
> == 19645 == L2裁判：40,020（38,284 rd + 1,736 wr）
> == 19645 == L2失误：10,176（8,800 rd + 1,376 wr）
> == 19645 == L2丢失率：0.0％（0.0％+ 0.0％）
> ```
>
> **图7.5：Cachegrind摘要输出**

给出了指令和内存引用的总数，以及它们为L1i / L1d和L2高速缓存产生的未命中次数，未命中率等。该工具甚至能够将L2访问分为指令和数据访问，并且所有数据缓存的使用都分为读写访问权限。

当更改模拟缓存的详细信息并比较结果时，这将变得更加有趣。通过使用的-I1， -D1和-L2参数，cachegrind可以指示无视处理器的高速缓存布局和使用该命令行上指定。例如：

> ```text
>   valgrind --tool = cachegrind --L2 = 8388608,8,64命令arg
> ```

将模拟具有8路设置关联性和64字节高速缓存行大小的8MB L2高速缓存。请注意，“- L2”选项出现在命令行中要模拟的程序的名称之前。

这不是所有cachegrind都能做到的。在进程退出之前，cachegrind会写出一个名为cachegrind.out.XXXXX的文件，其中 XXXXX是进程的PID。该文件包含每个功能和源文件中有关高速缓存使用的摘要信息和详细信息。可以使用cg\_annotate程序查看数据。

该程序产生的输出包含在进程终止时打印的缓存使用摘要，以及该程序每个功能中缓存行使用的详细摘要。生成此按功能的数据要求cg\_annotate能够将地址与功能进行匹配。这意味着调试信息应该可用以获得最佳结果。如果不这样做，ELF符号表可能会有所帮助，但是由于内部符号未在动态符号表中列出，因此结果不完整。图7.6显示了与图7.5相同的程序运行的部分输出。

```text
-------------------------------------------------- ------------------------------
        Ir I1mr I2mr Dr D1mr D2mr Dw D1mw D2mw文件：功能
-------------------------------------------------- ------------------------------
53,684,905 9 8 9,589,531 13 3 5,820,373 14 0 ???：_ IO_file_xsputn @@ GLIBC_2.2.5
36,925,729 6,267 114 11,205,241 74 18 7,123,370 22 0 ???：vfprintf
11,845,373 22 2 3,126,914 46 22 1,563,457 0 0 ???：__ find_specmb
 6,004,482 40 10 697,872 1,744 484 0 0 0 ???：strlen
 5,008,448 3 2 1,450,093 370 118 0 0 0 ???：strcmp
 3,316,589 24 4 757,523 0 0 540,952 0 0 ???：_ IO_padn
 2,825,541 3 3 290,222 5 1 216,403 0 0 ???：_ itoa_word
 2,628,466 9 6 730,059 0 0 358,215 0 0 ???：_ IO_file_overflow @@ GLIBC_2.2.5
 2,504,211 4 4 762,151 2 0 598,833 3 0 ???：_ IO_do_write @@ GLIBC_2.2.5
 2,296,142 32 7 616,490 88 0 321,848 0 0 dwarf_child.c：__ libdw_find_attr
 2,184,153 2,876 20 503,805 67 0 435,562 0 0 ???：__ dcigettext
 2,014,243 3 3 435,512 1 1 272,195 4 0 ???：_ IO_file_write @@ GLIBC_2.2.5
 1,988,697 2,804 4 656,112 380 0 47,847 1 1 ???：getenv
 1,973,463 27 6 597,768 15 0 420,805 0 0 dwarf_getattrs.c：dwarf_getattrs
```

**图7.6：cg\_annotate输出**

Ir，Dr和Dw列显示了总的缓存使用量，而不是缓存未命中，这在以下两列中显示。该数据可用于识别产生最多高速缓存未命中的代码。首先，人们可能会专注于L2高速缓存未命中，然后继续优化L1i / L1d高速缓存未命中。

cg\_annotate可以更详细地提供数据。如果给出了源文件的名称，它还会用与该行相对应的高速缓存命中和未命中次数来注释（因此为程序名）源文件的每一行。该信息使程序员可以深入到高速缓存未命中问题所在的确切行。程序接口有点原始：截至撰写本文时，cachegrind数据文件和源文件必须位于同一目录中。

在这一点上，应该再次指出：cachegrind是一个模拟器，它不使用来自处理器的测量结果。处理器中的实际缓存实现可能完全不同。cachegrind模拟最近最少使用（LRU）驱逐，这对于具有较大关联性的高速缓存而言可能太昂贵了。此外，该模拟未考虑上下文切换和系统调用，这两者都可能破坏L2的大部分，并且必须刷新L1i和L1d。这导致高速缓存未中的总数低于实际情况。尽管如此，cachegrind是一个很好的工具，可以了解程序的内存使用情况及其内存问题。

## 7.3 测量内存使用率

知道程序分配了多少内存以及分配可能在哪里进行是优化其内存使用的第一步。幸运的是，有一些易于使用的程序，它们甚至不需要重新编译或专门修改程序。

对于第一个称为massif的工具，不剥离编译器可以自动生成的调试信息就足够了。它概述了一段时间内累积的内存使用情况。图7.7显示了生成的输出示例。

> ![](https://static.lwn.net/images/cpumemory/massif.png)
>
> **图7.7：Massif输出**

与cachegrind（第7.2节）一样，massif是使用valgrind基础结构的工具。开始使用

```text
      valgrind --tool = massif命令arg
```

其中命令arg是要观察的程序及其参数，将模拟该程序并识别对内存分配函数的所有调用。记录呼叫站点以及时间戳记值；新的分配大小将添加到整个程序总数和特定呼叫站点的总数中。这同样适用于释放内存的功能，其中显然从适当的总和中减去已释放块的大小。然后可以使用此信息来创建一个图表，显示程序在整个生命周期内的内存使用情况，并根据请求分配的位置划分每个时间值。在该过程终止之前，massif创建两个文件：massif.XXXXX.txt和 massif.XXXXX.ps，其中两种情况下的XXXXX均为流程的PID。该.TXT文件是内存使用的所有调用点和汇总.PS是什么可以在图7.7中可以看出。

Massif还可以记录程序的堆栈使用情况，这对于确定应用程序的总内存占用量很有用。但这并不总是可能的。在某些情况下（某些线程堆栈或使用signaltstack时 ），valgrind运行时无法了解堆栈的限制。在这些情况下，将这些堆栈的大小加到总计中也没有多大意义。在其他几种情况下，这毫无意义。如果程序受此影响，则应该使用附加选项-stacks = no来启动massif。注意，这是valgrind的一个选项，因此必须在所观察程序的名称之前。

某些程序围绕系统的分配功能提供自己的内存分配功能或包装器功能。在第一种情况下，通常会错过分配；在第二种情况下，已记录的呼叫站点会隐藏信息，因为只有包装功能中的呼叫地址被记录。因此，可以将其他功能添加到分配功能列表中。所述 -alloc-FN =的xmalloc参数将指定的函数 的xmalloc也是分配功能，这通常是在GNU程序的情况。记录了对xmalloc的调用，但没有记录从xmalloc内部进行的分配调用。

第二个工具称为记忆。它是GNU C库的一部分。它是地块的简化版本（但存在于地块之前很长时间）。它仅记录堆的总内存使用情况（如果给定了-m选项，则包括对mmap的可能调用等），以及可选的堆栈。结果可以显示为一段时间内总内存使用量的图表，或者线性地显示为对分配函数的调用。这些图形是由memusage脚本分别创建的，该脚本与valgrind一样，必须用于启动应用程序：

```text
     记忆命令arg
```

该-p IMGFILE选项必须用于指定图形应在文件中生成IMGFILE，这将是一个PNG文件。收集数据的代码在实际程序本身中运行，而不是像valgrind这样的模拟。这意味着游乐比地块快得多，并且在地块无用的情况下可用。除了总的内存消耗外，该代码还记录了分配大小，并且在程序终止时，它还显示了所用分配大小的直方图。此信息被写入标准错误。

有时无法（或不可行）调用应该直接观察的程序。一个示例是gcc的编译器阶段，该阶段由gcc驱动程序启动。在这种情况下，必须使用-n NAME参数将应注意的程序名称提供给memusage脚本。如果观察到的程序启动其他程序，则此参数也很有用。如果未指定程序名称，则将对所有启动的程序进行概要分析。

地块和记忆这两个程序都有其他选项。程序员发现自己需要更多的功能时，应首先查阅手册或帮助消息，以确保尚未实现其他功能。

既然我们知道如何捕获有关内存分配的数据，那么有必要讨论如何在内存和缓存使用的上下文中解释此数据。有效动态内存分配的主要方面是线性分配和所用部分的紧凑性。这可以提高预取的效率并减少缓存丢失。

必须读取任意数量的数据以供以后处理的程序可以通过创建一个列表来实现此目的，其中每个列表元素都包含一个新的数据项。这种分配方法的开销可能很小（单链接列表一个指针），但是使用数据时的缓存效果会大大降低性能。

一个问题是，例如，不能保证顺序分配的存储器在存储器中顺序布置。造成这种情况的原因可能有很多：

* * 实际上，由内存分配器管理的大内存块中的内存块实际上是从后往前返回的；
* 内存块已耗尽，并且在地址空间的不同部分开始了新的内存块；
* 分配请求是针对不同大小的，它们是由不同的内存池提供的；
* 多线程程序的各个线程中分配的交织。

如果必须预先分配数据以进行后续处理，则链表方法显然不是一个好主意。不能保证（甚至不可能）列表中的连续元素在内存中连续布置。为确保连续分配，不得以小块分配该内存。必须使用另一层内存处理。它可以很容易地由程序员实现。一种替代方法是使用GNU C库中提供的obstack实现。该分配器向系统的分配器请求大块内存，然后将任意大块或小块内存移出。除非大内存块已用完，否则这些分配始终是顺序的，这取决于请求的分配大小，这非常少见。堆栈并不是内存分配器的完整替代品，它们释放物体的能力有限。有关详细信息，请参见GNU C库手册。

因此，如何从图中识别出建议使用堆栈（或类似技术）的情况？如果不咨询来源，就无法确定可能的更改候选者，但是图形可以为搜索提供入口点。如果从同一位置进行许多分配，这可能意味着批量分配可能会有所帮助。在图7.7中，我们可以在地址0x4c0e7d5的分配中看到这样一个可能的候选对象。从运行的大约800毫秒到运行的1800毫秒，这是唯一增长的区域（顶部，绿色区域除外）。此外，斜率并不陡峭，这意味着我们有大量相对较小的分配。确实，这是使用堆栈或类似技术的候选人。

这些图可能显示的另一个问题是分配总数高时。如果图形不是随时间线性地绘制，而是随调用次数线性地绘制（带有memusage的默认值），则特别容易看到。在这种情况下，图中的平缓斜率意味着很多小的分配。memusage不会说分配发生在哪里，但是与massif的输出进行比较可以说明这一点，否则程序员可能会立即意识到这一点。应该合并许多小的分配，以实现线性内存的使用。

但是，对于后一种情况，还有另一个同样重要的方面：许多分配也意味着更高的管理数据开销。就其本身而言，这可能不是问题。名为“ heap-admin”的红色区域在地块图中表示此开销，并且非常小。但是，根据malloc的实现，此管理数据与数据块一起分配在同一内存中。对于GNU C库中的当前malloc实现，情况就是这样：每个分配的块至少具有一个2字头（32位平台为8个字节，64位平台为16个字节）。另外，由于管理内存的方式，块大小通常会比所需的大小大一些（将块大小四舍五入为特定的倍数）。

所有这些都意味着程序使用的内存与仅分配器用于管理目的的内存散布了。我们可能会看到以下内容：

> ![](https://static.lwn.net/images/cpumemory/cpumemory.39.png)

每个块代表一个存储字，在这个很小的存储器区域中，我们有四个分配的块。由于块头和填充造成的开销为50％。由于头文件的放置，这自动意味着处理器的有效预取率也将降低多达50％。如果对这些块进行了顺序处理（以充分利用预取的优势），则处理器将把所有标头和填充字读入高速缓存中，即使它们从来不应该被应用程序本身读取或写入。只有运行时使用标题字，并且运行时仅在释放该块时才起作用。

现在，有人可能会争辩说，应该更改实现以将管理数据放到其他地方。实际上，这是在某些实现中完成的，可能被证明是一个好主意。但是，有许多方面需要牢记，安全并不是其中最重要的方面。无论将来是否会发生变化，填充问题都将永远不会消失（在忽略标题的情况下，示例中的数据占16％）。只有程序员直接控制分配，才能避免这种情况。当对齐要求发挥作用时，可能仍然会有漏洞，但这也是程序员控制下的事情。

## 7.4 改进分支预测

在6.2.2节中，提到了两种通过分支预测和块重新排序来提高L1i使用率的方法：通过\_\_builtin\_expect进行的静态预测和配置文件引导的优化（PGO）。正确的分支预测会影响性能，但是在这里我们对内存使用率的改进很感兴趣。

采用\_\_builtin\_expect（或更好的可能和 不可能宏）很简单。这些定义放在中央头文件中，编译器负责其余的工作。有一个小问题，虽然：它很容易足以让一个程序员使用 可能时真的不太可能是意味着，反之亦然。即使有人使用oprofile之类的工具来测量不正确的分支预测并且L1i遗漏，这些问题也很难发现。

不过，有一种简单的方法。9.2节中的代码显示了可能和不太可能的宏的替代定义，这些宏在运行时会主动测量静态预测是否正确。然后程序员或测试人员可以检查结果并进行调整。测量实际上并没有考虑程序的性能，它们只是测试程序员的静态假设。在上面引用的部分中，可以找到更多详细信息以及代码。

如今，PGO与gcc一起使用非常容易。但是，这是一个三步过程，必须满足某些要求。首先，必须使用附加的-fprofile-generate选项编译所有源文件 。必须将此选项传递给所有编译器运行和链接程序的命令。可以混合使用和不使用此选项编译的目标文件，但对于未启用此功能的对象，PGO将无济于事。

编译器会生成一个正常运行的二进制文件，但它会变得更大，更慢，因为它会记录（并发出）有关是否采用分支的所有信息。编译器还会为每个输入文件发出一个扩展名为.gcno的文件。该文件包含与代码中的分支有关的信息。必须保留以备后用。

一旦程序二进制文件可用，就应使用它来运行一组代表性的工作负载。无论使用哪种工作负载，最终二进制文件都将得到优化以很好地完成此任务。该程序可以连续运行，通常是必需的；所有运行将贡献到相同的输出文件。在程序终止之前，将在程序运行期间收集的数据写出到扩展名为.gcda的文件中。这些文件在包含源文件的目录中创建。可以从任何目录执行该程序，并且可以复制二进制文件，但是带有源的目录必须是可写的。同样，为每个输入源文件创建一个输出文件。如果程序多次运行，那么.gcda很重要可以在源目录中找到上一次运行的文件，因为否则运行数据不能累积在一个文件中。

运行了一组有代表性的测试后，就该重新编译该应用程序了。编译器必须能够在与源文件相同的目录中找到 .gcda文件。无法移动文件，因为编译器找不到它们，并且文件的嵌入式校验和不再匹配。对于重新编译，请将-fprofile-generate参数替换为 -fprofile-use。至关重要的是，源代码不得以任何会更改生成代码的方式进行更改。这意味着：可以更改空白和编辑注释，但是添加更多分支或基本块会使收集的数据无效，并且编译将失败。

这是程序员要做的所有事情。这是一个相当简单的过程。正确的最重要的事情是选择代表性的测试来执行测量。如果测试工作负载与程序的实际使用方式不匹配，则执行的优化实际上弊大于利。因此，将PGO用于库通常很困难。图书馆可用于许多（有时相差很大）场景中。除非用例确实相似，否则通常最好只使用\_\_builtin\_expect进行静态分支预测。

.gcno和.gcda文件中的 几句话。这些是二进制文件，不能立即用于检查。但是，也可以使用gcov工具（它也是gcc软件包的一部分）来检查它们。该工具主要用于覆盖率分析（因此得名），但使用的文件格式与PGO相同。gcov工具会为每个带有已执行代码的源文件生成扩展名为.gcov的输出文件（可能包括系统头文件）。这些文件是源列表，根据给gcov的参数进行注释，并带有分支计数器，概率等。

## 7.5 页面错误优化

在按需分页的操作系统（如Linux）上， mmap调用只能修改页表。它确保对于文件支持的页面，可以找到基础数据，对于匿名内存，可以确保在访问时提供以零初始化的页面。调用mmap时未分配实际内存。{如果您想说“错！” 稍等片刻，以后会有异常的资格。}

当通过读取或写入数据或通过执行代码首次访问存储页面时，发生分配部分。响应随之而来的页面错误，内核控制并使用页面表树确定必须在页面上显示的数据。解决页面错误的方法并不便宜，但是对于进程使用的每一个页面，它都会发生。

为了最大程度地减少页面错误的成本，必须减少已使用页面的总数。优化代码的大小将对此有所帮助。为了降低特定代码路径（例如，启动代码）的成本，还可以重新排列代码，以便在该代码路径中将触摸页面的数量最小化。但是，确定正确的顺序并不容易。

作者基于valgrind工具集编写了一个工具，用于在页面错误发生时对其进行测量。不是页面错误的数量，而是它们发生的原因。该 [页面调](http://people.redhat.com/drepper/pagein.html)工具发出关于页面错误的顺序和时间信息。写入到名为pagein。&lt;PID&gt;的文件中的输出如图7.8所示。

> ```text
>    0 0x3000000000 C 0 0x3000000B50：（在/lib64/ld-2.5.so内）
>    1 0x 7FF000000 D 3320 0x3000000B53：（在/lib64/ld-2.5.so中）
>    2 0x3000001000 C 58270 0x3000001080：_dl_start（在/lib64/ld-2.5.so中）
>    3 0x3000219000 D 128020 0x30000010AE：_dl_start（在/lib64/ld-2.5.so中）
>    4 0x300021A000 D 132170 0x30000010B5：_dl_start（在/lib64/ld-2.5.so中）
>    5 0x3000008000 C 10489930 0x3000008B20：_dl_setup_hash（在/lib64/ld-2.5.so中）
>    6 0x3000012000 C 13880830 0x3000012CC0：_dl_sysdep_start（在/lib64/ld-2.5.so中）
>    7 0x3000013000 C 18091130 0x3000013440：brk（在/lib64/ld-2.5.so中）
>    8 0x3000014000 C 19123850 0x3000014020：strlen（在/lib64/ld-2.5.so中）
>    9 0x3000002000 C 23772480 0x3000002450：dl_main（在/lib64/ld-2.5.so中）
> ```
>
> **图7.8：pagein工具的输出**

第二列指定被分页的页面的地址。在第三列中指出是代码页还是数据页，分别包含“ C”或“ D”。第四列指定自第一页错误以来经过的循环数。该行的其余部分是valgrind尝试查找导致页面错误的地址的名称。地址值本身是正确的，但是如果没有可用的调试信息，则名称并不总是准确的。

在图7.8的示例中，执行从地址0x3000000B50开始，这迫使地址0x3000000000的页面被分页。在该页面上调用的函数是\_dl\_start。初始代码访问页面0x7FF000000上的变量。这在第一页错误之后仅发生3,320个周期，并且很可能是程序的第二条指令（仅在第一条指令之后的三个字节）。如果人们看了一下程序，就会注意到这种内存访问有些特殊之处。有问题的指令是一个电话指令，它不会显式加载或存储数据。但是，它确实将返回地址存储在堆栈中，而这恰恰是在这里发生的情况。这不是进程的正式堆栈，但是，它是valgrind的应用程序内部堆栈。这意味着在解释pagein的结果时，请务必牢记valgrind会引入一些工件。

Pagein的输出可用于确定哪些代码序列在程序代码中理想情况下应该相邻。快速浏览 /lib64/ld-2.5.so代码，您会发现第一条指令立即调用了函数\_dl\_start，并且这两个位置位于不同的页面上。重新布置代码以将代码序列移至同一页面上可以避免（或至少可以延迟）页面错误。到目前为止，确定最佳代码布局应是繁琐的过程。由于设计上没有记录页面的第二次使用，因此需要反复试验才能看到更改的效果。使用调用图分析，可以猜测可能的调用序列；这可能有助于加快对函数和变量进行排序的过程。

在非常粗糙的级别上，可以通过查看构成可执行文件或DSO的目标文件来查看调用序列。从一个或多个入口点（即函数名称）开始，可以计算依赖关系链。无需付出太多努力，这在目标文件级别就可以很好地工作。在每一轮中，确定哪些目标文件包含所需的函数和变量。必须明确指定种子集。然后确定那些目标文件中的所有未定义引用，并将它们添加到所需符号集中。重复直到设置稳定。

该过程的第二步是确定订单。必须将各种目标文件组合在一起以填充尽可能少的页面。另外，没有功能可以跨越页面边界。所有这一切的麻烦之处在于，为了最好地排列目标文件，必须知道链接器以后将要执行的操作。此处的重要事实是，链接器会将对象文件以它们在输入文件（例如，归档文件）和命令行中出现的顺序放入可执行文件或DSO中。这给了程序员足够的控制权。

对于那些愿意花更多时间的人，使用-finstrument-functions选项\[oooreorder\]进行调用时，通过\_\_cyg\_profile\_func\_enter和 \_\_cyg\_profile\_func\_exit钩子gcc插入，使用自动调用跟踪进行了成功的重新排序尝试。有关这些\_\_cyg\_ \*的更多信息，请参见gcc手册。 接口。通过创建程序执行的跟踪，程序员可以更准确地确定调用链。仅通过对功能进行重新排序，\[oooreorder\]中的结果将启动成本降低了5％。主要好处是减少了页面错误的数量，但是TLB缓存也发挥了作用-越来越重要，因为在虚拟化环境中，TLB丢失的代价大大增加。

通过将pagein工具的分析与调用序列信息结合起来，应该有可能优化程序的某些阶段（例如启动），以最大程度地减少页面错误的数量。

Linux内核提供了两种附加的机制来避免页面错误。第一个是mmap的标志，它指示内核不仅修改页表，而且实际上还对映射区域中的所有页进行预故障处理。这是通过简单地将MAP\_POPULATE标志添加 到mmap 调用的第四个参数来实现的。这将导致mmap调用的成本大大增加，但是，如果立即使用该调用映射的所有页面，则好处可能很大。该程序将不具有多个页面错误，而这些页面错误由于同步需求等导致的开销而非常昂贵，因此该程序将具有一个更昂贵的mmap称呼。但是，在呼叫后不久（或从未）使用大量映射页面的情况下，使用此标志有缺点。映射的未使用页面显然浪费时间和内存。立即进行故障处理且仅在以后大量使用的页面也会阻塞系统。内存是在使用前分配的，这可能会导致同时内存不足。另一方面，在最坏的情况下，页面只是出于新的目的而被重新使用（因为它尚未被修改），它虽然不那么昂贵，但仍然与分配一起增加了一些成本。

MAP\_POPULATE 的粒度太粗糙了。还有第二个可能的问题：这是优化。实际上，映射所有页面并不重要。如果系统太忙而无法执行操作，则可以删除预故障。一旦真正使用了页面，程序就会出现页面错误，但这并不比人为地造成资源短缺更糟糕。另一种方法是将 POSIX\_MADV\_WILLNEED建议与posix\_madvise一起使用 功能。这是对操作系统的提示，在不久的将来，该程序将需要调用中描述的页面。内核可以随意忽略建议，但也可以对页面进行故障修复。这样做的好处是粒度更细。可以预先对任何映射的地址空间区域中的单个页面或页面范围进行故障处理。对于包含很多在运行时未使用的数据的内存映射文件，这比使用MAP\_POPULATE具有巨大的优势 。

除了这些主动方法可以最大程度地减少页面错误数之外，还可以采用一种更被动的方法，这种方法在硬件设计人员中很流行。DSO占用地址空间中的相邻页面，每个页面用于代码和数据。页面大小越小，容纳DSO所需的页面越多。反过来，这也意味着更多的页面错误。在这里重要的是，相反的情况也是如此。对于较大的页面大小，减少了映射（或匿名内存）所需的页面数；随之而来的是页面错误的数量。

大多数体系结构支持4k的页面大小。在IA-64和PPC64上，页面大小为64k也很流行。这意味着分配内存的最小单位为64k。该值必须在编译内核时指定，并且不能动态更改（至少当前不能更改）。多页面大小体系结构的ABI旨在允许运行具有任一页面大小的应用程序。运行时将进行必要的调整，并且正确编写的程序不会注意到任何事情。页面尺寸越大，表示部分使用页面的浪费就越大，但是在某些情况下，这是可以的。

大多数体系结构还支持1MB或更大的超大页面大小。这样的页面在某些情况下也很有用，但是以如此大的单位分配所有内存是没有意义的。物理RAM的浪费只会太大。但是很大的页面有其优点：如果使用海量数据集，则在x86-64上将它们存储在2MB页面中（每大页面）将比使用相同数量的4k页面的内存减少511个页面错误。这可以带来很大的不同。解决方案是有选择地请求内存分配，该内存分配仅针对请求的地址范围使用巨大的内存页面，而对于同一进程中的所有其他映射，则使用正常的页面大小。

不过，巨大的页面尺寸要付出代价。由于用于大页面的物理内存必须是连续的，因此一段时间后，由于内存碎片，可能无法分配此类页面。防止这种情况。人们正在努力进行内存碎片整理和避免碎片整理，但这非常复杂。对于大的页面（例如2MB），总是很难获得必要的512个连续页面，除非一次：系统启动时。这就是为什么当前的大页面解决方案需要使用特殊文件系统 ugettlbfs的原因。该伪文件系统是应系统管理员的要求分配的，方法是写出应保留给它们的大页面数

```text
    / proc / sys / vm / nr_hugepages
```

应当保留的大页面数。如果找不到足够的连续内存，此操作可能会失败。如果使用虚拟化，情况将变得特别有趣。使用VMM模型进行虚拟化的系统无法直接访问物理内存，因此无法单独分配 hugetlbfs。它必须依赖VMM，并且不能保证支持此功能。对于KVM模型，运行KVM模块的Linux内核可以执行hugetlbfs分配，并可能将由此分配的页面的子集传递到来宾域之一。

以后，当程序需要大页面时，有多种可能性：

* * 程序可以使用带有SHM\_HUGETLB标志的System V共享内存 。
* 该hugetlbfs的文件系统实际上可以安装，然后程序可以创建一个文件下的安装点和使用 的mmap映射一个或多个页面的匿名内存。

在第一种情况下，无需安装hugetlbfs。请求一个或多个大页面的代码可能如下所示：

> ```text
> key_t k = ftok（“ / some / key / file”，42）;
> int id = shmget（k，LENGTH，SHM_HUGETLB | IPC_CREAT | SHM_R | SHM_W）;
> 无效* a = shmat（id，NULL，0）;
> ```

该代码序列的关键部分是使用 SHM\_HUGETLB标志和选择LENGTH的正确值，该值 必须是系统巨大页面大小的倍数。不同的体系结构具有不同的价值。使用System V共享内存接口存在一个令人讨厌的问题，即依赖于关键参数来区分（或共享）映射。该 ftok接口可以很容易产生冲突，这是什么原因，如果可能的话，最好是使用其他机制。

如果挂载ugeltlbfs文件系统的要求不是问题，则最好使用它而不是System V共享内存。使用特殊文件系统的唯一真正问题是内核必须支持它，并且还没有标准化的挂载点。一旦挂载了文件系统（例如/ dev / hugetlb），程序就可以轻松使用它：

> ```text
> int fd = open（“ / dev / hugetlb / file1”，O_RDWR | O_CREAT，0700）;
> 无效* a = mmap（NULL，LENGTH，PROT_READ | PROT_WRITE，fd，0）;
> ```

通过在打开的调用中使用相同的文件名，多个进程可以共享相同的大页面并进行协作。也可以使页面可执行，在这种情况下，还必须在mmap调用中设置PROT\_EXEC标志。就像在System V共享内存示例中一样，LENGTH的值必须是系统巨大页面大小的倍数。

防御性编写的程序（应该是所有程序一样）可以使用以下函数在运行时确定安装点：

> ```text
> 字符* hugetlbfs_mntpoint（void）{
>   char * result = NULL;
>   FILE * fp = setmntent（_PATH_MOUNTED，“ r”）;
>   如果（fp！= NULL）{
>     结构* m;
>     而（（m = getmntent（fp））！= NULL）
>        如果（strcmp（m-> mnt_fsname，“ hugetlbfs”）== 0）{
>          结果= strdup（m-> mnt_dir）;
>          休息;
>        }
>     endmntent（fp）;
>   }
>   返回结果；
> }
> ```

关于这两种情况的更多信息，可以在作为内核源代码树的一部分的hugetlbpage.txt文件中找到。该文件还描述了IA-64所需的特殊处理。

> ![](https://static.lwn.net/images/cpumemory/cpumemory.68.png)
>
> **图7.9：跟随大页面，NPAD = 0**

为了说明大页面的优势，图7.9显示了针对NPAD = 0进行随机Follow测试的结果。这与图3.15中所示的数据相同，但是这次，我们也使用分配在大页面中的内存来测量数据。可以看出，性能优势是巨大的。对于2个20字节，使用大页面的测试速度提高了57％。这是由于该大小仍完全适合单个2MB页面，因此，不会发生DTLB遗漏的事实。

此后，奖金最初较小，但随着工作装置尺寸的增加而再次增加。对于512MB工作集大小，大页面测试速度提高了38％。大页面测试的曲线在大约250个周期处处于平稳状态。除了2个27字节的工作集之外，数字再次显着增加。达到稳定水平的原因是2MB页的64个TLB条目覆盖2 27个字节。

如这些数字所示，使用大型工作台尺寸的大部分成本来自TLB缺失。使用本节中描述的接口可以节省大量时间。图中的数字很可能是上限，但即使是实际程序也显示出明显的加速。由于数据库使用大量数据，因此它们是当今使用大量页面的程序之一。

当前无法使用大页面来映射文件支持的数据。有实现这种功能的兴趣，但是到目前为止提出的建议都明确涉及使用大页面，并且它们依赖于 hugetlbfs文件系统。这是不可接受的：这种情况下的大页面使用必须是透明的。内核可以轻松确定哪些映射较大，并自动使用较大的页面。一个大问题是内核并不总是知道使用模式。如果内存可以映射为大页面，则以后需要4k页粒度（例如，因为使用mprotect更改了部分内存范围的保护，）会浪费很多宝贵的资源，尤其是线性物理内存。因此，成功实施这种方法肯定还需要更多时间。

