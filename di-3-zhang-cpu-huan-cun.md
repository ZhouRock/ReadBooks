# 第3章：CPU缓存

比起 25 年前的 CPU，现在的 CPU 已经复杂的多了，在那个时候，CPU 内核的频率与内存总线在相同的等级，访问内存相比访问寄存器只是慢了一点点，但是这一点在 90 年代初期大大地改变了，那时 CPU 设计者提升了 CPU 内核的频率，但是内存总线的频率以及 RAM 芯片的性能并没有等比例增长，这不是因为无法生产出更快的 RAM，正如我们在前一章所解释过的，这是可能的，但是并不经济，内存如果要达到与当前CPU一样快的速度，那么它的成本比动态 RAM要高好几个数量级。

如果给你两个选择，一个是内存的容量非常小，速度非常快的机器，另一个是大容量很大，速度相对还算快的机器，那么在考虑到工作集的大小会超过前一种机器，以及访问辅存（比如磁盘）的成本的情况下，后者总是赢家。这里的主要原因是访问辅助存储介质（比如硬盘）的速度，它必须用来保存交换出去的部分工作集，访问辅存的速度往往要比访问主存要慢上好几个数量级。

幸运的是，我们实际上不必做出非黑即白（all-or-nothing）的选择。一台计算机可以拥有一个小容量的高速 SRAM，再加上大容量的 DRAM。一个可能的实现是：将处理器地址空间的某块区域固定划分给 SRAM，剩下的则划分给 DRAM。这样一来，操作系统的任务就是优化数据的分配以便善用 SRAM。基本上，在这种情景下，SRAM 基本上是作为处理器寄存器集的扩充来使用的。

虽然这看上去像是可能的实现方式，但实际上并不可行。将 SRAM 内存的物理资源映射到进程的虚拟地址空间的问题本身就已经非常困难了，再加上这种方法还要求每个进程在软件中管理该SRAM内存区域的分配。内存区域的大小因处理器而异（也就是说，处理器有着不同容量的SRAM \)。組成一个程序的各个模块都会要求它的那份SRAM，还会引入额外的同步需求，简而言之，拥有快速内存的收益完全被管理资源的额外开销抵消了。

因此，我们并非将 SRAM 置于OS（操作系统）或者使用者的控制之下，而是让它变成由处理器透明地使用和管理的资源。在这种模式下，SRAM是用来保存不久就会被处理器用到的数据的临时副本（或者叫它缓存）。我们可以这么做的依据是代码和数据都具有时间局部性和空间局部性。这表示，在短时间内，相同的程序或数据很可能会被重复使用。对于程序来说，这表示非常有可能会在程序中进行循环（loop），相同的代码被反复执行（空间局部性的完美例子），数据的访问在理想情况下也会被限定在一小块区域中。即使在短时间內用到的内存并非邻近，同样的信息也有很高的机会在不久之后被再次用到（时间局部性），对代码来说，这表示，举例来说，在一个循环中产生一次函数调用（function call），这个函数在内存中可能相距甚远，但是调用这个函数在时间上可能会很接近，对于数据来说，这表示一次使用的内存总量（工作集大小）理想上是有限的，但是使用的内存，由于RAM随机存取的本质，并不是相邻的，知道局部性的存在是理解CPU缓存概念的关键，因为我们至今仍在使用它们。

一个简单的计算就能看出缓存在理论上有多么的有效。假设访问主存花费 200 个周期，而访问缓存花费 15 个周期。代码使用 100 个数据元素各 100 次，如果没有缓存，将会在内存操作上耗费 2,000,000 个周期，而如果所有数据都被缓存，只需要 168,500 个周期，提升了 91.5%。

用作缓存的 SRAM 大小比起主存小了好几倍。根据我的使用经验，缓存的大小是主存大小的 1/1000 左右（目前是4MB 缓存与 4GB 主存），这一点并没有什么问题，假如工作集（正在处理的数据）的大小比缓存大小还小，这没有关系，但是计算机一般都会使用比较大的主存，因此工作集总是会比缓存更大。尤其是那些运行多个进程的系统，其工作集的大小是所有进程与系统内核之和。

对于缓存大小的限制，我们需要制定一组很好的策略，它能够决定在给定的时间点应该缓存什么数据。由于并非所有的工作集数据都会正好在相同的时间点被使用，所以我们可以使用一些技术来暂时地将一些缓存中的数据替换成别的数据，而且这也许能在真正需要访问数据之前就完成。这种预取会省去一些访问主存的成本，因为对于程序的执行而言，这是异步进行的。所有的这些技术能让缓存看起来比实际上更大。我们将在 3.3 节讨论它们，协助处理器更好地工作是程序员的责任，我们在第 6 章中讨论该怎么样来做。

[https://blog.csdn.net/weixin\_33725272/article/details/85551352?utm\_medium=distribute.pc\_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-14.control&dist\_request\_id=&depth\_1-utm\_source=distribute.pc\_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-14.control](https://blog.csdn.net/weixin_33725272/article/details/85551352?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-14.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-14.control)

## 3.1 CPU缓存概览

在深入探讨 CPU 缓存的技术细节之前，我们先来理解一下缓存是如何融入现代计算机系统的大局之中的。

![&#x5716; 3.1&#xFF1A;&#x6700;&#x7C21;&#x6613;&#x7684;&#x5FEB;&#x53D6;&#x914D;&#x7F6E;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.1.png)

图 3.1：最简单的缓存配置

图 3.1 展示了最简单的缓存配置。早期的CPU 缓存系統架构就是这样的。CPU 核心不再直接连接到主存。所有的装载和储存都必须经过缓存。CPU 核心与缓存之间是一条特殊的、快速的连线 。在这个简化的示意图中，主存与缓存都被连接到系统总线上，这条总线同时也用来与其他系统组件通信。我们已经以FSB这个名词介绍过了系统总线，这是它现今使用的名称（参见 2.2节） 。在这一节中，我们会省略北桥；假定它存在，以方便 CPU 与主存的沟通。

即便过去数十年来，大多数计算机都采用冯诺依曼架构（von Neumann architecture），但实验证实程序与数据分离的缓存是比较好的。Intel 自 1993 年起采用程序与数据分离的缓存，就再也没有回头过。程序与数据的缓存所需的内存区域彼此相互独立，这也是独立的缓存运作得更好的原因。近年来，它的另一个优点也逐渐浮现：对大多数常见的处理器而言，指令解码（decoding）的步骤是很慢的；缓存解码过的指令能够让执行加速，预测失误或者无法预测的分支而导致流水线变空的情况下尤其如此。 

在引入缓存之后不久，系统就变得越来越复杂。缓存与主存之间的速度差异再次增加，直到加入了另一层级的缓存，比起第一级缓存来得更大也更慢，仅仅提升第一级缓存的大小，以经济因素来说并非一个可行的办法。今天，甚至现在的有些系统开始使用具有三层缓存的机器，具有这种处理器的系统看起来就像图 3.2 那样。随着单一 CPU 中的核心数增加，未来缓存层级也许会变得更多。

![&#x5716; 3.2&#xFF1A;&#x5177;&#x6709;&#x4E09;&#x5C64;&#x5FEB;&#x53D6;&#x7684;&#x8655;&#x7406;&#x5668;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.2.png)

图 3.2：具有三层缓存的处理器

图 3.2 显示了三层缓存，并引入了我们将会在本文其余部分使用的术语。L1d 是一级数据缓存、L1i 是一级指令缓存等等。注意，这只是一张示意图；真实的数据流从核心到主存的路上并不需要通过任何较高层级的缓存。CPU 设计者在缓存接口的设计上有着很大的自由。对软件开发人员来说，是看不到这些设计上的抉择的。 此外，我们有多核的处理器，每个核都能拥有多个线程（thread）。一个核与一个线程的差别在于，不同的核拥有（几乎）所有硬体资源各自的副本。除非同时用到相同的资源，像是对外连线，否则一个核是能够完全独立运作的。另一方面，线程则共享几乎所有处理器的资源。Intel 的线程实现只让其拥有个别的寄存器，甚至还是有限的，某些寄存器是共享的。所以，现代 CPU 的完整架构看起来就像图 3.3。

![](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.3.png)

图 3.3：多处理器，多核，多线程

在这张图中，我们有两颗处理器，每颗两个核心，每个核拥有两个线程，线程之间共享一级缓存，核心（以深灰色为底）拥有独立的一级缓存。所有 CPU 的核心共享更高层级的缓存。两颗处理器（两个浅灰色为底的大方块）自然不会共享任何缓存，这些都很重要，在我们讨论缓存对多进程与多线程应用程序的影响时尤为如此。

## 3.2 缓存操作一瞥

我们必须结合第2章所学到的机器架构与RAM技术、以及前一节所描述的缓存结构，以了解使用缓存的开销与节约之处。 默认情况下，由 CPU 核心读取或写入的所有数据都存在缓存中。有些内存区域无法被缓存，但只有 OS 的实现者需要去考虑这一点；这对应用程序员而言是不可见的。也有一些指令能让程序员刻意地绕过某些缓存，这些将会在第六章中讨论。 假如 CPU 需要一个数据字节，会先从缓存开始搜寻。显而易见的是，缓存无法容纳整个主存的全部内容（否则我们也就不需要缓存了），但由于所有内存地址都能被缓存，所以每个缓存条目（entry）都会使用数据字节在主存中的地址来标记（tag）。如此一来，读取或写入到某个地址的请求便会在缓存中搜寻符合的标签。在这个情境中，地址可以是虚拟的或是物理的，根据缓存的实现而有所不同。

除了真正的内存之外，标签也会需要额外的空间，因此使用一个字作为缓存的粒度（granularity）是很浪费的。对于一台 x86 机器上的一个 32 位的字而言，标签本身可能会需要 32 位以上。再者，空间局部性作为缓存的一个基本原理也应该被纳入考量的范畴，即邻近的内存很可能会一起被用到，所以它也应该一起被载入到缓存中。也要记得我们在 2.2.1 节所学到的：假如 RAM 模块能够在不需新的 $$\overline{\text{CAS}}$$、甚至是 $$\overline{\text{RAS}}$$ 信号的情况下传输多个数据字，这是更有效率的。所以储存在缓存中的条目并非是单一的字，而是由多个连续的字组成的行（line）。在早期的缓存中，这些行的长度为 32个 字节，如今一般是 64个字节。假如内存总线的宽度是 64 位，这表示每个缓存行要传输 8 次。DDR 有效地支持了这种传输方式。

当内存内容为处理器所需时，整个缓存行都会被载入到 L1d 中。每个缓存行的内存地址会根据缓存行的大小，以地址掩模（mask）的方式来计算。对于一个 64 字节的缓存行来说，这表示低 6 位为零。舍弃的位用作缓存行内的偏移量（offset），剩余的位在某些情况下用以定位缓存中的行、以及作为标签。在实践中，一个地址值会被切成三个部分，对于一个 32 位的地址来说，这看起来像下面这样：

[![&#x5FEB;&#x53D6;&#x4F4D;&#x5740;&#x503C;&#x793A;&#x610F;&#x5716;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/cache-address.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/cache-address.png)

一个大小为 2的$$\mathbf{O}$$次方的缓存行，低 $$\mathbf{O}$$ 位用作缓存行内的偏移量。接下来的 $$\mathbf{S}$$ 位选择缓存集（cache set）。我们马上就会深入探讨为何缓存行会使用集合的细节。现在只要知道有 2的$$\mathbf{S}$$次方个缓存行的集合就够了。剩下的 $$32 - \mathbf{S} - \mathbf{O} = \mathbf{T}$$ 位组成标签。这 $$\mathbf{T}$$ 个位是与每个缓存行相关联的、以区分在同一缓存集中所有别名（alias）的值。不必储存用以寻址缓存集的 $$\mathbf{S}$$ 位，因为它们对同个集合中的所有缓存行而言都是相同的。

当一个指令修改内存时，处理器依旧得先载入一个缓存行，因为没有指令能够一次修改一整个缓存行（这个规则有个例外：合并写入〔write-combining〕，会在 6.1 节说明）。因此在写入操作之前，得先载入缓存行的内容。缓存无法持有不完全的缓存行。已被写入、并且仍未写回主存的缓存行被称为脏的（dirty）。一旦将其写入，脏标志（dirty flag）便会被清除。

为了能够在缓存中载入新的数据，几乎总是得先在缓存中腾出空间。从 L1d 的逐出操作（eviction）会将缓存行往下推入 L2（使用相同的缓存行大小）。这自然代表 L2 也得腾出空间。这可能转而将内容推入 L3，最终到主存中。每次逐出操作都会越来越昂贵。这里所描述的是现代 AMD 与 VIA 处理器所优先采用的独占式缓存（exclusive cache）模型。Intel 实现了包含式缓存（inclusive caches），其中每个在 L1d 中的缓存行也会存在 L2 中。因此，从 L1d 进行逐出操作是更为快速的。有了足够的 L2 缓存的话，将内容存在两处而造成内存浪费的缺点是很小的，而这在逐出操作时会带来回报。独占式缓存的一个可能的优点是，载入一个新的缓存行只需碰到 L1d 而不需 L2，这会快一些。

只要处理器架构所规定的内存模型没有改变，CPU 是被允许以它们自己想要的方式来管理缓存的。举例来说，善用少量或没有内存总线活动的时段，并主动地将脏的缓存行写回到主存中，对处理器来说是非常好的。x86 与 x86-64，不同厂商、甚至是同一厂商的不同型号的处理器之间，有着各式各样的缓存架构，证明了内存模型抽象化的能力。

在对称式多处理器（Symmetric Multi-Processor，SMP）系统中，CPU 的缓存无法独立于彼此运作。所有处理器在任何时间都应该要看到相同的内存内容。这种内存一致性的维持被称为 “缓存一致性（cache coherency）”。假如一个处理器只看它自己拥有的缓存与主存，它就不会看到其它处理器中的脏缓存行的内容。提供从一个处理器到另一个处理器缓存的直接存取会非常昂贵，而且是个极大的瓶颈。取而代之的是，处理器能够在另一个处理器要读取或写入到某个缓存行时察觉到。

假如侦测到一次写入访问，并且处理器在其缓存中有这个缓存行的干净副本，这个缓存行就会被标为无效（invalid）。未来的查询会需要重新载入这个缓存行。注意到在另一颗 CPU 上的读取存取并不需要进行无效化，多个干净副本能够被保存得很好。

更加复杂的缓存实现容许其它的可能性发生。假设在一个处理器缓存中的一个缓存行是脏的，并且第二个处理器想要读取或写入这个缓存行。在这个情况下，主存的内容太旧了，而请求的处理器必须––作为替代––从第一个处理器取得缓存行的内容。第一个处理器经由窥探注意到了这个状况，并自动地将数据传输给请求的处理器。这个动作绕过了主存，虽然在某些实现中，是假定内存控制器会注意到这个直接传输、并将更新的缓存行内容储存到主存中。假如是为了写入而进行访问，第一个处理器便会将它所在区域缓存行的副本无效化。

许多缓存一致性协议随着时间被逐渐发展出来，最重要的是 MESI，我们将会 3.3.4 节中介绍它。这所有的结果可以被总结为一些简单的规则：

* 一个脏的缓存行不会出现在任何其它处理器的缓存中
* 相同缓存行的干净副本能够存在任意数量的缓存中

假如能够维持这些规则，即便在多处理器的系统中，处理器也能够有效地使用它们的缓存。所有处理器所需要做的，就是去监控其它处理器的写入，并将这个地址与它们区域缓存中的地址做比较。在下一节，我们将会深入更多实现、尤其是开销的一些细节。

最后，我们该至少有一个缓存命中（hit）与不命中（miss）的成本印象。这些是 Intel 针对 Pentium M 列出的数字：

| 到 | 周期 |
| :--- | :--- |
| 寄存器 | &lt;= 1 |
| L1d | ~3 |
| L2 | ~14 |
| 主存 | ~240 |

这些是以 CPU 周期测量的实际存取时间。有趣的是，对内建于芯片上的 L2 缓存而言，大部分（甚至可能超过一半）的存取时间都是由线路延迟造成的。这是一个只会随着缓存大小变大而变糟的实体限制。只有制程的缩小（举例来说，从 Intel 系列中 Merom 的 60nm 到 Penryn 的 45nm）能提升这些数字。

表格中的数字看起来很大，但幸运的是，不必在每次发生缓存载入与不命中时都负担全部的成本。一部分的成本可以被隐藏。现今的处理器全都会使用不同长度的内部流水线，指令会在其中被解码、并且为执行而准备。部份的准备是从内存（或缓存）载入值，假如它们要被传输到寄存器的话。假如内存载入操作能够足够早就在流水线中开始，它也许会与其它操作并行进行，而整个载入成本就可能被隐藏了。这对 L1d 经常是可能的；对某些有着较长流水线的处理器来说，L2 亦是如此。

提早开始内存读取有着诸多阻碍。也许简单的像是没有足够的资源来存取内存，或者是载入的地址需要作为另一个指令的结果才能取得。在这些情况中，载入成本无法被（完全地）隐藏。

对于写入操作，CPU 不必一直等到值被储存进内存中为止。只要接下来指令的执行就像是与值已被存入内存有着似乎相同的效果，就没有什么能阻止 CPU 走捷径了。它能够尽早开始执行下一条指令。有着影子寄存器（shadow register），其能够持有一般寄存器无法取得的值的帮助_，_甚至可能改变未完成的写入操作所要储存的值。

[![&#x5716; 3.4&#xFF1A;&#x96A8;&#x6A5F;&#x5BEB;&#x5165;&#x7684;&#x5B58;&#x53D6;&#x6642;&#x9593;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.4.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.4.png)

图 3.4：随机写入的存取時間

有关缓存行为影响的图表，见图 3.4。我们稍候会谈到这里产生数据的程序；这是一个连续以随机的方式访问某块大小可配的内存区域的程序。每笔数据有着固定的大小。元素的数量视选择的工作集大小而定。Y 轴表示处理一个元素所花费的 CPU 周期的平均值；注意到 Y 轴为对数刻度。这同样适用于所有这类图表的 X 轴。工作集的大小总是以2的幂次表示。

这张图显示了三个不同的平稳阶段。这并不让人意外：这个处理器有 L1d 与 L2 缓存，但没有 L3。经由一些经验，我们可以推论这个 L1d 大小为 2的13次方个字节，而 L2 大小为 2的20次方个字节。假如整个工作集能塞进 L1d 中，对每个元素的每次操作的周期数会低于 10。一旦超过了 L1d 的大小，处理器就必须从 L2 载入资料，而平均时间则迅速成长到 28 左右。一旦 L2 也不够大了，时间便飙升到 480 个周期以上。这即是许多、或者大部分操作必须从主存载入数据的时候了。更糟的是：由于数据被修改了，脏的缓存行也必须被写回。

看了这张图，大家应该有充分的动机来探究程序撰写上的改进、协助提升缓存使用方式了吧。我们在这里所谈论的并不是少得可怜的几个百分点；我们说的是有时可能的几个数量级的提升。在第6章，我们将会讨论能让我们写出更有效率的程序的技术。下一节会深入更多 CPU 缓存设计的细节，这部分知识很好，但是对于阅读本文的其余部分来说并非完全必要。所以你也可以选择性地跳过。

## 3.3 缓存实现的细节

缓存实现者有个麻烦是，在庞大的主存中，每个内存单元都可能得被缓存。假如一个程序的工作集足够大，这表示有许多为了缓存中的各个地方打架的主存位置。先前曾经提过，缓存与主存大小的比率为 1 比 1000 的情况并不罕见。

### 3.3.1 关联度

实现一个每个缓存行都能保存任意内存位置副本的缓存是有可能的（见图 3.5）。这被称为一个全关联式缓存（fully associative cache）。要存取一个缓存行，处理器核心必须要将每个缓存行的标签与请求地址的标签进行比较。标签由地址中除了偏移量之外的整个部分组成（这表示在 3.2 节图示中的 $$\mathbf{S}$$ 为零）。

有些缓存确实是像这样实现的，但是看看现今使用的 L2 数量，证明这是不切实际的。给定一个有着 64B 缓存行的 4MB 缓存，这个缓存将会有 65,536 个条目。为了达到足够的性能，缓存逻辑必须要能够在短短几个周期内，从这所有的条目中挑出符合给定标签的那一个，实现这一点要付出很大的精力。

[![&#x5716; 3.5&#xFF1A;&#x5168;&#x95DC;&#x806F;&#x5F0F;&#x5FEB;&#x53D6;&#x793A;&#x610F;&#x5716;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.5.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.5.png)

图 3.5：全关联式缓存示意图

对每个缓存行来说，都需要一个比较器（comparator）来比对很大的标签（注意，$$\mathbf{S}$$ 为零）。紧邻着每条连线的字母代表以位为单位的宽度。假如没有给定，那么它就是一条单一位的线路。每个比较器都必须比对两个 $$\mathbf{T}$$ 位宽的值。接着，基于这个结果，选择合适的缓存行内容，并令它能被取得。有多少缓存行，都得合并多少组 $$\mathbf{O}$$ 数据线。实现一个比较器所需的晶体管数量很大，特别是它必须运行的非常快的时候。迭代比较器（iterative comparator）是不可用的。节省比较器数量的唯一方式，就是反复地比较标签以减少比较器的数量。这与迭代比较器并不合适的理由相同：它太花时间了。

全关联式缓存对小缓存（例如在某些 Intel 处理器的 TLB 缓存就是全关联式的）来说是有实用价值的，但那些缓存都很小，非常小。我们所指的是至多只有几十个条目的情况。 

对 L1i、L1d、以及更高层级的缓存来说，需要采用不同的方法。我们所能做的是限制搜寻。在最极端的限制中，每个标签都恰好映射到一个缓存条目。计算方式很简单：给定 4MB／64B、有着 65,536 个条目的缓存，我们能够直接使用地址的 6 到 21 位（16 个位）来直接寻址每个条目，低 6 位是缓存行内部的索引。

[![&#x5716; 3.6&#xFF1A;&#x76F4;&#x63A5;&#x5C0D;&#x6620;&#x5F0F;&#x5FEB;&#x53D6;&#x793A;&#x610F;&#x5716;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.6.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.6.png)

图 3.6：直接映射式缓存示意图

如图 3.6 所见到的，这种直接映射式缓存（direct-mapped cache）很快，而且实现起来相对简单。它需要一个比较器、一个多路选择器（在这张示意图中有两个，标签与数据是分离的，但在这个设计上，这点并不是一个硬性要求）、以及一些用以选择有效缓存行内容的逻辑。比较器是因速度要求而复杂，但现在只有一个；因此，便能够花费更多的精力来让它变快。在这个方法中，实际的复杂之处都落在多路选择器上。在一个简易的多路选择器上，晶体管的数量以 $$O(\log N)$$ 成长，其中 $$N$$ 为缓存行的数量。这能够容忍，但可能会慢了点，在这种情况下，借由在多路选择器中增加更多的晶体管以并行化某些工作，便能够提升速度。晶体管的总数能够缓慢地随着缓存大小的增长而增长，使得这种解法非常有吸引力。但它有个缺点：只有在程序用到的地址，对于用以直接映射来说是均匀分布的情况下，它才能运行得很好。若非如此，而且经常这样的话，某些缓存条目会因为频繁地使用而被重复地逐出，而其余的条目则几乎完全没用到、或者一直是空的。

[![&#x5716; 3.7&#xFF1A;&#x96C6;&#x5408;&#x95DC;&#x806F;&#x5F0F;&#x5FEB;&#x53D6;&#x793A;&#x610F;&#x5716;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.7.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.7.png)

图 3.7：集合关联式缓存示意图

这个问题能借助缓存集合关联（set associative）来解决。一个集合关联式缓存结合了全关联式以及直接映射式缓存的良好特性，在很大程度上避免了那些设计的弱点。图 3.7 显示了一个集合关联式缓存的设计。标签与数据的储存被分成集合，其中之一会被缓存行的地址所选择。这与直接映射式缓存相似。但少数的值能以相同的集合编号缓存，而非令缓存中的每个集合编号都只有一个元素。所有集合内成员的标签会并行地比对，这与全关联式缓存的运行方式相似。

结果是，缓存不容易被不幸地，或者蓄意地，以相同集合编号的地址选择所击败，同时缓存的大小也不会受限于能被经济地实现的比较器的数量。假如缓存增长，它（在这张图中）只有行数会增加，列数则否。行数（以及比较器）只会在缓存的关联度（associativity）增加的时候才会增加。现今的处理器为 L2 或者更高层级的缓存所使用的关联度层级高达 24，L1 缓存通常使用 8 个集合。 

假设我们使用 4MB／64B 缓存以及 8 路（8-way）集合关联度，于是这个缓存便拥有 8,192 个集合，并且仅有 13 位的标签被用于寻址缓存集。要决定缓存集中的哪个（如果有的话）条目包含被寻址的缓存行，必须要比较 8 个标签。在非常短的时间内做到如此是可行的。借由实验我们能够看到，这是合理的。

| L2 缓存大小 | 关联度 |  |  |  |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 直接 | 2 | 4 | 8 |  |  |  |  |  |
| CL=32 | CL=64 | CL=32 | CL=64 | CL=32 | CL=64 | CL=32 | CL=64 |  |
| 512k | 27,794,595 | 20,422,527 | 25,222,611 | 18,303,581 | 24,096,510 | 17,356,121 | 23,666,929 | 17,029,334 |
| 1M | 19,007,315 | 13,903,854 | 16,566,738 | 12,127,174 | 15,537,500 | 11,436,705 | 15,162,895 | 11,233,896 |
| 2M | 12,230,962 | 8,801,403 | 9,081,881 | 6,491,011 | 7,878,601 | 5,675,181 | 7,391,389 | 5,382,064 |
| 4M | 7,749,986 | 5,427,836 | 4,736,187 | 3,159,507 | 3,788,122 | 2,418,898 | 3,430,713 | 2,125,103 |
| 8M | 4,731,904 | 3,209,693 | 2,690,498 | 1,602,957 | 2,207,655 | 1,228,190 | 2,111,075 | 1,155,847 |
| 16M | 2,620,587 | 1,528,592 | 1,958,293 | 1,089,580 | 1,704,878 | 883,530 | 1,671,541 | 862,324 |

表 3.1：缓存大小、关联度、以及缓存行大小的影响

表 3.1 显示了对于一个程序（在这个例子中是 gcc，根据 Linux 系统核心的人们的说法，它是所有基准中最重要的一个）在改变缓存大小、缓存行大小、以及关联度集合大小时，L2 缓存不命中的次数。在 7.2 节中，我们将会介绍对于这个测试，所需要用以模拟缓存的工具。

以防这些值的关联仍不明显，这里所有的值的关系是，缓存的大小为 $$\text{缓存行大小} \times \text{关联度} \times \text{集合的数量}$$ 

地址是以 3.2 节的图中示意的方式，使用下面的公式来对应到缓存中的。

$$\begin{aligned} \mathbf{O} &= \log_{2} \text{缓存行大小} \ \mathbf{S} &= \log_{2} \text{集合的数量} \end{aligned}$$ 

[![&#x5716; 3.8&#xFF1A;&#x5FEB;&#x53D6;&#x5927;&#x5C0F; vs &#x95DC;&#x806F;&#x5EA6;&#xFF08;CL=32&#xFF09;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.8.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.8.png)

图 3.8：缓存大小 vs 关联度（CL=32）

图 3.8 让这个表格的数据更容易理解。它显示了缓存行大小固定为 32 位的数据。看看对于给定缓存大小的数字，我们可以发现关联度确实有助于显著地降低缓存不命中的次数。以一个 8MB 缓存来说，从直接映射式变成 2 路集合关联式避免了几乎 44% 的缓存不命中。相比于一个直接映射式缓存，使用一个集合关联式缓存的话，处理器能够在缓存中保存更多的工作集。

在文献中，偶尔会读到引入关联度与加倍缓存大小有著相同的影响。在某些极端的例子中，如同能够从 4MB 跳到 8MB 缓存所看到的，确实如此。但再一次加倍关联度的话，显然就不是如此了。如同我们能从数据中所看到的，接下来的提升要小得多。不过，我们不该完全低估这个影响。在范例程序中，内存使用的尖峰为 5.6M。所以使用一个 8MB 缓存，同样的缓存集不大可能被多次（超过两次）使用。有个较大的工作集的话，能够节约的更多。如同我们能够看到的，对于较小的缓存大小来说，关联度的获益较大。

一般来说，将一个缓存的关联度提升到 8 以上，似乎对一个单线程的工作量来说只有很小的影响。随着共享第一级缓存的超线程处理器、以及使用一个共享 L2 缓存的多核处理器的引入，形势转变了。现在你基本上会有两个程序命中相同的缓存，这导致关联度会在实践上打对折（对四核处理器来说是四分之一）。所以能够预期，提升核心的数量，共享缓存的关联度也应该成长。一旦这不再可能（16 路集合关联度已经很难了），处理器设计师就必须开始使用共享的 L3 或者更高层级的缓存，而 L2 缓存则是潜在地由核心的子集所共享。

我们能在图 3.8 学到的另一个影响是，增加缓存大小是如何提升性能的。这个数据无法在不知道工作集大小的情况下解释。显然地，一个与主存一样大的缓存，会导致比一个较小缓存更好的结果，所以一般来说不会有带着可预见优势的最大缓存大小的限制。

如同上面所提到的，工作集大小的尖峰为 5.6M。这并没有给我们任何最佳缓存大小的确切数字，但它能让我们估算出这个数字。问题是，并非所有被用到的内存都是连续的，因此我们会有––即使是以一个 16M 的缓存与一个 5.6M 的工作集––冲突（conflict）（看看 2 路集合关联式的 16MB 快取相较于直接映射版本的优势）。但有把握的是，以同样的工作量，一个 32MB 缓存的获益是可以忽略不计的。但谁说过工作集大小必须维持不变了？工作量是随著时间成长的，缓存大小也应该如此。在购买机器、并且在你得去挑选愿意为此买单的缓存大小时，是值得去衡量工作集大小的。在图 3.10 中能够看到这件事何以重要。

[![&#x5716; 3.9&#xFF1A;&#x6E2C;&#x8A66;&#x8A18;&#x61B6;&#x9AD4;&#x4F48;&#x5C40;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.9.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.9.png)

图 3.9：测试内存布局

执行了两种类型的测试。在第一个测试中，元素是循序处理的。测试程序沿着指针（pointer）n 前进，但数组会以令它们在内存中排列的顺序被访问。这能够在图 3.9 的下半部看到。有个来自最后一个元素的回溯参考。在第二个测试中（图中的上半部），元素是以随机访问的。在这两种情况中，元素都会形成一个循环的单向链（single-linked list）。

### 3.3.2 缓存影响的测量

所有的图表都是由一个能模拟任意大小的工作集、读取与写入存取、以及循序或随机存取的程序所产生的。我们已经在图 3.4 中看过一些结果。这个程序会产生与工作集大小相同、这种型别的列表：

```text
struct l {
  struct l *n;
  long int pad[NPAD];
};
```

所有的项目都使用 n 个元素，以循序或是随机的顺序，链结在一个循环的列表中。即使元素是循序排列的，从一个条目前进到下一个条目总是会用到这个指针。pad 元素为数据负载（payload），并且能增长为任意大小。在某些测试中，数据会被修改，而在其余的情况中，程序只会执行读取操作。

在性能测量中，我们讨论的是工作集的大小。工作集是由一个 struct l 元素的列表所组成的。一个 2N 位的工作集包含 2N / sizeof\(struct l\) 个元素。显然地，sizeof\(struct l\) 视 NPAD 的值而定。以 32 位的系统来说，NPAD=7 代表每个列表元素的大小为 32 位，以 64 位的系统来说，大小为 64 位。

**单线程循序存取**

[![&#x5716; 3.10&#xFF1A;&#x5FAA;&#x5E8F;&#x8B80;&#x53D6;&#x5B58;&#x53D6;&#xFF0C;NPAD=0](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.10.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.10.png)

图 3.10：循序读取存取，NPAD=0

最简单的情况就是直接遍历列表的所有条目。列表元素是循序排列、紧密地塞在一起的。不管处理的顺序是正向或反向都无所谓，处理器在两个方向上都能处理得一样好。我们这里––以及在接下来的所有测试中––所要测量的是，处理一个单向列表元素要花多久。时间单位为处理器周期。图 3.10 显示了这个结果。除非有另外说明，否则所有的测量都是在一台 Pentium 4 以 64 位模式获得的，这表示 NPAD=0 的结构 l 大小为八位。

前两个测量结果受到了噪声的污染。测量的工作量太小了，因而无法过滤掉其余系统的影响。我们能够放心地假设这些值都在 4 个周期左右。考虑到这点，我们能够看到三个不同的水平（level）：

* 工作集大小至多到 214 位
* 从 215 位到 220 位
* 221 位以上

这些阶段能够轻易地解读：处理器拥有一个 16kB L1d 与 1MB L2。我们没有在从一个水平到另一个水平的转变之处看到尖锐的边缘，因为缓存也会被系统的其它部分用到，因此缓存并不是专门给这个程序的数据所使用的。特别是 L2 缓存，它是一个统一式缓存（unified cache），也会被用来存放指令（注：Intel 使用包含式缓存）。

或许完全没有预期到的是，对于不同工作集大小的实际时间。L1d 命中的时间是预期中的：在 P4 上，L1d 命中之后的载入时间大约是 4 个周期。但 L2 存取怎么样呢？一旦 L1d 不足以保存数据，可以预期这会让每个元素花上 14 个周期以上，因为这是 L2 的存取时间。但结果显示只需要大约 9 个周期。这个差异能够以处理器中的先进逻辑来解释。预期使用连续的内存区域时，处理器会预取下一个缓存行。这表示，当真的用到下个缓存行时，它已经载入一半了。等待下一个缓存行载入所需的延迟因而比 L2 存取时间要少得多。

一旦工作集大小增长到超过 L2 的大小，预取的效果甚至更明显。先前我们说过，一次主存的存取要花费 200+ 个周期。只有利用有效的预取，处理器才可能让存取时间维持在低至 9 个周期。如同我们能从 200 与 9 之间的差异所看到的，它的效果很好。

[![&#x5716; 3.11&#xFF1A;&#x5FAA;&#x5E8F;&#x8B80;&#x53D6;&#x591A;&#x7A2E;&#x5927;&#x5C0F;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.11.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.11.png)

图 3.11：循序读取多种大小

我们能够在预取的时候––至少间接地––观察处理器。在图 3.11 中，我们看到的是相同工作集大小的时间，但这次我们看到的是不同 l 结构大小的曲线。这表示在列表中有比较少、但比较大的元素。不同大小有着令（仍然连续的）列表中的 n 元素之间的距离成长的影响。在图中的四种情况，距离分别为 0、56、120、248 位。

在底部我们可以看到图 3.10 的线，但这时它看起来差不多像是条平坦的线。其它情况的时间要糟得多了。我们也能在这张图中看到三个不同的水平，我们也看到在工作集大小很小的情况下有着很大的误差（再次忽略它们）。只要仅有 L1d 牵涉其中，这些线差不多都相互重合，无需预取，因此所有元素大小每次访问都需达到L1d

在 L2 缓存命中的情况下，我们看到三条新的线相互重合得很好，但它们在比较高的水平上（大约 28）。这是 L2 存取时间的水平。这表示从 L2 到 L1d 的预取基本上失效了。即使是 NPAD=7，我们在循环的每一次迭代都需要一个新的缓存行；以 NPAD=0 而言，在需要下一个缓存行之前，循环得迭代八次。预取逻辑无法每个周期都载入一个新的缓存行。因此，我们看到的便是在每次迭代时，从 L2 载入的延误。

一旦工作集大小超过 L2 的容量，甚至变得更有趣了。现在四条线全都离得很远。不同的元素大小显然在效能差异上扮演着一个重大的角色。处理器应该要识别出步伐（stride）的大小，不为 NPAD=15 与 31 获取不必要的缓存行，因为元素的大小是比预取窗（prefetch window）还小的（见 6.3.1 节）。元素大小妨碍预取效果之处，是一个硬件预取限制的结果：它无法横跨页（page）边界。我们在每次增加大小时，都减少了 50% 硬件调度器（scheduler）的效率。假如硬件调度器（prefetcher）被允许横跨页面边界，并且下一个页面不存在或者无效时，OS 就得被卷入分页的定位中。这表示程序要经历并非由它自己产生的页面错误（page fault）。这是完全无法接受的，因为处理器并不知道一个页面是不在内存内还是不存在。在后者的情况下，OS 必须要中断进程。在任何情况下，假定––以 NPAD=7 或以上而言––每个列表元素都需要一个缓存行，硬件调度器便爱莫能助了。由于处理器一直忙着读取一个字、然后载入下一个元素，根本没有时间去从内存载入资料。

变慢的另一个主要原因是 TLB 缓存的不命中。这是一个储存了从虚拟地址到物理地址的转译结果的缓存，如同在第四节所详细解释的那样。由于 TLB 缓存必须非常地快，所以它非常地小。假如重复存取的页数比 TLB 缓存拥有的还多，就必须不断地重算代表着虚拟到物理地址的转译结果的条目。这是一个非常昂贵的操作。对比较大的元素大小而言，一次 TLB 查询的成本是分摊在较少的元素上的。这表示对于每个列表元素，必须要计算的 TLB 条目总数较多。

为了观察 TLB 的影响，我们可以执行一个不同的测试。对于第一个量测，我们像往常一样循序地摆放元素。我们使用 NPAD=7 作为占据一整个快取行的元素。对于第二个量测，我们将每个列表元素放置在个别的页中。每个页的其余部分维持原样，我们不会将它算在工作集大小的总和中。结果是，对于第一个量测，每次列表迭代都需要一个新的缓存行，并且每 64 个元素一个新的页。对第二个量测而言，每次迭代都需要载入一个在另一个页上的缓存行。

[![&#x5716; 3.12&#xFF1A;TLB &#x5C0D;&#x5FAA;&#x5E8F;&#x8B80;&#x53D6;&#x7684;&#x5F71;&#x97FF;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.12.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.12.png)

图 3.12：TLB 对循序读取的影响

结果可以在图 3.12 中看到。量测都是在与图 3.11 相同的机器上执行的。由于可用 RAM 的限制，工作集大小必须限制在 224 位，其需要 1GB 以将物件放置在个别的页上。下方的红色曲线正好对应到图 3.11 中的 NPAD=7 曲线。我们看到了显示了 L1d 与 L2 缓存大小的不同阶段。第二条曲线看起来完全不同。重要的特征是，当工作集大小达到 213 位时开始的大幅飙升。这即是 TLB 缓存溢出（overflow）的时候了。由于一个元素大小为 64 位，我们能够计算出 TLB 缓存有 64 个条目。由于程序锁定了内存以避免它被移出，所以成本不会受页错误影响。

可以看出，计算实体地址、并将它储存在 TLB 中所花的周期数非常高。图 3.12 中的曲线显示了极端的例子，但现在应该能清楚的一点是，对于较大的 NPAD 值而言，一个变慢的重大因素即是 TLB 缓存效率的降低。由于物理地址必须要在缓存行能从 L2 或主存读取前算出来，因此地址转译的损失就被附加到了内存存取时间上。这在某种程度上解释了，为何每个列表元素在 NPAD=31 的总成本会比 RAM 在理论上的存取时间还高的原因。

[![&#x5716; 3.13&#xFF1A;&#x5FAA;&#x5E8F;&#x8B80;&#x53D6;&#x8207;&#x5BEB;&#x5165;&#xFF0C;, NPAD=1](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.13.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.13.png)

图 3.13：循序读取与写入，NPAD=1

我们可以透过观察修改列表元素的测试执行的数据，来一瞥预取实现的多一些细节。图 3.13 显示了三条线。在所有情况中的元素宽度都是 16 位。第一条线是现在已经很熟悉的串列巡访，它会被当作一条基准线。第二条线––标为“Inc”––仅会在前往下一个元素前，增加当前元素的 pad\[0\] 成员的值。第三条线––标为“Addnext0”––会取下一个元素的 pad\[0\] 的值，并加到当前串列元素的 pad\[0\] 成员中。

天真的假设大概是“Addnext0”测试跑得比较慢，因为它有更多工作得做。在前进到下一个串列元素之前，就必须载入这个元素的值。这即是看到这个测试实际上––对于某些工作集大小而言––比“Inc”测试还快这点会令人吃惊的原因了。对此的解释是，载入下个串列元素基本上就是一次强制的预取。无论程式在何时前进到下个串列元素，我们都确切地知道这个元素已经在 L1d 快取中了。因此我们看到，只要工作集大小能塞进 L2 快取，“Addnext0”就执行得跟单纯的“Follow”一样好。

不过“Addnext0”测试比“Inc”测试更快耗尽 L2。因为它需要从主记忆体载入更多的资料。这即是在工作集大小为 221 位元组时，“Addnext0”测试达到 28 个循环水平的原因了。28 循环水平是“Follow”测试所达到的 14 循环水平的两倍高。这也很容易解释。由于其它两个测试都修改了记忆体，L2 快取为了腾出空间给新的快取行的逐出操作便不能直接把资料丢掉。它必须被写到记忆体中。这表示 FSB 中的可用频宽被砍了一半，因此加倍了资料从主记忆体传输到 L2 所花的时间。

[![&#x5716; 3.14&#xFF1A;&#x8F03;&#x5927; L2&#xFF0F;L3 &#x5FEB;&#x53D6;&#x7684;&#x512A;&#x52E2;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.14.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.14.png)

图 3.14：较大 L2／L3 缓存的优势

循序、有效的快取管理的最后一个面向是快取的大小。虽然这应该很明显，但仍需要被提出来。图 3.14 显示了以 128 位元组元素（在 64 位元机器上，NPAD=15）进行 Increment 测试的时间。这次我们看到量测结果来自三台不同的机器。前两台机器为 P4，最后一台为 Core2 处理器。前两台由不同的快取大小来区分它们自己。第一个处理器有一个 32k L1d 与一个 1M L2。第二个处理器有 16k L1d、512k L2、与 2M L3。Core2 处理器有 32k L1d 与 4M L2。

这张图有趣的部分不必然是 Core2 处理器相对于其它两个表现得有多好（虽然这令人印象深刻）。这里主要有兴趣的地方是，工作集大小对于各自的最后一层快取来说太大、并使得主记忆体得大大地涉入其中之处。

如同预期，最后一层的快取越大，曲线在相应于 L2 存取成本的低水平停留得越久。要注意的重要部分是它所提供的效能优势。第二个处理器（它稍微旧了一点）在 220 位元组的工作集上能够以两倍于第一个处理器的速度执行。这全都归功于最后一层快取大小的提升。有著 4M L2 的 Core2 处理器甚至表现得更好。

对于随机的工作量而言，这可能不代表什么。但若是工作量能被裁剪成最后一层快取的大小，程式效能便能够极为大幅地提升。这也是有时候值得为拥有较大快取的处理器花费额外金钱的原因。

**单线程随机存取**

我们已经看过，处理器能够借由预取缓存行到 L2 与 L1d，来隐藏大部分主存、甚至是 L2 的存取等待时间。不过，这只有在能够预测内存的存取时才能良好运作。

[![&#x5716; 3.15&#xFF1A;&#x5FAA;&#x5E8F; vs &#x96A8;&#x6A5F;&#x8B80;&#x53D6;&#xFF0C;NPAD=0](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.15.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.15.png)

图 3.15：循序 vs 随机读取，NPAD=0

若是存取模式是不可预测、或者随机的，情况便大大地不同。图 3.15 比较了循序存取每个串列元素的时间（如图 3.10）以及当串列元素是随机分布在工作集时的时间。顺序是由随机化的链结串列所决定的。没有让处理器能够确实地预取资料的方法。只有一个元素偶然在另一个在记忆体中也彼此邻近的元素不久之后用到，这才能起得了作用。

在图 3.15 中，有两个要注意的重点。第一点是，增长工作集大小需要大量的周期数。机器能够在 200-300 个周期内存取主记忆体，但这里我们达到了 450 个周期以上。我们先前已经看过这个现象了（对比图 3.11）。自动预取在这里实际上起了反效果。

[![&#x5716; 3.16&#xFF1A;L2d &#x932F;&#x5931;&#x7387;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.16.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.16.png)

图 3.16：L2d 错失率

第二个有趣的地方是，曲线并不像在循序存取的例子中那样，在多个平缓阶段变得平坦。曲线持续上升。为了解释这点，我们能够针对不同的工作集大小量测程式的 L2 存取次数。结果能够在图 3.16 与表 3.2 看到。

图表显示，当工作集大小大于 L2 的大小时，缓存错失率（L2 存取数 / L2 错失数）就开始成长了。这条曲线与图 3.15 的曲线有著相似的形式：它快速地上升、略微下降、然后再度开始上升。这与每串列元素所需循环数的曲线图有著密切的关联。L2 错失率最终会一直成长到接近 100% 为止。给定一个足够大的工作集（以及 RAM），任何随机选取的缓存行在 L2 或是载入过程中的机率便能够被随心所欲地降低。

| 集合大小 | 循序 | 隨機 |  |  |  |  |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| L2 命中数 | L2 错失数 | 迭代次数 | 错失／命中比率 | 每迭代 L2 存取数 | L2 命中数 | L2 错失数 | 迭代次数 | 错失／命中比率 | 每迭代 L2 存取数 |  |
| 220 | 88,636 | 843 | 16,384 | 0.94% | 5.5 | 30,462 | 4721 | 1,024 | 13.42% | 34.4 |
| 221 | 88,105 | 1,584 | 8,192 | 1.77% | 10.9 | 21,817 | 15,151 | 512 | 40.98% | 72.2 |
| 222 | 88,106 | 1,600 | 4,096 | 1.78% | 21.9 | 22,258 | 22,285 | 256 | 50.03% | 174.0 |
| 223 | 88,104 | 1,614 | 2,048 | 1.80% | 43.8 | 27,521 | 26,274 | 128 | 48.84% | 420.3 |
| 224 | 88,114 | 1,655 | 1,024 | 1.84% | 87.7 | 33,166 | 29,115 | 64 | 46.75% | 973.1 |
| 225 | 88,112 | 1,730 | 512 | 1.93% | 175.5 | 39,858 | 32,360 | 32 | 44.81% | 2,256.8 |
| 226 | 88,112 | 1,906 | 256 | 2.12% | 351.6 | 48,539 | 38,151 | 16 | 44.01% | 5,418.1 |
| 227 | 88,114 | 2,244 | 128 | 2.48% | 705.9 | 62,423 | 52,049 | 8 | 45.47% | 14,309.0 |
| 228 | 88,120 | 2,939 | 64 | 3.23% | 1,422.8 | 81,906 | 87,167 | 4 | 51.56% | 42,268.3 |
| 229 | 88,137 | 4,318 | 32 | 4.67% | 2,889.2 | 119,079 | 163,398 | 2 | 57.84% | 141,238.5 |

表 3.2：循序与随机访问时的 L2 命中与错失，NPAD=0

光是快取错失率的提高就能够解释一部分成本。但有著另一个因素。看看表 3.2，我们能够看到在 L2 / 叠代数那栏，程式每次叠代所使用的 L2 总数都在成长。每个工作集都是前一个的两倍大。所以，在没有快取的情况下，我们预期主记忆体的存取次数会加倍。有了快取以及（几乎）完美的可预测性，我们看到显示在循序存取的数据中，L2 使用次数增长得很保守。其增长除了工作集大小的增加以外，就没有别的原因了。

[![&#x5716; 3.17&#xFF1A;&#x9010;&#x9801;&#xFF08;page-wise&#xFF09;&#x96A8;&#x6A5F;&#x5316;&#xFF0C;NPAD=0](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.17.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.17.png)

图 3.17：逐頁（page-wise）随机化，NPAD=7

对于随机存取，每次工作集大小加倍的时候，每个元素的存取时间都超过两倍。这表示每个串列元素的平均存取时间增加了，因为工作集大小只有变成两倍而已。背后的原因是 TLB 错失率提高了。在图 3.17 中，我们看到在 NPAD=7 时随机存取的成本。只是这次，随机化的方式被修改了。一般的情况下，是将整个串列作为一个区块（block）随机化（以标签〔label〕 $$\infty$$ 表示），而其它的 11 条曲线则表示在比较小的区块内进行随机化。标记为“60”的曲线，代表每组由 60 个分页（245,760 位元组）组成的集合会分别进行随机化。这表示在走到下一个区块的元素之前，会先巡访过所有区块内的串列元素。这使得在任何一个时间点使用的 TLB 项目的数量有所限制。

在 NPAD=7 时的元素大小为 64 位元组，这与快取行大小一致。由于串列元素的顺序被随机化了，因此硬体预取器不大可能有任何效果，尤其在有一堆元素的情况下。这表示 L2 快取的错失率与在一个区块内的整个串列随机化相比并不会有显著地不同。测试的效能随著区块大小增加而逐渐地逼近单一区块随机化的曲线。这表示后者的测试案例的效能显著地受到了 TLB 错失的影响。假如 TLB 错失次数能够降低，效能便会显著地提升（在我们稍候将会看到的测试中，高达 38%）。

### 3.3.3 写入行为

在我们开始研究在多执行环境（execution context）（线程或进程）使用相同内存的缓存行为之前，我们必须先探究一个缓存实现的细节。缓存是假定为一致的（coherent），而且对使用者层级的程序而言，这个一致性是假定为完全透明的。系统核心程序是不同的情况；它偶尔会要求缓存冲出（flush）。

这具体意味著，假如一个缓存行被修改了，在这个时间点之后，对系统而言的结果与根本没有缓存、并且是主存位置本身被修改的情况是相同的。这能以两种方式或策略来实行：

* 直写式（write-through）缓存实现
* 回写式（write-back）缓存实现

直写式缓存是最简单的缓存一致性的实行方式。若是缓存行被写入的话，处理器也会立即将缓存行写到主存中。这保证了主存与缓存永远保持一致。能够在任何缓存行被取代的时候直接丢弃缓存的内容。这个缓存策略很简单，但并不是非常快。举例来说，一个不断地修改一个局部变量的程序会在 FSB 产生大量的流量，尽管数据很可能不会在别处用到、而且可能只会短暂存在。

回写式策略更为复杂。这时处理器不会立即将被修改的缓存行写回到主存里。取而代之地，缓存行只会被标记为脏的。当缓存行在未来的某个时间点从缓存被丢弃时，脏位（dirty bit）将会在这时通知处理器去把数据写回去，而不是直接丢弃内容。

写回式缓存有机会做得好非常多，这就是大多有着像样处理器的系统中，内存都会以这种方式缓存的原因了。处理器甚至能在缓存行必须被清除之前，利用 FSB 的闲置容量来储存缓存行的内容。这使得脏位被清除，并且在需要缓存中的空间的时候，处理器能够直接丢弃这个缓存行。

但回写式实现也有个重大的问题。当有多于一个处理器（或是核心或超线程），并且存取到同样的内存时，它仍旧必须保证每个处理器看到的一直都是相同的内存内容。假如一个缓存行在一个处理器上是脏的（也就是说，它还没被写回去），并且第二个处理器试著读取相同的内存位置，这个读取操作就不能直接送到主存去。而是需要第一个处理器的缓存行的内容。在下一节，我们将会看到这在当前是如何实作的。

在此之前，还有两种缓存策略要提一下：

* 合并写入（write-combining）；以及
* 不可缓存（uncacheable）

这两种策略都是用在地址空间中、并非被真正的 RAM 所支援的特殊区域。系统核心为这些位址范围设置了这些策略（在使用了记忆体型态范围暂存器〔Memory Type Range Register，MTRR〕的 x86 处理器上），剩下的部分自动地进行。MTRR 也能用于在直写式与回写式策略之间选择。

合并写入是一种受限的缓存最佳化，更常用于显卡一类装置上的 RAM。由于对装置来说，传输成本比区域 RAM 存取的成本还高得多，因此避免过多的传输是更为重要的。仅因为缓存行中的一个字组被修改，就传输一整个缓存行，在下一个操作修改了下一个字组的情况下是很浪费的。能够轻易地想像，一种常见的情况是，表示萤幕上水平相邻的像素点的记忆体，在多数情况下也是相邻的。如同名字所暗示的，合并写入会在快取行被写出去之前合并多个写入存取。在理想情况下，快取行会被一个字组接著一个字组地修改，并且只有在写入最后一个字组之后，快取行才会被写到装置中。这能够显著地加速装置对 RAM 的存取。

最后，不可缓存的内存。这通常表示内存位置根本不被 RAM 所支持。它可能是一个被写死的特殊地址，以拥有某个在 CPU 外部实现的功能。对商用硬件来说，最常见的例子是内存映射的（memory-mapped）地址范围，转译为对附属于总线上的扩充卡以及装置（PCIe 等等）的存取。在嵌入式单板（embedded board）上，有时候会发现能够用来开关 LED 的内存地址。缓存这类地址显然是个坏点子。在这种情境下的 LED 是用以除错或者状态回报的，会想要尽可能快地看到它。在 PCIe 扩充卡上的内存能够在不与 CPU 互动的情况下改变，因此这种内存不应该被缓存。

### 3.3.4 多处理器的支持

在上一节，我们已经指出，当多处理器开始起作用时我们会遇到的问题。多核处理器甚至有那些并没有被共享的快取层级（至少 L1d）的问题。

提供从一个处理器到另一个处理器的缓存的直接存取是完全不切实际的。首先，连线根本不够快。实际的替代方案是，将缓存内容传输给另一个处理器––假如需要的话。注意到这也同样适用于不在相同处理器上共享的缓存。

现在的问题是，什么时候得传输这个缓存行？这是个相当容易回答的问题：当一个处理器需要读取或写入一个缓存行，而其在另一个处理器的缓存上是脏的。但处理器要怎么样才能判断一个缓存行在另一个处理器的缓存上是脏的呢？仅因为一个缓存行被另一个处理器载入就假定如此，（至多）也是次佳的（suboptimal）。通常，大多数的内存存取都是读取操作，产生的缓存行也不是脏的。处理器对缓存行的操作是很频繁的（那当然，不然我们怎么会有这篇论文？），这表示在每次写入操作之后，都去广播被改变的缓存行的信息是不切实际的。

这些年来所发展出来的就是 MESI 缓存一致性协定（修改〔Modified〕、独占〔Exclusive〕、共享〔Shared〕、无效〔Invalid〕）。这个协定的名称来自采用 MESI 协定时、一个缓存行能够变成的四个状态：修改本地的处理器已经修改过缓存行。这也暗指它是在任何缓存中的唯一副本。独占缓存行没有被修改过，但已知没有被载入到任何其它处理器的快取中。共享缓存行没有被修改过，并且可能存在于另一个处理器的缓存中。无效缓存行是无效的––也就是说，没有被使用。

多年来，这个协议从比较不复杂、但也比较没效率的较简易版本开始发展。有了这四个状态，便可能有效率地实作回写式缓存，而又支持同时在不同的处理器上使用唯读的信息。

[![&#x5716; 3.18&#xFF1A;MESI &#x5354;&#x5B9A;&#x7684;&#x72C0;&#x614B;&#x8F49;&#x63DB;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.18.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.18.png)

图 3.18：MESI 协议的状态转换

借由处理器监听––或者窥探––其它处理器的运作，不用太多精力便得以完成状态改变。处理器执行的某些操作会被发布在外部针脚上，因而让处理器的缓存处理能被外界看到。处理中的缓存行地址能在地址总线上看到。在接下来对状态与其转换（显示在图 3.18）的描述中，我们会指出总线是何时被牵扯进来的。

起初所有快取行都是空的，因此也是无效的。若是资料是为了写入而载入快取，则改为修改。若是资料是为了读取而载入，新的状态则取决于另一个处理器是否也已载入这个快取行。如果是的话，新的状态为共享，否则为独占。

若是一个修改的快取行从本地处理器被读取或写入，这个指令能够使用当前的快取内容，并且状态不变。若是第二个处理器想要读取这个快取行，第一个处理器就必须将它的快取内容寄送给第二个处理器，然后它就能将状态改为共享。寄送给第二个处理器的资料也会被记忆体控制器接收并处理，其会将内容储存在记忆体中。假如没有这么做，快取行就不能被标为共享。若是第二个处理器想要写入快取行，第一个处理器便会寄送快取行的内容，并将自己的快取行标为无效。这即是恶名昭彰的“所有权请求（Request For Ownership，RFO）”操作。在最后一个层级的快取中执行这个操作，就像是 I→M 的转换一样，相当昂贵。对直写式快取而言，我们也得加上它将新的快取行内容写入到更高阶快取或主记忆体所花费的时间，进而提高了成本。

若是一个快取行处于共享状态，并且本地处理器要读取它，那么就不必改变状态，读取请求能够由这个快取来达成。若是快取行要在本地写入，也能够使用这个快取行，但状态会被改成修改。这也需要令其它处理器的所有可能的快取行副本被标为无效。因此，写入操作必须要透过一个 RFO 讯息发布给其它处理器。若是快取行被第二个处理器请求读取，那么什么也不必做。主记忆体包含了当前的资料，本地的状态也已经是共享了。在第二个处理器想要写入到快取行的情况下（RFO），就直接将快取行标为无效。不需要汇流排操作。

独占状态与共享状态大致相同，只有一个重大的不同：本地的写入操作不必发布到汇流排上。因为已经知道本地快取是唯一一个持有这个独有的快取行的了。这会是一个巨大的优势，所以处理器会试著令尽可能多的快取行维持在独占状态，而非共享。后者是在这种时刻，无法取得这个资讯的退而求其次。独占状态也能够在完全不引发功能问题的情况下被省去。唯一会变糟的只有效能，因为 E→M 转换比 S→M 转换要快得多了。

从这些状态转换的描述中，应该很清楚多处理器操作特有的成本在哪了。是的，填入快取仍旧昂贵，但现在我们也必须留意 RFO 讯息。每当必须发送这种讯息时，工作就会变慢。

有两种必须要 RFO 讯息的情况：

* 一条执行绪从一个处理器迁移到另一个，并且所有快取行都必须一起移动到新的处理器上。
* 一个快取行真的被两个不同的处理器所需要。

在多执行绪或多行程的程式中，总是有一些同步的需求；这种同步是使用记忆体实作的。所以有些有根据的 RFO 讯息。它们仍旧得尽可能地降低频率。不过，还有其他 RFO 讯息的来源。我们将会在第六节解释这些情况。快取一致性协定的讯息必须被分发给系统中的处理器。MESI 转换直到确定系统中的所有处理器都有机会回复讯息之前都不会发生。这表示一个回复能花上的最长可能时间决定了一致性协定的速度。可能会有汇流排上的冲突、NUMA 系统的等待时间会很长、而且突发的流量当然也会让事情变慢。这全都是专注在避免不必要流量的好理由。

还有一个与拥有多于一个处理器有关的问题。这个影响是与机器高度相关的，但原理上这个问题总是存在：FSB 是一个共享的资源。在大多数机器上，所有处理器会透过单一一条汇流排连结到记忆体控制器（见图 2.1）。假如单一个处理器能够占满汇流排（通常是这样），那么共享相同汇流排的二或四个处理器甚至会更加地限制每个处理器的可用频宽。

即使每个处理器都如图 2.2 一样，有它自己的、连结到记忆体控制器的汇流排，但仍旧有连结到记忆体模组的汇流排。通常这是唯一一条汇流排，而––即使在图 2.2 的扩充模型中––同时存取相同的记忆体模组将会限制频宽。

每个处理器都能拥有本地记忆体的 AMD 模型亦是如此。所有处理器确实能快速地共时存取它们的本地记忆体，尤其在使用整合式记忆体控制器的情况。但多执行绪与多行程程式––至少偶尔––必须存取相同的记忆体区域以进行同步。

共时是受可用于必要的同步实作的有限频宽所严重地限制的。程式需要被小心地设计，以将不同处理器与核心对相同记忆体位置的存取降到最小。接下来的量测将会显示这点、以及其它与多执行绪程式有关的快取影响。

**多线程存取**

为了确保大家理解在不同处理器上同时使用相同快取行所引入的问题的严重性，我们将会在这里多看到一些针对我们先前用过的相同程式的效能图表。不过，这次会同时执行多于一条执行绪。所要量测的是最快的执行绪的执行时间。这意味著完成所有执行绪的完整执行时间还会更长。使用的机器有四个处理器；测试使用至多四条执行绪。所有处理器共享连结到记忆体控制器的汇流排，而且仅有一条连结到记忆体模组的汇流排。

[![&#x5716; 3.19&#xFF1A;&#x5FAA;&#x5E8F;&#x8B80;&#x53D6;&#xFF0C;&#x591A;&#x689D;&#x57F7;&#x884C;&#x7DD2;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.19.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.19.png)

图 3.19：循序读取，多线程

图 3.19 显示了循序唯读存取 128 位元组项目的效能（在 64 位元机器上，NPAD=15）。对于单执行绪的曲线，我们能预期是条与图 3.11 相似的曲线。量测使用了一台不同的机器，所以实际的数字会有所不同。

这张图中重要的部分当然是执行多条执行绪时的行为。注意到在走访链结串列时，没有记忆体会被修改，亦无让执行绪保持同步的企图。尽管不必有 RFO 讯息、而且所有的快取行都能被共享，但我们看到当使用两条执行绪时，效能减低了高达 18%，而使用四条执行绪时则高达 34%。由于没有必须在处理器之间传输的快取行，因此变慢仅仅是由两个瓶颈中的一或二者所引起的：从处理器到记忆体控制的共享汇流排、以及从记忆体控制器到记忆体模组的汇流排。 Once the working set size is larger than the L3 cache in this machine all three threads will be prefetching new list elements. 即便只有两条执行绪，可用频宽也不足以线性延展（scale）（即，没有执行多条执行绪带来的损失）。

[![&#x5716; 3.20&#xFF1A;&#x5FAA;&#x5E8F; Increase&#xFF0C;&#x591A;&#x689D;&#x57F7;&#x884C;&#x7DD2;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.20.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.20.png)

图 3.20：循序 Increase，多线程

当我们修改记忆体时，情况变得更可怕了。图 3.20 显示了循序 Increase 测试的结果。这个图表的 Y 轴使用了对数尺度。所以，别被看似很小的差异给骗了。我们在执行两条执行绪的时候仍有大约 18% 的损失，而执行四条执行绪则是惊人的 93% 损失。这表示，在使用四条执行绪时，预取流量加上回写流量就把汇流排占得非常满了。

我们使用对数尺度来显示 L1d 范围的结果。能够看到的是，一旦执行了多于一条执行绪，L1d 基本上就没什么效果了。只有在 L1d 不足以容纳工作集的时候，单执行绪的存取时间才会超过 20 个周期。当执行了多条执行绪时，存取时间却立即就达到了––即便使用的是最小的工作集大小。

这里没有显示出问题的一个面向。这个特定的测试程式是难以量测的。即使测试修改了记忆体、而我们因此预期必定会有 RFO 讯息，但当使用了多于一条执行绪时，我们并没有在 L2 范围内看到更高的成本。程式必须要使用大量的记忆体，并且所有执行绪必须要平行地存取相同的记忆体。没有大量的同步––其会占据大多的执行时间––这是很难实现的。

[![&#x5716; 3.21&#xFF1A;&#x96A8;&#x6A5F; Addnextlast&#xFF0C;&#x591A;&#x689D;&#x57F7;&#x884C;&#x7DD2;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.21.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.21.png)

图 3.21：随机Addnextlast，多线程

最后在图 3.21，我们有 Addnextlast 测试以随机的方式存取记忆体的数据。提供这张图主要是为了显示出这些高得吓人的数字。现在在极端的状况下，处理一个单一的串列元素要花上大约 1,500 个周期。使用更多执行绪的情况还要更加严重。我们能使用一张表格来总结多条执行绪的效率。

| \#线程 | 循序读取 | 循序递增 | 随机增加 |
| :--- | :--- | :--- | :--- |
| 2 | 1.69 | 1.69 | 1.54 |
| 4 | 2.98 | 2.07 | 1.65 |

表 3.3：多线程的效率

表格显示了在图 3.19、3.20、与 3.21 中，多执行绪以最大工作集大小执行的效率。数据表示在使用二或四条执行绪处理最大的工作集大小时，测试程式可能达到的最佳加速。以两条执行绪而言，加速的理论极限为 2，对于四条执行绪而言为 4。两条执行绪的数据并没有那么糟。但对于四条执行绪，最后一个测试的数据显示了，几乎不值得扩展到超过两条执行绪。额外的获益是非常小的。如果我们以略为不同的方式表示图 3.21 的资料，我们便能更轻易地看出这点。

[![&#x5716; 3.22&#xFF1A;&#x7D93;&#x7531;&#x5E73;&#x884C;&#x5316;&#x7684;&#x52A0;&#x901F;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.22.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.22.png)

图 3.22：经由并行的加速

图 3.22 的曲线显示了加速因子––也就是相比于以单一执行绪执行的程式的相对效能。我们得忽略最小大小的情况，因为量测结果不够精确。以 L2 与 L3 快取的范围而言，我们能够看到我们确实达到了几乎是线性的加速。我们分别达到了差不多 2 与 4 倍速。但一旦 L3 快取不足以容纳工作集，数字就往下掉了。两条与四条执行绪的加速因子都掉到一样的值（见表 3.3 的第四行）。这是难以找到主机板有著超过四个全都使用同个记忆体控制器的 CPU 插槽的其中一个理由。有著更多处理器的机器必须要以不同的方式来做（见第五节）。

这些数字并不普遍。在某些情况下，甚至连能塞进最后一层快取的工作集都无法做到线性加速。事实上，这才是常态，因为执行绪通常并不若这个测试程式的例子一般解耦（decoupled）。另一方面，是可能运作在大工作集上，而仍旧拥有多于两条执行绪的优势的。不过，做到这点需要一些思考。我们会在第六节讨论一些方法。

**特例：超线程**

超执行绪（有时被称为对称多执行绪〔Symmetric Multi-Threading，SMT〕）由 CPU 实作，并且是个特例，因为个别执行绪无法真的同时执行。它们全都共享著暂存器集以外、几乎所有的处理资源。个别的核心与 CPU 仍然平行地运作，但实作在每颗核心上的执行绪会受到这个限制。理论上，每颗核心可以有许多执行绪，但是––到目前为止––Intel CPU 的每颗核心至多仅有两条执行绪。CPU 有时域多工（time-multiplex）执行绪的职责。不过单是如此并没太大意义。实际的优点是，当同时执行的超执行绪被延迟时，CPU 可以调度另一条超执行绪，并善用像是算数逻辑一类的可用资源。在大多情况下，这是由记忆体存取造成的延迟。

假如两条执行绪执行在一颗超执行绪核心上，那么只有在两条执行绪_合并的（combined）_执行时间小于单执行绪程式的执行时间时，程式才会比单执行绪程式还有效率。借由重叠经常重复发生的不同记忆体存取的等待时间，这是可能的。一个简单的计算显示了为了达到某程度的加速，快取命中率的最小需求。

一支程式的执行时间能够以一个仅有一层快取的简易模型来估算，如下（见 \[16\]）：

$$ T\_{\text{exe}} = N \[ \(1 - F\_{\text{mem}}\) T\_{\text{proc}} + F\_{\text{mem}} \(G\_{\text{hit}} T\_{\text{cache}} + \(1 - G\_{\text{hit}}\) T\_{\text{miss}}\) \] $$

变量的意义如下：

$$ \begin{aligned} N &= \text{指令數} \ F\_{\text{mem}} &= N \text{ 次中存取記憶體的比率} \ G\_{\text{hit}} &= \text{載入次數中命中快取的比率} \ T\_{\text{proc}} &= \text{每個指令的週期數} \ T\_{\text{cache}} &= \text{快取命中的週期數} \ T\_{\text{miss}} &= \text{快取錯失的週期數} \ T\_{\text{exe}} &= \text{程式執行時間} \end{aligned} $$

为了要让使用两条执行绪有任何意义，两条执行绪任一的执行时间都必须至多为单执行绪程式码的一半。在任一边的唯一变数为快取命中的数量。若是我们求解方程式，以得到令执行绪的执行不减慢超过 50% 以上所需的最小快取命中率，我们会得到图 3.23 的结果。

[![&#x5716; 3.23&#xFF1A;&#x52A0;&#x901F;&#x7684;&#x6700;&#x5C0F;&#x5FEB;&#x53D6;&#x547D;&#x4E2D;&#x7387;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.23.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.23.png)

图 3.23：加速的最小缓存命中率

输入––刻在 X 轴上––为单执行绪程式码的快取命中率 $$G_{\text{hit}}$$。Y 轴显示了多执行绪程式码的快取命中率。这个值永远不能高于单执行绪的命中率，不然单执行绪程式码也会使用这个改良的程式码。以单执行绪的命中率––在这个特定的情况下––低于 55% 而言，在所有情况下程式都能够因为使用执行绪而获益。由于快取错失，CPU 或多或少有足够的空闲来执行第二条超执行绪。

绿色的区域是目标。假如对执行绪而言的减慢小于 50%，且每条执行绪的工作量都减半，那么合并的执行时间就可能会小于单执行绪的执行时间。以用作模型的处理器（使用一个有著超执行绪的 P4 的数据）而言，一支命中率为 60% 的单执行绪程式，对双执行绪程式来说需要至少 10% 的命中率。这通常是做得到的。但若是单执行绪程式的命中率为 95%，那么多执行绪程式就需要至少 80% 的命中率。这更难了。尤其––这是使用超执行绪的问题––因为现在每条超执行绪可用的有效快取大小（这里是 L1d，在实际上 L2 也是如此）被砍半了。超执行绪都使用相同的快取来载入它们的资料。若是两条执行绪的工作集没有重叠，那么原始的 95% 命中率也会打对折，因而远低于所需要的 80%。

超执行绪因而只有在有限范围的情境中有用。单执行绪程式的快取命中率必须足够低，以在给定上面的等式、以及减小的快取大小时，新的命中率仍然满足要求。这时，也只有这时，才有任何使用超执行绪的意义。实际上结果是否比较快，取决于处理器是否足以能将一条执行绪的等待时间重叠在另一条执行绪的执行时间上。平行化程式码的间接成本必须被加到新的总执行时间上，这个额外成本经常无法忽视。

在 6.3.4 节，我们将会看到一种执行绪紧密合作、而通过共有快取的紧密耦合竟然是个优点的技术。这个技术能够用于多种情境，只要程式设计师乐于将时间与精力投入到扩展他们的程式码的话。

应该清楚的是，假如两条超执行绪执行了完全不同的程式码（也就是说，两条执行绪被 OS 如同单独的处理器一般对待，以执行个别的行程），快取大小固然会减半，这表示快取错失的显著攀升。除非快取足够大，不然这种 OS 排程的实行是有问题的。除非机器由行程组成的负载确实––经由它们的设计––能够获益于超执行绪，否则最好在电脑的 BIOS 把超执行绪关掉。

### 3.3.5 其他细节

到目前为止，我们已经讨论过由三个部分––标签、集合索引、以及快取行偏移量––组成的位址。但实际使用的位址是什么呢？所有有关的处理器现今都是将虚拟位址空间提供给行程，这代表有两种不同的位址：虚拟的以及实体的。

虚拟记忆体的问题是，它们不是唯一的。虚拟记忆体能够––随著时间––指涉到不同的实体记忆体位址。在不同行程中的相同位址也可能会指涉到不同的实体位址。所以使用实体记忆体位址永远比较好，对吧？

这里的问题是，在执行期间使用的虚拟记忆体必须在记忆体管理单元（Memory Management Unit，MMU）的帮助下转译成实体位址。这是个不单纯的操作。在执行一个指令的管线中，实体位址可能只有在之后的阶段才能取得。这表示快取逻辑必须非常快速地判定这个记忆体位置是否被快取了。若是能够使用虚拟位址，快取查询就能够早点在管线中进行，并且在快取命中的情况下，就能够取得记忆体内容了。结果是，管线能够隐藏更多记忆体存取的成本。

处理器设计者目前是使用虚拟位址来标记第一层级的快取。这些快取非常地小，而且清除也不会太费力。若是一个行程的分页表树（page table tree）改变了，至少必须局部地清理快取。假如处理器拥有能够指定被改变的虚拟位址范围的指令，就有可能避免一次完整的冲出。考虑到 L1i 与 L1d 快取的等待时间很短（~3 周期），使用虚拟位址几乎是强迫性的。

对于大一点的快取，包含 L2、L3、... 快取，是需要实体位址标记的。这些快取有比较长的等待时间，而虚拟→实体位址转译能够在时间内完成。因为这些快取比较大（即，当它们被冲出时，会损失大量的资讯），并且因为主记忆体存取的等待时间，重新填入它们会花上很久的时间，因此冲出它们的代价经常不小。

一般来说，应该没必要知道在那些快取中的位址管理的细节。它们无法改变，而且所有会影响效能的因子通常是应该避免、或者是与高成本相关联的东西。塞满快取容量是很糟的，而且若是多数被使用的快取行都落在同一组集合中，所有快取都会很快地碰到问题。后者能够以虚拟定址的快取来避免，但对于使用者层级的行程来说，要避免使用实体位址定址的快取是不可能的。或许唯一应该记住的细节是，可能的话，别在同个行程里将同个实体记忆体位置映射到两个以上的虚拟位址。

另一个对程式设计师来说满无趣的快取细节是快取的替换策略。大多快取会先逐出近期最少使用的（Least Recently Used，LRU）元素。以较大的关联度（由于增加了更多的核心，关联度可能会在接下来几年内进一步地增长）维持 LRU 清单会变得越来越昂贵，我们可能会看到被采用的不同策略。

至于快取的替换，一介程式设计师能做的不多。若是快取是使用实体位址的标签，就没有任何办法能找出虚拟位址与快取集之间的关联。所有逻辑分页的快取行可能都映射到相同的快取集，留著大量的快取不用。如果是这样的话，不让这太常发生就是 OS 的工作了。

随著虚拟化（virtualization）的出现，事情变得更加复杂。现在甚至不是 OS 拥有对实体记忆体指派的控制。而是虚拟机器监视器（Virtual Machine Monitor，VMM，又称 Hypervisor）有指派实体记忆体的职责。

一介程式设计师能做的最多就是 a\) 完全使用逻辑记忆体分页 b\) 使用尽可能大的分页大小，以尽可能地多样化实体位址。较大的分页大小也有其它好处，但这是另一个主题（见第四节）。

## 3.4 指令缓存

并非只有处理器用到的资料有被快取；处理器执行的指令也会被快取。然而，这个快取比起资料快取，问题少了许多。有几个理由：

* 执行的程式码的量取决于所需的程式码大小。程式码的大小一般视问题的复杂度而定。而问题的复杂度是固定的。
* 程式的资料管理是由程式设计师所设计的，而程式的指令通常是由编译器产生的。编译器撰写者知道产生良好程式的规则。
* 程式流程比起资料存取模式更加能够预测。现今的 CPU 非常擅于发现模式。这有助于预取。
* 程式码总是有相当好的空间与时间局部性。

有一些程式设计师应该遵循的规则，但这些主要都是如何使用工具的规则。我们将会在第六节讨论它们。这里我们仅讨论指令快取的技术细节。

自从 CPU 核心时脉急遽增加、以及快取（即使是第一层快取）与核心之间的速度差距成长以来，CPU 便以管线来设计了。这表示一条指令的执行会分阶段进行。一条指令会先被解码、接著准备参数、最后再执行它。这种管线能够非常长（以 Intel 的 Netburst 架构而言，&gt; 20 个阶段）。一条很长的管线意味著，若是管线延误了（即，通过它的指令流被中断了），它会花上一段时间才能恢复速度。管线拖延发生在––举例来说––下一条指令的位置无法被正确地预测、或者载入下一条指令花了太长时间（如，当它必须从记忆体读取的时候）的时候。

因此 CPU 设计者花费了大量的时间与晶片面积在分支预测上，以尽可能地降低管线延误发生的频率。

在 CISC 处理器上，解码阶段也会花上一些时间。x86 与 x86-64 处理器尤其受此影响。在最近几年，这些处理器因而不在 L1i 上快取指令的原始位元组序列，而是快取被解码的指令。在这种情况下的 L1i 被称作“追踪快取（trace cache）”。追踪快取令处理器能够在快取命中的情况下略过管线的前面几步，这在管线被延误时格外有效。

如同先前说过的，L2 的快取是包含程式码与资料的统一式快取。在这里，程式码显然是以位元组序列的形式被快取，而不是被解码过的。

为了达到最好的效能，只有一些与指令快取相关的规则：

1. 产生尽可能小的程式码。有些例外，像是为了使用管线的软体管线化（software pipelining）需要建立额外的程式码的时候、以及使用小程式码的间接成本太高之处。
2. 协助处理器做出好的预取决策。这能够通过程式布局或是显式的预取来做到。

这些规则通常由编译器的程式码产生（code generation）来强制执行。有一些程式设计师能做的事情，我们会在第六章讨论它们。

### 3.4.1 自我修改的程序

在电脑时代的早期，记忆体是很珍贵的。人们不遗馀力地减少程式的大小，以为程式资料腾出更多的空间。一个经常使用的技巧是，随著时间改变程式自身。偶尔仍旧会找到这种自我修改的程式码（Self Modifying Code，SMC），如今多半是为了效能因素、或者用在安全漏洞上。

一般来说应该避免 SMC。虽然它通常都被正确地执行，但有著并非如此的边界案例（boundary case），而且没有正确完成的话，它会产生效能问题。显然地，被改变的程式码无法维持在保存被解码指令的追踪快取中。但即使程式码完全（或者有时候）不会被执行，因而不会使用到追踪快取，处理器也可能会有问题。若是接下来的指令在它已经进入管线的期间被改变了，处理器就得丢掉大量的成果，然后从头开始。甚至有处理器的大多状态都必须被丢弃的情况。

最后，由于处理器假定––为了简化起见，而且因为这在 99.9999999% 的情况下都成立––程式码分页是不可修改的（immutable），所以 L1i 的实作不会采用 MESI 协定，而是一种简化的 SI 协定。这表示，若是侦测到了修改，就必须做出许多的悲观假设。

强烈地建议尽可能避免 SMC。记忆体不再是如此稀有的资源。最好是撰写各自的函式，而非根据特定的需求修改一个函式。或许有天 SMC 的支援能够是可选的，而我们就能够以这种方式侦测出尝试修改程式码的漏洞程式码（exploit code）。若是真的必须使用 SMC，写入操作应该要绕过快取，以免因为 L1i 所需的 L1d 的资料造成问题。关于这些指令的更多讯息，见 6.1 节。

在 Linux 上，识别出包含 SMC 的程式通常非常容易。使用正规工具链（toolchain）建构的话，所有程式的程式码都是防写的（write-protected）。程式设计师必须在连结期（link time）施展强大的魔法，以产生程式分页能够被写入的可执行档。当这种情况发生时，现代的 Intel x86 与 x86-64 处理器具有专门的、计算自我修改的程式码使用次数的效能计数器。有了这些计数器的帮助，非常轻易就能够识别有著 SMC 的程式，即使程式由于宽松的许可而成功执行。

## 3.5 缓存不命中的因素

我们已经看过，在记忆体存取没有命中快取时的成本一飞冲天。有时候这是无可避免的，而了解实际的成本、以及能做些什么来减轻问题是很重要的。

### 3.5.1 缓存和存储器带宽

为了更好地理解处理器的能力，我们要量测在最理想情况下的可用频宽。这个量测格外有趣，因为不同处理器版本的差异很大。这即是本节充满著数个不同机器数据的原因。量测效能的程式使用了 x86 与 x86-64 处理器的 SSE 指令以同时载入或储存 16 位元组。就如同我们的其它测试一样，工作集从 1kB 增加到 512MB，并量测每个周期能够载入或储存多少位元组。

[![&#x5716; 3.24&#xFF1A;Pentium 4 &#x7684;&#x983B;&#x5BEC;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.24.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.24.png)

图 3.24：Pentium 4 的带宽

图 3.24 显示了在一台 64 位元 Intel Netburst 处理器上的效能。对于能够塞进 L1d 的工作集大小，处理器每个周期能够读取完整的 16 位元组––即，每个周期执行一次载入指令（movaps 指令一次搬移 16 位元组）。测试不会对读取的资料做任何事，我们测试的仅有读取指令本身。一旦 L1d 不再足够，效能就立刻大幅地降低到每周期少于 6 位元组。在 218 的一步是因为 DTLB 快取的枯竭，表示每个新分页的额外工作。由于读取是循序的，预取能够完美地预测存取，并且对于所有工作集大小，FSB 能以大约每周期 5.3 位元组传输记忆体内容。不过，预取的资料不会传播到 L1d。这些当然是真实程式中永远无法达到的数字。将它们想成实际上的极限吧。

比起读取的效能，令人更为吃惊的是写入与复制的效能。写入的效能––即便对于很小的工作集大小––始终不会上升到每周期 4 位元组以上。这暗示著，在这些 Netburst 处理器上，Intel 为 L1d 选择使用了直写模式，其中的效能显然受限于 L2 的速度。这也代表复制测试––其从一个记忆体区域复制到第二个、不重叠的记忆体区域––的效能并没有显著地变差。所需的读取操作要快得多，并且能够与写入操作部分重叠。写入与复制量测中，最值得注意的细节是，当 L2 快取不再足够时的低效能。效能跌落到每周期 0.5 位元组！这表示写入操作比读取操作慢了十倍。这意味著，对于这个程式的效能而言，最佳化那些操作是更加重要的。

[![&#x5716; 3.25&#xFF1A;&#x6709;&#x8457; 2 &#x689D;&#x8D85;&#x57F7;&#x884C;&#x7DD2;&#x7684; P4 &#x983B;&#x5BEC;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.25.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.25.png)

图 3.25：有著着2条超线程的 P4 带宽

在图 3.25 中，我们看到在相同处理器上、但以两条执行绪执行的结果，每条执行绪各自归属于处理器的两条超执行绪的其中一条上。这张图表与前一张使用相同的刻度，以阐明两者的差异。曲线有些微抖动，仅是因为量测两条共时执行绪的问题。结果如同预期。由于超执行绪共享了暂存器以外的所有资源，每条执行绪都只有一半的快取与可用频宽。这表示，即使每条执行绪都必须等待很久、并能够将执行时间拨给另一条执行绪，这也没有造成任何不同，因为另一条执行绪也必须等待记忆体。这忠实地显示了使用超执行绪的最差情况。

[![&#x5716; 3.26&#xFF1A;Core 2 &#x7684;&#x983B;&#x5BEC;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.26.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.26.png)

图 3.26：Core 2 的带宽

[![&#x5716; 3.27&#xFF1A;&#x6709;&#x8457; 2 &#x689D;&#x8D85;&#x57F7;&#x884C;&#x7DD2;&#x7684; Core 2 &#x983B;&#x5BEC;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.27.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.27.png)

圖 3.27：有2 条超线程的 Core 2 带宽

对比图 3.24 与图 3.25，对于 Intel Core 2 处理器，图 3.26 与 3.27 的结果看起来差异甚大。这是一个有著共享 L2 的双核处理器，其 L2 是 P4 机器上的 L2 的四倍大。不过，这只解释了写入与复制效能延后下降的原因。

有另一个、更大的不同。整个工作集范围内的读取效能停留在大约是最佳的每周期 16 位元组。读取效能在 220 位元组之后的下降同样是因为工作集对 DTLB 来说太大了。达到这么高的数字代表处理器不仅能够预取资料、还及时传输了资料。这也代表资料被预取至 L1d 中。

写入与复制的效能也大为不同。处理器没有直写策略；写入的资料被储存在 L1d 中，而且仅会在必要时逐出。这使得写入速度接近于最佳的每周期 16 位元组。一旦 L1d 不再足够，效能便显著地降低。如同使用 Netburst 处理器的情况，写入的效能显著地降低了。由于读取效能很高，这里的差距甚至更大。事实上，当 L2 也不再足够时，速度差异甚至提升到了 20 倍！这不代表 Core 2 处理器表现得很糟。相反的，它们的效能一直都比 Netburst 核心还好。

在图 3.27 中，测试执行了两条执行绪，各自在 Core 2 处理器两颗核心的其中一颗上。两条执行绪都存取相同的记忆体，不过不需要完美地同步。读取效能的结果跟单执行绪的情况没什么不同。看得到稍微多一点的抖动，这在任何多执行绪的测试案例里都在预期之中。

有趣的一点是，对于能塞进 L1d 的工作集大小的写入与复制效能。如同图中能看到的，效能就像是资料必须从主记忆体读取一样。两条执行绪都争夺著相同的记忆体位置，因而必须送出快取行的 RFO 讯息。麻烦之处在于，即使两颗核心共享了快取，这些请求也不是以 L2 快取的速度处理。一旦 L1d 快取不再足够，被修改的项目会从每颗核心的 L1d 冲出到共享的 L2。这时，由于 L1d 的错失被 L2 快取所弥补、而且只有在资料还没被冲出时才需要 RFO 讯息，效能便显著地增加。这即是我们看到，对于这些工作集大小，速度降低了 50% 的原因。这个渐近行为如同预期一般：由于两颗核心共享了相同的 FSB，每颗核心会得到一半的 FSB 频宽，这表示对于大工作集而言，每条执行绪的效能大约是单执行绪时的一半。

[![&#x5716; 3.28&#xFF1A;AMD 10h &#x5BB6;&#x65CF; Opteron &#x7684;&#x983B;&#x5BEC;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.28.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.28.png)

图 3.28：AMD 10h 家族 Opteron 的带宽

即使同个供应商的处理器版本之间都有显著的差异，所以当然也值得看看其它供应商的处理器效能。图 3.28 显示了一个 AMD 10h 家族 Opteron 处理器的效能。这个处理器拥有 64kB L1d、512kB L2、以及 2MB 的 L3。L3 快取被处理器的所有核心所共享。效能测试的结果能在图 3.28 看到。

注意到的第一个关于数字的细节是，假如 L1d 快取足够的话，处理器每个周期能够处理两条指令。读取效能超过了每周期 32 位元组，甚至连写入效能都很高––每周期 18.7 位元组。不过，读取的曲线立刻就掉了下去，而且非常低––每周期 2.3 位元组。对于这个测试，处理器没有预取任何资料，至少不怎么有效。

另一方面，写入曲线的表现则取决于不同快取的大小。在 L1d 全满时达到效能高峰，于 L2 降到 6 位元组，于 L3 降到 2.8 位元组，最后在连 L3 也无法容纳所有资料时，降到每周期 .5 位元组。在 L1d 快取时的效能超越了（较旧的）Core 2 处理器，L2 存取一样快（因为 Core 2 有个比较大的快取），而 L3 与主记忆体存取则比较慢。

复制的效能无法比读取或写入的效能还要来得好。这即是我们看到，这条曲线起初被压在读取效能下面、而后又被压在写入效能下面的原因。

[![&#x5716; 3.29&#xFF1A;&#x6709;&#x8457; 2 &#x689D;&#x8D85;&#x57F7;&#x884C;&#x7DD2;&#x7684; AMD 10h &#x5BB6;&#x65CF;&#x7684;&#x983B;&#x5BEC;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.29.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.29.png)

圖 3.29：有2条超线程的 AMD 10h 家族的带宽

Opteron 处理器的多执行绪效能显示于图 3.29。读取效能基本上不受影响。每条执行绪的 L1d 与 L2 如先前一般运作，而在这个例子下的 L3 快取也没预取得很好。两条执行绪没有因其目的而过分地压榨 L3。在这个测试中的大问题是写入的效能。所有执行绪共享的资料都得通过 L3 快取。这种共享看起来非常没有效率，因为即使 L3 快取的大小足以容纳整个工作集，成本也远大于一次 L3 存取。将这张图与图 3.27 相比，我们看到在适当的工作集大小范围中，Core 2 处理器的两条执行绪是以共享的 L2 快取的速度来运作的。对于 Opteron 处理器，这种效能水平只有在一个非常小范围的工作集大小内才能达到，而即使在这里，它也只能接近 L3 的速度，比 Core 2 的 L2 还慢。

### 3.5.2 关键字的载入

记忆体以比快取行大小还小的区块从主记忆体传输到快取中。现今是一次传输 64 位元，而快取行的大小为 64 或 128 位元组。这表示每个快取行需要 8 或 16 次传输。

DRAM 晶片能够以突发（burst）模式传输那些 64 位元组的区块。这能够在没有来自记忆体控制器的额外命令、以及可能伴随的延迟的情况下填满快取行。若是处理器预取了快取行，这可能是最好的操作方式。

若是一支程式的资料或快取存取没有命中（这表示，这是个强制性快取错失〔compulsory cache miss〕––因为资料是第一次使用、或者是容量性快取错失〔capacity cache miss〕––因为受限的快取大小需要逐出快取行），情况便不同了。程式继续执行所需的快取行里头的字组也许不是快取行中的第一个字组。即使在突发模式下、并以双倍资料速率来传输，个别的 64 位元区块也会在明显不同的时间点抵达。每个区块会在前一个区块抵达之后 4 个 CPU 周期以上抵达。若是程式继续执行所需的字组是快取行的第八个，程式就必须在第一个字组抵达之后，等待额外的 30 个周期以上。

事情并不必然非得如此。记忆体控制器能够以不同的顺序随意请求快取行的字组。处理器能够传达程式正在等待哪个字组––即关键字组，而记忆体控制器能够先请求这个字组。一旦这个字组抵达了，程式便能够在快取行其馀部分抵达、并且快取还不在一致状态的期间继续执行。这个技术被称为关键字组优先与提早重新启动（Critical Word First & Early Restart）。

现今的处理器实作了这项技术，但有些不可能达成的情况。若是处理器预取了资料，并且关键字组是未知的。万一处理器在预取操作的途中请求这个快取行，就必须在不能够影响顺序的情况下，一直等到关键字组抵达为止。

[![&#x5716; 3.30&#xFF1A;&#x5728;&#x5FEB;&#x53D6;&#x884C;&#x672B;&#x7AEF;&#x7684;&#x95DC;&#x9375;&#x5B57;&#x7D44;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.30.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.30.png)

圖 3.30：在缓存行末端的关键字组

即使在适当的地方有了这些最佳化，关键字组在快取行的位置也很重要。图 3.30 显示了循序与随机存取的 Follow 测试结果。显示的是以用来巡访的指标位在第一个字组来执行测试，对比指标位在最后一个字组的情况下的速度减慢的结果。元素大小为 64 位元组，与快取行的大小一致。数字受到许多杂讯干扰，但能够看到，一旦 L2 不再足以持有工作集大小，关键字组在末端时的效能立刻就慢了约 0.7%。循序存取似乎受到了多一点影响。这与前面提及的、预取下个快取行时的问题一致。

### 3.5.3 缓存的配置

快取在与超执行绪、核心、和处理器的关系中的位置并不在程式设计师的控制之下。但程式设计师能够决定执行绪要在何处执行，于是快取如何与使用的 CPU 共处就变得很重要了。

这里我们不会深入在何时选择哪颗核心来执行执行绪的细节。我们只会描述在设置执行绪的亲和性（affinity）时，程式设计师必须要考虑的架构细节。

超执行绪，根据定义，共享了暂存器集以外的所有东西。这包含了 L1 快取。这里没什么好说的。有趣之处从一个处理器的个别核心开始。每颗核心至少拥有它自己的 L1 快取。除此之外，现今共有的细节并不多：

* 早期的多核处理器完全不共享快取。
* 之后的 Intel 模型的双核处理器拥有共享的 L2 快取。对于四核处理器，我们必须为由两颗核心组成的每一对处理个别的 L2 快取。没有更高层级的快取了。
* AMD 的 10h 处理器家族拥有独立的 L2 快取与一个统一式 L3 快取。

在处理器供应商的宣传品中已经写了许多关于它们各自的模型的优点。若是由核心处理的工作集并不重叠，拥有不共享的快取就有一些优势。这对于单执行绪程式而言非常有用。由于这仍经常是当下的真实情况，因此这种做法并不怎么差。但总是会有一些重叠的。快取都包含通用执行期函式库（runtime library）中最活跃使用的部分，代表有一些快取空间会被浪费。

与 Intel 的双核处理器一样完全共享 L1 以外的所有快取有个大优点。若是在两颗核心上的执行绪工作集有大量的重叠，可用的快取记忆体总量也会增加，工作集也能够更大而不致降低效能。若是工作集没有重叠，Intel 的进阶智慧型快取（Advanced Smart Cache）管理应该要防止任何一颗核心独占整个快取。

不过，如果两颗核心为了它们各自的工作集使用了大约一半的快取，也会有一些冲突。快取必须不断地掂量两颗核心的快取使用量，而作为这个重新平衡的一部分而执行的逐出操作可能会选得很差。为了看到这个问题，让我们看看另一个测试程式的结果。

测试程式拥有一个不断––使用 SSE 指令––读取或写入一个 2MB 记忆体区块的行程。选择 2MB 是因为这是这个 Core 2 处理器的 L2 快取大小的一半。行程被钉在一颗核心上，而第二个行程则被钉在另一颗核心上。第二个行程读写一块可变大小的记忆体区域。图表显示了每周期被读取或写入的位元组数。显示了四条不同的曲线，每条代表一种行程读取与写入的组合。 The read/write graph is for the background process, which always uses a 2MB working set to write, and the measured process with variable working set to read.

[![&#x5716; 3.31&#xFF1A;&#x5169;&#x500B;&#x884C;&#x7A0B;&#x7684;&#x983B;&#x5BEC;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.31.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.31.png)

图 3.31：两个进程的带宽

这张图有趣的部分在于 220 与 223 位元组之间。若是两颗核心的 L2 快取完全分离，我们能够预期四个测试的效能全都会在 221 与 222 之间––这表示，L2 快取耗尽的时候––往下掉。如同我们能在图 3.31 中看到的，情况并非如此。以在背景行程写入的情况而言，这是最明显的。效能在工作集大小达到 1MB 之前就开始下降。两个行程没有共享记忆体，因此行程也不会导致 RFO 讯息被产生。这纯粹是逐出的问题。智慧型快取管理有它的问题，导致感觉到的快取大小比起每颗核心可用的 2MB，更接近于 1MB。只能期望，若是在核心之间共享的快取依旧是未来处理器的特征的话，智慧型快取管理所使用的演算法会被修正。

有一个拥有两个 L2 快取的四核处理器仅是能够引入更高层级快取之前的权宜之计。比起独立的插槽与双核处理器，这个设计并没有什么显著的效能优势。两颗核心透过在外部被视为 FSB 的相同的汇流排沟通。没有什么特别快的资料交换。

针对多核处理器的快取设计的未来将会有更多的层级。AMD 的 10h 处理器家族起了个头。我们是否会继续看到被一个处理器核心的一个子集所共享的更低层级的快取仍有待观察（在 2008 年处理器的世代中，L2 快取没有被共享）。额外的快取层级是必要的，因为高速与频繁使用的快取无法被多颗核心所共享。效能会受到影响。也会需要非常大的高关联度快取。快取大小以及关联度两者都必须随著共享快取的核心数量而增长。使用一个大的 L3 快取以及合理大小的 L2 快取是个适当的权衡。L3 快取较慢，但它理想上并不如 L2 快取一样常被使用。

对程式设计师而言，所有这些不同的设计都代表进行排程决策时的复杂性。为了达到最好的效能，必须知道工作负载以及机器架构的细节。幸运的是，我们拥有确定机器架构的依据。这些介面会在之后的章节中介绍。

### 3.5.4 FSB的影响

[![&#x5716; 3.32&#xFF1A;FSB &#x901F;&#x5EA6;&#x7684;&#x5F71;&#x97FF;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-3.32.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-3.32.png)

图 3.32：FSB 速度的影响

FSB 在机器的效能中扮演一个重要的角色。快取内容只能以跟记忆体的连线所允许的一样快地被储存与写入。我们能够借由在两台仅在记忆体模组速度上有差异的机器上执行一支程式，来看看到底怎么样。图 3.32 显示了以一台 64 位元机器、NPAD=7 而言，Addnext0 测试（将下一个元素的 pad\[0\] 元素加到自己的 pad\[0\] 元素上）的结果。两台机器都拥有 Intel Core 2 处理器，第一台使用 667MHz DDR2 模组，第二台则是 800MHz 模组（提升了 20%）。

数据显示，当 FSB 真的受很大的工作集大小所压迫时，我们的确看到了巨大的优势。在这项测试中，量测到的最大效能提升为 18.2%，接近理论最大值。这表示，更快的 FSB 确实能够省下大量的时间。当工作集大小能塞入快取时（这些处理器有一个 4MB L2），这并不重要。必须记在心上的是，这里我们测量的是一支程式。一个系统的工作集包含所有同时执行的行程所需的记忆体。如此一来，以小得多的程式就可能轻易超过 4MB 以上的记忆体。

现今，一些 Intel 的处理器支援加速到 1,333MHz 的 FSB，这可能代表著额外 60% 的提升。未来将会看到更高的速度。若是速度很重要、并且工作集大小更大了，肯定是值得投资金钱在快速的 RAM 与很高的 FSB 速度的。不过必须小心，因为即使处理器可能会支援更高的 FSB 速度，但主机板／北桥可能不会。检查规格是至关重要的。



