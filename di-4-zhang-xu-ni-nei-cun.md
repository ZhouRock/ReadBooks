# 第4章：虚拟内存

一個處理器的虛擬記憶體（virtual memory，VM）子系統實作了提供給每個行程的虛擬位址空間。這令每個行程都認為它是獨自在系統中的。虛擬記憶體的優點清單會在其它地方仔細地描述，所以這裡就不重複這些了。這一節會聚焦在虛擬記憶體子系統的實際的實作細節、以及與此相關的成本。

虛擬位址空間是由 CPU 的記憶體管理單元（Memory Management Unit，MMU）實作的。OS 必須填寫分頁表（page table）資料結構，但大多數 CPU 會自行做掉剩下的工作。這真的是個非常複雜的機制；理解它的最佳方式是引入使用的資料結構來描述虛擬位址空間。

由 MMU 實行的位址轉譯的輸入為一個虛擬位址。它的值通常有極少量––如果有的話––的限制。在 32 位元系統上的虛擬位址為 32 位元的值，而在 64 位元系統上為 64 位元的值。在某些系統上，像是 x86 與 x86-64，使用的位址實際上牽涉到另一層級的間接性：這些架構使用了分段（segment），其只不過是將偏移量加到每個邏輯位址上。我們可以忽略位址產生過程的這個部分，它很瑣碎，而且就記憶體管理的效能而言，不是程式設計師必須要關心的東西。\[^24\]

\[^24\]: x86 上的分段限制是攸關效能的，但這又是另一個故事了。

## 4.1 最简单的地址翻译

有趣的部分是虛擬位址到實體位址的轉譯。MMU 能夠逐個分頁重新映射位址。就如同定址快取行的時候一樣，虛擬位址會被切成多個部分。這些部分用來索引多個用以建構最終實體位址的表格。以最簡單的模型而言，我們僅有一個層級的表格。

[![&#x5716; 4.1&#xFF1A;&#x4E00;&#x5C64;&#x4F4D;&#x5740;&#x8F49;&#x8B6F;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-4.1.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-4.1.png)圖 4.1：一層位址轉譯

圖 4.1 顯示了到底是怎麼使用虛擬位址的不同部分的。開頭的部分用以選擇一個分頁目錄（Page Directory）中的一個項目；在這個目錄中的每個項目都能由 OS 個別設定。分頁目錄項目決定了一個實體記憶體分頁的位址；在分頁目錄中，能夠有多於一個指到相同實體位址的項目。記憶單元的完整實體位址是由分頁目錄的分頁位址、結合虛擬位址的低位元所決定的。分頁目錄項目也包含一些像是存取權限這類關於分頁的額外資訊。

分頁目錄的資料結構儲存於主記憶體中。OS 必須分配連續的實體記憶體、並將這個記憶體區域的基底位址（base address）儲存在一個特殊的暫存器中。虛擬記憶體中適當的位元量接著會被用作一個分頁目錄的索引––它實際上是一個目錄項目的陣列。

作為一個實際的例子，以下是在 x86 機器上的 4MB 分頁所使用的佈局。虛擬記憶體的偏移量部分的大小為 22 位元，足以定址一個 4MB 分頁中的每個位元組。虛擬記憶體剩餘的 10 位元選擇了分頁目錄裡 1024 個項目中的其中一個。每個項目包含一個 4MB 分頁的一個 10 位元的基底位址，其會與偏移量結合以構成完整的 32 位元位址。

## 4.2 多层级分页表

4MB 的分頁並非常態，它們會浪費很多的記憶體，因為 OS 必須執行的許多操作都需要與記憶體分頁對齊（align）。以 4kB 分頁而言（32 位元機器、甚至經常是 64 位元機器上的常態），虛擬位址的偏移量部分的大小僅有 12 位元。這留了 20 位元作為分頁目錄的選擇器。一個有著 220 個項目的表格是不切實際的。即使每個項目只會有 4 位元組，表格大小也會有 4MB。由於每個行程都可能擁有它自己獨有的分頁目錄，這些分頁目錄會佔據系統中大量的實體記憶體。

解決方法是使用多個層級的分頁表。階層於是形成一個巨大、稀疏的分頁目錄；沒有真的用到的位址空間範圍不需要被分配的記憶體。這種表示法因而緊密得多了，使得記憶體中能夠擁有許多行程的分頁表，而不會太過於影響效能。

[![&#x5716; 4.2&#xFF1A;&#x56DB;&#x5C64;&#x4F4D;&#x5740;&#x8F49;&#x8B6F;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-4.2.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-4.2.png)圖 4.2：四層位址轉譯

現今最複雜的分頁表結構由四個層級所構成。圖 4.2 顯示了這種實作的示意圖。虛擬記憶體––在這個例子中––被切成至少五個部分。其中四個部分為不同目錄的索引。第四層目錄被 CPU 中一種特殊用途的暫存器所指涉。第二層到第四層目錄的內容為指向更低層級目錄的參考。若是一個目錄項目被標記為空，它顯然不需要指到任何更低層的目錄。如此一來，分頁表樹便能夠稀疏且緊密。第一層目錄的項目為––就像在圖 4.1 一樣––部份的實體位址，加上像存取權限這類輔助資料。

要確定對應到一個虛擬位址的實體位址，處理器首先會確定最高層目錄的位址。這個位址通常儲存在一個暫存器中。CPU 接著取出對應到這個目錄的虛擬記憶體的索引部分，並使用這個索引來挑選合適的項目。這個項目是下一個目錄的位址，使用虛擬位址的下一個部分來索引。這個過程持續到抵達第一層目錄，這時目錄項目的值為實體位址的高位部分。加上來自虛擬記憶體的分頁偏移位元便組成了完整的實體位址。這個過程被稱為分頁樹走訪（page tree walking）。有些處理器（像是 x86 與 x86-64）會在硬體中執行這個操作，其它的則需要來自 OS 的協助。

每個在系統中執行的行程會需要它自己的分頁表樹。部分地共享樹是可能的，但不如說這是個例外狀況。因此，如果分頁表樹所需的記憶體盡可能地小的話，對效能與延展性而言都是有益的。理想的情況是將用到的記憶體彼此靠近地擺在虛擬位址空間中；實際用到的實體位址則無關緊要。對一支小程式而言，僅僅使用在第二、三、四層各自的一個目錄、以及少許第一層目錄，可能還過得去。在有著 4kB 分頁以及每目錄 512 個項目的 x86-64 上，這能夠以總計 4 個目錄（每層一個）來定址 2MB。1GB 的連續記憶體能夠以一個第二到第四層目錄、以及 512 個第一層目錄來定址。

不過，假設能夠連續地分配所有記憶體也太過於簡化了。為了彈性起見，一個行程的堆疊（stack）與堆積（heap）區域––在大多情況下––會被分配在位址空間中極為相對的兩端。這令兩個區域在需要時都能盡可能地增長。這表示，最有可能是兩個需要的第二層目錄，以及與此相應的、更多低層級的目錄。

但即使如此也不總是符合當前的實際狀況。為了安全考量，一個可執行程式的多個部分（程式碼、資料、堆積、堆疊、動態共享物件〔Dynamic Shared Object，DSO〕，又稱共享函式庫〔shared library〕）會被映射在隨機化的位址上 \[9\]。隨機化擴大了不同部份的相對位置；這暗示著，在一個行程裡使用中的不同記憶體區域會廣泛地散布在虛擬位址空間中。藉由在隨機化的位址的位元數上施加一些限制也能夠限制範圍，但這無疑––在大多情況下––會讓一個行程無法以僅僅一或兩個第二與第三層目錄來執行。

若是效能比起安全性真的重要太多了，也能夠把隨機化關閉。於是 OS 通常至少會在虛擬記憶體中連續地載入所有的 DSO。

## 4.3 分页存取的优化

分頁表的所有資料結構都會被保存在主記憶體中；OS 就是在這裡建構並更新表格的。在一個行程的創建、或是分頁表的一次修改之後，都會立即通知 CPU。分頁表是用以將每個虛擬位址，使用上述的分頁表走訪來轉成實體位址。更準確地說：每一層至少會有一個目錄會被用在轉換一個虛擬位址的過程中。這需要高達四次記憶體存取（以執行中行程的一個單一存取而言），這很慢。將這些目錄表的項目視為普通的資料、並在 L1d、L2、等等快取它們是辦得到的，但這可能還是太慢了。

從最早期的虛擬記憶體開始，CPU 設計者便已採用了一種不同的最佳化。一個簡單的計算能夠顯示，僅將目錄表的項目保存在 L1d 以及更高層級的快取中會招致可怕的效能。每個獨立的位址計算會需要相符於分頁表深度的若干 L1d 存取。這些存取無法平行化，因為它們都依賴於前一次查詢的結果。單是這樣就會––在一台有著四個分頁表階層的機器上––需要至少 12 個週期。再加上 L1d 錯失的機率，結果是指令管道無法隱藏任何東西。額外的 L1d 存取也需要將寶貴的頻寬偷到快取去。

所以，不只是將目錄表的項目快取起來，而是連實體分頁位址的完整計算結果也會被快取。跟程式碼與資料快取行得通的理由相同，這種快取的位址計算結果是很有效的。由於虛擬位址的分頁偏移量的部分不會參與到實體分頁位址的計算，僅有虛擬位址的剩餘部分會用來作為快取的標籤。視分頁大小而定，這代表數百或數千條的指令或資料物件會共享相同的標籤，因而共享相同的實體位址前綴（prefix）。

儲存計算得來的值的快取被稱為轉譯後備緩衝區（Translation Look-Aside Buffer，TLB）。它通常是個很小的快取，因為它必須非常快。現代的 CPU 提供了多層 TLB 快取，就如同其他快取一樣；更高層的快取更大也更慢。L1TLB 的小容量通常藉由令快取為全關聯式、加上 LRU 逐出策略來彌補。近來，這種快取的大小已經持續成長，並且––在進行中––被轉變為集合關聯式。因此，當一個新的項目必須被加入時，被逐出並取代的項目可能不是最舊的一個。

如同上面所註記的，用來存取 TLB 的標籤為虛擬位址的一部分。若是在快取中有比對到標籤，最終的實體位址就能夠藉由將來自虛擬位址的分頁偏移量加到被快取的值上而計算出來。這是個非常快的過程；它必須如此，因為實體位址必須可用於每條使用獨立位址的指令、以及––在某些情況下––使用實體位址作為鍵值（key）的 L2 查詢。若是 TLB 查詢沒有命中，處理器必須要進行一次分頁表走訪；這可能是非常昂貴的。

透過軟體或硬體預取程式碼或資料時，若是位址在另一個分頁上，能夠暗自預取 TLB 的項目。這對於硬體預取而言是不可能的，因為硬體可能會引發無效的分頁表走訪。程式設計師因而無法仰賴硬體預取來預取 TLB 項目。必須明確地使用預取指令來達成。TLB––就像是資料與指令快取––能夠出現在多個層級。就如同資料快取一樣，TLB 通常有兩種：指令 TLB（ITLB）以及資料 TLB（DTLB）。像是 L2TLB 這種更高層級的 TLB 通常是統一式的，與其它快取的情況相同。

### 4.3.1 使用TLB的预警

TLB 是個處理器核心的全域（global）資源。所有執行在處理器核心的執行緒與行程都使用相同的 TLB。由於虛擬到實體位址的轉譯是看設置的是哪一個分頁表樹而定的，因此若是分頁表被更改了，CPU 就不能盲目地重複使用快取的項目。每個行程有個不同的分頁表樹（但同個行程中的執行緒並非如此）。假如有的話，系統核心與 VMM（虛擬機器監視器）亦是如此。一個行程的位址空間佈局也是可能改變的。有兩種處理這個問題的方式：

* 每當分頁表樹被更改都沖出 TLB。
* 擴充 TLB 項目的標籤，以額外且唯一地識別它們所指涉到的分頁表樹。

在第一種情況中，每當情境切換（context switch）都會沖出 TLB。由於––在大多 OS 中––從一個執行緒／行程切換到另一個時，需要執行一些系統核心的程式碼，TLB 沖出會被限制在離開（有時候是進入）系統核心位址空間時。在虛擬化的系統上，當系統核心必須呼叫 VMM、並在返回的途中時，這也會發生。若是系統核心和／或 VMM 不必使用虛擬位址、或是能夠重複使用與發出系統／VMM 呼叫的行程或系統核心相同的虛擬位址（即，位址空間被重疊了），TLB 只須在––離開系統核心或 VMM 後––處理器恢復一個不同的行程或系統核心的執行時沖出。

沖出 TLB 有效但昂貴。舉例來說，在執行一個系統呼叫時，系統核心程式可能會被限制在數千行觸及––或許––少數的新分頁（或者一個大分頁，如同在某些架構上的 Linux 的情況）的指令。這個操作僅會取代與被觸及的分頁一樣多的 TLB 項目。以 Intel 的 Core2 架構、附加它的 128 ITLB 與 256 DTLB 的項目而言，一次完整的沖出可能意味著被不必要地沖出的項目（分別）會超過 100 與 200 個。當系統呼叫返回（return）到相同的行程時，所有那些被沖出的 TLB 項目都能夠被再次用到，但它們將會被丟掉。對於在系統核心或 VMM 中經常用到的程式碼亦是如此。儘管系統核心以及 VMM 的分頁表通常不會改變，因此 TLB 項目––理論上––能夠被保存相當長的一段時間，但在每次進入系統核心時，TLB 也必須從零開始填入。這也解釋了為何現今處理器中的 TLB 快取並沒有更大的原因：程式的執行時間非常可能不會長到足以填入這所有的項目。

這個事實––當然––不會逃出 CPU 架構師的掌心。最佳化快取沖出的一個可能性是，單獨令 TLB 項目失效。舉例來說，若是系統核心與資料落在一個特殊的位址範圍，那麼僅有落在這個位址範圍的分頁必須從 TLB 逐出。這只需要比對標籤，因而不怎麼昂貴。這個方法在位址空間的一部分––例如，透過一次 `munmap` 呼叫––被更改的情況下也是有用的。

一個好得多的解法是擴充用來 TLB 存取的標籤。若是––除了虛擬位址的部分以外––為每個分頁表樹（即，一個行程的位址空間）加上一個唯一的識別子（identifier），TLB 根本就不必完全沖出。系統核心、VMM、以及獨立的行程全都能夠擁有唯一的識別子。採用這個方案的唯一議題是，可用於 TLB 標籤的位元數量會被嚴重地限制，而位址空間的數量則否。這表示是有必要重複使用某些識別子的。當這種情況發生時，TLB 必須被部分沖出（如果可能的話）。所有帶著被重複使用的識別子的項目都必須被沖出，但這––但願如此––是個非常小的集合。

當多個行程執行在系統中時，這種擴充的 TLB 標記在虛擬化的範圍之外是有優勢的。假如每個可執行行程的記憶體使用（是故 TLB 項目的使用）受限了，有個好機會是，當一個行程再次被排程時，它最近使用的 TLB 項目仍然在 TLB 中。但還有兩個額外的優點：

1. 特殊的位址空間––像是那些被系統核心或 VMM 所用到的––通常只會被進入一段很短的時間；後續的控制經常是返回到啟動這次進入的位址空間。沒有標籤的話，便會執行一或兩次 TLB 沖出。有標籤的話，呼叫位址空間的快取轉譯會被保留，而且––由於系統核心與 VMM 位址空間根本不常更改 TLB 項目––來自前一次系統呼叫的轉譯等仍然可以被使用。
2. 當在兩條相同行程的執行緒之間切換時，根本不需要 TLB 沖出。不過，沒有擴充的 TLB 標籤的話，進入系統核心就會銷毀第一條執行緒的項目。

某些處理器已經––一段時間了––實作了這些擴充標籤。AMD 以 Pacifica 虛擬化擴充引入了一種 1 位元的標籤擴充。這個 1 位元位址空間 ID（Address Space ID，ASID）是––在虛擬化的情境中––用以從客戶域（guest domain）的位址空間區別出 VMM 的位址空間。這使得 OS 得以避免在每次進入 VMM（舉例來說，處理一個分頁錯誤〔page fault〕）時沖出客戶端的 TLB 項目、或者在返回客戶端時沖出 VMM 的 TLB 項目。這個架構未來將會允許使用更多的位元。其它主流處理器可能也會遵循這套方法並支援這個功能。

### 4.3.2 影响TLB效能

有幾個影響 TLB 效能的因素。第一個是分頁的大小。顯然地，分頁越大、會被塞進去的指令或資料物件也越多。所以一個比較大的快取大小減少了所需位址轉譯的整體數量，代表 TLB 快取中需要更少的項目。大部分架構現今允許使用多種不同的分頁大小；有些大小能夠並存地使用。舉例來說，x86／x86-64 處理器擁有尋常的 4kB 分頁大小，但它們也分別能夠使用 4MB 與 2MB 的分頁。IA-64 與 PowerPC 允許像是 64kB 的大小作為基礎分頁大小。

不過，大分頁尺寸的使用也隨之帶來了一些問題。為了大分頁而使用的記憶體區域在實體記憶體中必須是連續的。若是實體記憶體管理的單位大小被提高到虛擬記憶體分頁的大小，浪費的記憶體總量就會增加。各種記憶體操作（像是載入可執行程式）需要對齊到分頁邊界。這表示，平均而言，在每次映射的實體記憶體中，每次映射浪費了一半的分頁大小。這種浪費能夠輕易地累加；這因此對實體記憶體分配的合理單位大小加了個上限。

將單位大小提升到 2MB，以容納 x86-64 上的大分頁無疑並不實際。這個大小太大了。但這又意味著每個大分頁必須由多個較小的分頁所構成。而且這些小分頁在_實體_記憶體中必須是連續的。以 4kB 的單位分頁大小分配 2MB 的連續實體記憶體具有挑戰性。這需要尋找一個有著 512 個連續分頁的空閒區域。在系統執行一段時間、並且實體記憶體變得片段之後，這可能極端困難（或者不可能）。

因此在 Linux 上，有必要在系統啟動的時候使用特殊的 `hugetlbfs` 檔案系統來分配這些大分頁。一個固定數量的實體分頁會被保留來專門作為大虛擬分頁來使用。這綁住了可能不會一直用到的資源。這也是個有限的池（pool）；增加它通常代表著重新啟動系統。儘管如此，在效能貴重、資源充足、且麻煩的設置不是個大阻礙的情況下，龐大的分頁便為大勢所趨。資料庫伺服器就是個例子。

```text
$ eu-readelf -l /bin/ls
Program Headers:
  Type   Offset   VirtAddr           PhysAddr           FileSiz  MemSiz   Flg Align
...
  LOAD   0x000000 0x0000000000400000 0x0000000000400000 0x0132ac 0x0132ac R E 0x200000
  LOAD   0x0132b0 0x00000000006132b0 0x00000000006132b0 0x001a71 0x001a71 RW  0x200000
...
```

圖 4.3：ELF 程式標頭指示了對齊需求

提高最小的虛擬分頁大小（對比於可選的大分頁）也有它的問題。記憶體映射操作（例如，載入應用程式）必須遵循這些分頁大小。不可能有更小的映射。一個可執行程式不同部分的位置––對大多架構而言––有個固定的關係。若是分頁大小增加到超過在可執行程式或者 DSO 創建時所考慮的大小時，就無法執行載入操作。將這個限制記在心上是很重要的。圖 4.3 顯示了能夠如何決定一個 ELF 二進位資料（binary）的對齊需求的。它被編碼在 ELF 的程式標頭（header）。在這個例子中，一個 x86-64 的二進位資料，值為 $$ 200000\_{16} = 2,097,152 = \text{2MB} $$，與處理器所支援的最大分頁大小相符。

使用較大的分頁大小有個次要的影響：分頁表樹的層級數量會被減低。由於對應到分頁偏移量的虛擬位址部分增加了，就沒有剩下那麼多需要透過分頁目錄處理的位元了。這表示，在一次 TLB 錯失的情況下，必須完成的工作總量減少了。

除了使用大分頁尺寸外，也可能藉由將同時用到的資料搬移到較少的分頁上，以減少所需的 TLB 項目數量。這類似於我們先前討論的針對快取使用的一些最佳化。 Only now the alignment required is large. 考慮到 TLB 項目的數量非常少，這會是個重要的最佳化。

## 4.4 虚拟化的影响

OS 映像（image）的虛擬化會變得越來越流行；這表示記憶體管理的另一層會被加到整體中。行程（基本上為監獄〔jail〕）或 OS 容器（container）的虛擬化並不屬於這個範疇，因為只有一個 OS 會牽涉其中。像 Xen 或 KVM 這類技術能夠––無論有沒有來自處理器的協助––執行獨立 OS 映像。在這些情況下，只有一個直接控制實體記憶體存取的軟體。

[![&#x5716; 4.4&#xFF1A;Xen &#x865B;&#x64EC;&#x5316;&#x6A21;&#x578B;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-4.4.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-4.4.png)圖 4.4：Xen 虛擬化模型

在 Xen 的情況下（見圖 4.4），Xen VMM 即是這個軟體。不過 VMM 本身並不實作太多其它的硬體控制。不若在其它較早期的系統（以及首次釋出的 Xen VMM）上的 VMM，除了記憶體與處理器之外的硬體是由具有特權的 Dom0 域所控制的。目前，這基本上是與沒有特權的 DomU 系統核心相同的系統核心，而且––就所關心的記憶體管理而言––它們沒什麼區別。重要的是，VMM 將實體記憶體分發給了 Dom0 與 DomU 系統核心，其因而實作了普通的記憶體管理，就好像它們是直接執行在一個處理器上一樣。

為了實現完成虛擬化所需的域的分離，Dom0 與 DomU 系統核心中的記憶體處理並不具有無限制的實體記憶體存取。VMM 不是藉由分發獨立的實體分頁、並讓客戶端 OS 處理定址的方式來分發記憶體；這不會提供任何針對有缺陷或者流氓客戶域的防範。取而代之地，VMM 會為每個客戶域建立它自己擁有的分頁表樹，並使用這些資料結構來分發記憶體。好處是能夠控制對分頁表樹的管理資訊的存取。若是程式沒有合適的權限，它就什麼也無法做。

這種存取控制被利用在 Xen 提供的虛擬化之中，無論使用的是半虛擬化（paravirtualization）或是硬體虛擬化（亦稱全虛擬化）。客戶域採用了有意與半虛擬化以及硬體虛擬化十分相似的方式，為每個行程建立了它們的分頁表樹。無論客戶端 OS 在何時修改了它的分頁表，都會呼叫 VMM。VMM 於是使用在客戶域中更新的資訊來更新它自己擁有的影子分頁表。這些是實際被硬體用到的分頁表。顯然地，這個過程相當昂貴：分頁表樹每次修改都需要一次 VMM 的呼叫。在沒有虛擬化的情況下對記憶體映射的更動並不便宜，而它們現在甚至變得更昂貴了。

考慮到從客戶端 OS 到 VMM 的更改並返回，它們本身已經非常昂貴，額外的成本可能非常大。這即是為何處理器開始擁有額外的功能，以避免影子分頁表的建立。這很好，不僅因為速度的關係，它也減少了 VMM 的記憶體消耗。Intel 有擴充分頁表（Extended Page Table，EPT），而 AMD 稱它為巢狀分頁表（Nested Page Table，NPT）。基本上這兩個技術都擁有客戶端 OS 從「客戶虛擬位址（guest virtual address）」產生「宿主虛擬位址（host virtual address）」的分頁表。宿主虛擬位址接著必須被進一步––使用每個域的 EPT／NPT 樹––轉譯成真正的實體位址。這會令記憶體處理以幾乎是非虛擬化情況的速度來進行，因為大多數記憶體管理的 VMM 項目都被移除了。它也減少了 VMM 的記憶體使用，因為現在每個域（對比於行程）都僅有一個必須要維護的分頁。

這個額外的位址轉譯步驟的結果也會儲存在 TLB 中。這表示 TLB 不會儲存虛擬的實體位址，而是查詢的完整結果。已經解釋過 AMD 的 Pacifica 擴充引入了 ASID 以避免在每個項目上的 TLB 沖出。ASID 的位元數量在最初釋出的處理器擴充中只有一位；這足以區隔 VMM 與客戶端 OS 了。Intel 擁有用於相同目的的虛擬處理器 ID（virtual processor ID，VPID），只不過有更多的位元數。但是對於每個客戶域而言，VPID 都是固定的，因此它無法被用來標記個別的行程，也不能在這個層級避免 TLB 沖出。

每次位址空間修改所需的工作量是有著虛擬化 OS 的一個問題。不過，基於 VMM 的虛擬化還有另一個固有的問題：沒有辦法擁有兩層記憶體處理。但是記憶體處理很難（尤其在將像 NUMA 這類難題納入考慮的時候，見第五節）。Xen 使用一個分離 VMM 的方式使得最佳化的（甚至是好的）處理變得困難，因為所有的記憶體管理實作的難題––包含像記憶體區域的探尋這類「瑣碎」事––都必須在 VMM 中重複。OS 擁有成熟且最佳化的實作；真的應該避免重複這些事。

[![&#x5716; 4.5&#xFF1A;KVM &#x865B;&#x64EC;&#x5316;&#x6A21;&#x578B;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-4.5.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-4.5.png)圖 4.5：KVM 虛擬化模型

這即是為何廢除 VMM／Dom0 模型是個如此有吸引力的替代方案。圖 4.5 顯示了 KVM Linux 系統核心擴充是如何試著解決這個問題的。沒有直接執行在硬體上、並控制所有客戶的分離 VMM；而是一個普通的 Linux 系統核心接管了這個功能。這表示在 Linux 系統核心上完整且精密的記憶體處理功能被用來管理系統中的記憶體。客戶域與被創造者稱為「客戶模式（guest mode）」的普通的使用者層級行程一同執行。虛擬化功能––半虛擬化或全虛擬化––是由 KVM VMM 所控制。這只不過是另一個使用者層級的行程，使用系統核心實作的特殊 KVM 裝置來控制一個客戶域。

這個模型相較於 Xen 模型的分離 VMM 的優點是，即使在使用客戶端 OS 時仍然有兩個運作的記憶體處理者，但只需要唯一一種在 Linux 系統核心中的實作。沒有必要像 Xen VMM 一樣在另一段程式碼中重複相同的功能。這導致更少的工作、更少的臭蟲、以及––也許––更少兩個記憶體管理者接觸的摩擦，因為在一個 Linux 客戶端中的記憶體管理者會與外部在裸機上執行的 Linux 系統核心的記憶體管理者做出相同的假設。

總而言之，程式設計師必須意識到，採用虛擬化的時候，快取錯失（指令、資料、或 TLB）的成本甚至比起沒有虛擬化還要高。任何減少這些工作的最佳化，在虛擬化的環境中甚至會獲得更多的回報。處理器設計者將會––隨著時間的推移––透過像是 EPT 與 NPT 這類技術來逐漸減少這個差距，但它永遠也不會完全消失。

