# 第6章：程序员能做什么

在经过了前面几节的描述之后，相信你已经清楚地知道了，程序员有非常非常多影响程序性能的机会，无论是正面的还是负面 ，讨论只与内存操作有关。我们将会全面地解释这些部分，从最底层的物理 RAM 访问和 L1 缓存开始，一直到影响内存管理的操作系统功能。

## 6.1 绕过缓存

当数据被产生并且没有（立即）被再次使用时，内存操作会先读取完整的缓存行然后修改缓存数据，这点对性能是有害的。这个操作会将可能再次用到的数据踢出缓存，以让给那些短期内不会再次被用到的数据。尤其是像矩阵––它会先被填值、接着才被使用––这类大数据结构。在填入矩阵的最后一个元素前，第一个元素就会因为矩阵太大被踢出缓存，导致写入缓存丧失效率。

对于这类情况，处理器提供了对_非暂存（non-temporal）_写入操作的支援。这个情境下的非暂存指的是资料在短期内不会被使用，所以没有任何快取它的理由。这些非暂存的写入操作不会先读取快取行然后才修改它；反之，新的内容会被直接写进记忆体。

这听来代价高昂，但并不是非得如此。处理器会试著使用合併写入（见 3.3.3 节）来填入整个快取行。若是成功了，那麽记忆体读取操作是完全不必要的。如 x86 以及 x86-64 架构，gcc 提供了若干内建函数：

```text
#include <emmintrin.h>
void _mm_stream_si32(int *p, int a);
void _mm_stream_si128(int *p, __m128i a);
void _mm_stream_pd(double *p, __m128d a);
#include <xmmintrin.h>
void _mm_stream_pi(__m64 *p, __m64 a);
void _mm_stream_ps(float *p, __m128 a);
#include <ammintrin.h>
void _mm_stream_sd(double *p, __m128d a);
void _mm_stream_ss(float *p, __m128 a);
```

最有效率地使用这些指令的情况是一次处理大量资料。资料从记忆体载入、经过一或多步处理、而后写回记忆体。资料「流（stream）」经处理器，这些指令便得名于此。

记忆体位置必须各自对齐至 8 或 16 位元组。在使用多媒体扩充（multimedia extension）的程式码中，也可以用这些非暂存的版本替换一般的 _mm\_store_\* 指令。我们并没有在 A.1 节的矩阵相乘程式中这麽做，因为写入的值会在短时间内被再次使用。这是串流指令无所助益的一个例子。6.2.1 节会更加深入这段程式码。

处理器的合併写入缓衝区可以将部分写入快取行的请求延迟一小段时间。一个接著一个执行所有修改单一快取行的指令，以令合併写入能真的发挥功用通常是必要的。以下是一个如何实践的例子：

```text
#include <emmintrin.h>
void setbytes(char *p, int c)
{
  __m128i i = _mm_set_epi8(c, c, c, c,
                           c, c, c, c,
                           c, c, c, c,
                           c, c, c, c);
  _mm_stream_si128((__m128i *)&p[0], i);
  _mm_stream_si128((__m128i *)&p[16], i);
  _mm_stream_si128((__m128i *)&p[32], i);
  _mm_stream_si128((__m128i *)&p[48], i);
}
```

假设指标 p 被适当地对齐，呼叫这个函式会将指向的快取行中的所有位元组设为 c。合併写入逻辑会看到四个生成的 movntdq 指令，并仅在最后一个指令被执行之后，才对记忆体发出写入命令。总而言之，这段程式不仅避免了在写入前读取快取行，也避免快取被并非立即需要的资料污染。这在某些情况下有著巨大的好处。一个经常使用这项技术的例子即是 C 函式库中的 memset 函数，它在处理大块记忆体时应该要使用类似于上述程式的作法。

某些架构提供了专门的解法。PowerPC 架构定义了dcbz 指令，它能用以清除整个快取行。这个指令不会真的绕过快取，因为快取行仍会被分配来存放结果，但没有任何资料会从记忆体被读出来。这相比于非暂存储存指令更加受限，因为快取行只能全部被清空而污染了快取（在资料为非暂存的情况），但其不需合併写入逻辑来达到这个结果。

为了一探非暂存指令的运作，我们将观察一个用以测量矩阵––由一个二维阵列所组成––写入效能的新测试。编译器将矩阵置放于记忆体中，以令最左边的（第一个）索引指向一列在记忆体中连续置放的所有元素。右边的（第二个）索引指向一列中的元素。测试程式以两种方式叠代矩阵：第一种是在内部迴圈增加行号，第二种是在内部迴圈增加列号。这代表其行为如图 6.1 所示。

[![&#x5716; 6.1&#xFF1A;&#x77E9;&#x9663;&#x5B58;&#x53D6;&#x6A21;&#x5F0F;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.1.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.1.png)

图 6.1：矩阵访问模式

我们测量了初始化一个 3000 × 3000 矩阵所花的时间。为了观察记忆体的表现，我们採用不会使用快取的储存指令。在 IA-32 处理器上，「非暂存提示（non-temporal hint）」即被用在于此。作为比较，我们也测量了一般的储存操作。结果见于表 6.1。

|  | 內部迴圈增加 |  |
| :--- | :--- | :--- |
| 列 | 行 |  |
| 一般 | 0.048s | 0.127s |
| 非暫存 | 0.048s | 0.160s |

表 6.1：矩阵初始化计时

对于使用快取的一般写入操作，我们观察到预期中的结果：若是记忆体被循序地使用，我们会得到比较好的结果，整个操作费了 0.048s，相当于 750MB/s，几近于随机存取的情况却花了 0.127s（大约 280MB/s）。这个矩阵已经大到令快取没那麽有效了。

我们感兴趣的部分主要是绕过快取的写入操作。可能令人吃惊的是，在这裡循序存取跟使用快取的情况一样快。这个结果的原因是处理器执行了上述的合併写入操作。此外，对于非暂存写入的_记忆体排序（memory ordering）_规则亦被放宽：程式需要明确地插入记忆体屏障（memory barriers）（如 x86 与 x86-64 处理器的 sfence 指令）。意味著处理器在写回资料时有著更多的自由，因此能尽可能地善用可用的频宽。

内部迴圈以行向（column-wise）存取的情况就不同了。无快取存取的结果明显地慢于快取存取（0.16s，约 225MB/s）。这裡我们可以理解到，合併写入是不可能的，每个记忆单元都必须被独立处理。这需要不断地从 RAM 晶片上选取新的几列，附带著与此相应的延迟。结果是比有快取的情况还慢了 25%。

在读取操作上，处理器––直到最近––除了非暂存存取（Non-Temporal Access，NTA）预取指令的弱提示之外，仍欠缺相应的支援。没有与合併写入对等的读取操作，这对诸如记忆体对映 I/O（memory-mapped I/O）这类无法被快取的记忆体尤其糟糕。Intel 附带 SSE4.1 扩充引入了 NTA 载入。它们以一些串流载入缓衝区（streaming load buffer）实作；每个缓衝区包含一个快取行。针对一个快取行的第一个 movntdqa 指令会将快取行载入一个缓衝区––可能会替换掉另一个快取行。随后，对同一个快取行、以 16 位元组对齐的存取操作将会由载入缓衝区以少量的成本来提供服务。除非有其它理由，快取行将不会被载入到快取中，于是便能够在不污染快取的情况下载入大量的记忆体。编译器为这个指令提供了一个内建函数：

```text
#include <smmintrin.h>
__m128i _mm_stream_load_si128 (__m128i *p);
```

这个内建函数应该要以 16 位元组区块的地址做为参数执行多次，直到每个快取行都被读取为止。在这时才应该开始处理下一个快取行。由于只有少数几个串流读取缓衝区，可能要一次从两个记忆体位置进行读取。

我们应该从这个实验得到的是，现代的 CPU 非常巧妙地最佳化了无快取写入––近来甚至包括读取操作，只要它们是循序操作的。在处理只会被用到一次的大资料结构时，这个知识是非常有用的。再者，快取能够降低一些––但不是全部––随机记忆体存取的成本。在这个例子中，由于 RAM 存取的实作，导致随机存取慢了 70%。在实作改变以前，无论何时都应该避免随机存取。

我们将会在谈论预取的章节再次一探非暂存旗标。

## 6.2 缓存存取

希望改进他们程式效能的程式设计师会发现，最好聚焦在影响一阶快取的改变上，因为这很可能会产生最好的结果。我们将会在讨论延伸到其它层级之前先讨论它。显然地，所有针对一阶快取的最佳化也会影响其它快取。所有记忆体存取的主题都是相同的：改进局部性（空间与时间）并对齐程式码与资料。

### 6.2.1 最佳化一階資料快取存取

在 3.3 节，我们已经看过 L1d 快取的有效使用能够提升效能。在这一节，我们会展示什麽样的程式码改变能够协助改进这个效能。延续前一节，我们首先聚焦在循序存取记忆体的最佳化。如同在 3.3 节中看到的数字，处理器在记忆体被循序存取的时候会自动预取资料。

使用的范例程式码为矩阵乘法。我们使用两个 $$1000 \times 1000$$ double 元素的方阵（square matrices）。对于那些忘记数学的人，给定元素为 $$a_{ij}$$ 与 $$b_{ij}$$ 的矩阵 $$A$$ 与 $$B$$，$$0 \leq i,j < N$$，乘积为

$$(AB){ij} = \sum^{N - 1}{k = 0} a_{ik} b_{kj} = a_{i1} b_{1j} + a_{i2} b_{2j} + \cdots + a_{i(N - 1)} b_{(N - 1)j}$$

一个直观的 C 实作看起来可能像这样

```text
for (i = 0; i < N; ++i)
  for (j = 0; j < N; ++j)
    for (k = 0; k < N; ++k)
      res[i][j] += mul1[i][k] * mul2[k][j];
```

两个输入矩阵为 mul1 与 mul2。假定结果矩阵 res 全被初始化为零。这是个既好又简单的实作。但应该很明显的是，我们有个正好是在图 6.1 解释过的问题。在 mul1 被循序存取的时候，内部的迴圈增加了 mul2 的列号。这表示 mul1 是像图 6.1 中左边的矩阵那样处理，而 mul2 是像右边的矩阵那样处理。这可能不太好。

有一个能够轻易尝试的可能补救方法。由于矩阵中的每个元素会被多次存取，是值得在使用第二个矩阵 mul2 之前将它重新排列（数学术语的话，「转置〔transpose〕」）的。

$$(AB){ij} = \sum^{N - 1}{k = 0} a_{ik} b^{\text{T}}{jk} = a{i1} b^{\text{T}}{j1} + a{i2} b^{\text{T}}{j2} + \cdots + a{i(N - 1)} b^{\text{T}}_{j(N - 1)}$$

在转置之后（通常以上标「T」表示），我们现在循序地叠代两个矩阵。就 C 程式而言，现在看起来像这样：

```text
double tmp[N][N];
for (i = 0; i < N; ++i)
  for (j = 0; j < N; ++j)
    tmp[i][j] = mul2[j][i];
for (i = 0; i < N; ++i)
  for (j = 0; j < N; ++j)
    for (k = 0; k < N; ++k)
      res[i][j] += mul1[i][k] * tmp[j][k];
```

我们建立一个容纳被转置的矩阵的暂时变数（temporary variable）。这需要动到额外的记忆体，但这个成本会被––希望如此––弥补回来，因为每行 1000 次非循序存取是更为昂贵的（至少在现代的硬体上）。是进行一些效能测试的时候了。在有著 2666MHz 时脉的 Intel Core 2 上的结果为（以时钟週期为单位）：

|  | 原始 | 轉置 |
| :--- | :--- | :--- |
| 週期數 | 16,765,297,870 | 3,922,373,010 |
| 相對值 | 100% | 23.4% |

虽然只是个简单的矩阵转置，但我们能达到 76.6% 的加速！複製操作的损失完全被弥补了。1000 次非循序存取真的很伤。

下个问题是，我们是否能做得更好。无论如何，我们确实需要一个不需额外複製的替代方法。我们并不是总有馀裕能进行複製：矩阵可能太大、或者可用的记忆体太小。

替代实作的探寻应该从彻底地检验涉及到的数学与原始实作所执行的操作开始。简单的数学知识让我们能够发现，只要每个加数（addend）正好出现一次，对结果矩阵的每个元素执行的加法顺序是无关紧要的。^28这个理解让我们能够寻找将执行在原始程式码内部迴圈的加法重新排列的解法。

现在，让我们来检验在原始程式码执行中的实际问题。被存取的 mul2 元素的顺序为：$$(0, 0)$$、$$(1, 0)$$、 ... 、$$(N - 1, 0)$$、$$(0,1)$$、$$(1, 1)$$、 ...。元素 $$(0, 0)$$ 与 $$(0, 1)$$ 位于同一个快取行中，但在内部迴圈完成一轮的时候，这个快取行早已被逐出了。以这个例子而言，每一轮内部迴圈都需要––对三个矩阵的每一个而言––1000 个快取行（Core 2 处理器为 64 位元组）。这加起来远比 L1d 可用的 32k 还多。

但若是我们在执行内部迴圈的期间，一起处理中间迴圈的两次叠代呢？在这个情况下，我们使用两个来自必定在 L1d 中的快取行的 double 值。我们将 L1d 错失率减半了。这当然是个改进，但––视快取行的大小而定––也许仍不是我们能够得到的最好结果。Core 2 处理器有个快取行大小为 64 位元组的 L1d。实际的大小能够使用

`sysconf (_SC_LEVEL1_DCACHE_LINESIZE)`

在执行期查询、或是使用命令列（command line）的 getconf 工具程式（utility），以让程式能够针对特定的快取行大小编译。以 sizeof\(double\) 为 8 来说，这表示––为了完全利用快取行––我们应该展开内部迴圈 8 次。继续这个想法，为了有效地使用 res 矩阵––即，为了同时写入 8 个结果––我们也该展开外部迴圈 8 次。我们假设这裡的快取行大小为 64，但这个程式码也能在 32 位元组快取行的系统上运作，因为快取行也会被 100% 利用。一般来说，最好在编译期像这样使用 getconf 工具程式来写死（hardcode）快取行大小：

`gcc -DCLS=$(getconf LEVEL1_DCACHE_LINESIZE) ...`

若是二元档是假定为一般化（generic）的话，应该使用最大的快取行大小。使用非常小的 L1d 表示并非所有资料都能塞进快取，但这种处理器无论如何都不适合高效能程式。我们写出的程式码看起来像这样：

```text
#define SM (CLS / sizeof (double))
for (i = 0; i < N; i += SM)
  for (j = 0; j < N; j += SM)
    for (k = 0; k < N; k += SM)
      for (i2 = 0, rres = &res[i][j],
           rmul1 = &mul1[i][k]; i2 < SM;
           ++i2, rres += N, rmul1 += N)
        for (k2 = 0, rmul2 = &mul2[k][j];
             k2 < SM; ++k2, rmul2 += N)
          for (j2 = 0; j2 < SM; ++j2)
            rres[j2] += rmul1[k2] * rmul2[j2];
```

这看起来超可怕的。在某种程度上它是如此，但只是因为它包含了一些技巧。最显而易见的改变是，我们现在有六层巢状迴圈了。外部迴圈以 SM（快取行大小除掉 sizeof\(double\)）为间隔叠代。这将乘法切成多个能够以更多快取局部性处理的较小的问题。内部迴圈叠代外部迴圈漏掉的索引。再一次，这裡有三层迴圈。这裡唯一巧妙的部分是 k2 与 j2 迴圈的顺序不同。这是因为在实际运算中，仅有一个表示式取决于 k2、但有两个取决于 j2。

这裡其馀的複杂之处来自 gcc 在最佳化阵列索引的时候并不是非常聪明的结果。额外变数 rres、rmul1、与 rmul2 的引入，藉由将内部迴圈的常用表示式（expression）尽可能地拉出来，以最佳化程式码。C 与 C++ 语言预设的别名规则（aliasing rule）并不能帮助编译器做出这些决定（除非使用 restrict，所有指标存取都是别名的潜在来源）。这即是为何对于数值程式设计而言，Fortran 仍是一个偏好语言的原因：它令快速程式的撰写更简单。

|  | 原始 | 轉置 | 子矩陣 | 向量化 |
| :--- | :--- | :--- | :--- | :--- |
| 週期數 | 16,765,297,870 | 3,922,373,010 | 2,895,041,480 | 1,588,711,750 |
| 相對值 | 100% | 23.4% | 17.3% | 9.47% |

表 6.2：矩陣乘法計時

所有努力所带来的成果能够在表 6.2 看到。藉由避免複製，我们增加了额外的 6.1% 效能。此外，我们不需要任何额外的记忆体。只要结果矩阵也能塞进记忆体，输入矩阵可以是任意大小的。这是我们现在已经达成的一个通用解法的一个必要条件。

在表 6.2 中还有一栏没有被解释过。大多现代处理器现今包含了针对向量化（vectorization）的特殊支援。经常被标为多媒体扩充，这些特殊指令能够同时处理 2、4、8、或者更多值。这些经常是 SIMD（单指令多资料，Single Instruction, Multiple Data）操作，藉由其它操作的协助，以便以正确的形式获取资料。由 Intel 处理器提供的 SSE2 指令能够在一个操作中处理两个 double 值。指令参考手册列出了提供对这些 SSE2 指令存取的内建函数。若是用了这些内建函数，程式执行会变快 7.3%（相对于原始实作）。结果是，一支以原始程式码 10% 的时间执行的程式。翻译成人们认识的数字，我们从 318 MFLOPS 变为 3.35 GFLOPS。由于我们在这裡仅对记忆体的影响有兴趣，程式的原始码被摆到了 A.1 节。

应该注意的是，在最后一版的程式码中，我们仍然有一些 mul2 的快取问题；预取仍然无法运作。但这无法在不转置矩阵的情况下解决。或许快取预取单元将会变得聪明地足以识别这些模式，那时就不需要额外的更动了。不过，以一个 2.66 GHz 处理器上的单执行绪程式而言，3.19 GFLOPS 并不差了。

我们在矩阵乘法的例子中最佳化的是被载入的快取行的使用。一个快取行的所有位元组总是会被用到。我们只是确保在快取行被逐出前会用到它们。这当然是个特例。

更常见的是，拥有塞满一或多个快取行的资料结构，而程式在任何时间点都只会使用几个成员。我们已经在图 3.11 看过，大结构尺寸在只有一些成员被用到时的影响。

[![&#x5716; 6.2&#xFF1A;&#x6563;&#x5E03;&#x5728;&#x591A;&#x500B;&#x5FEB;&#x53D6;&#x884C;&#x4E2D;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.2.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.2.png)

圖 6.2：散布在多個快取行中

图 6.2 显示了使用现在已熟知的程式执行另一组基准测试的结果。这次会加上同个串列元素的两个值。在一个案例中，两个元素都在同一个快取行内；在另一个案例中，一个元素位在串列元素的第一个快取行，而第二个位在最后一个快取行。这张图显示了我们正遭受的效能衰减。

不出所料，在所有情况下，若是工作集塞得进 L1d 就不会有任何负面影响。一旦 L1d 不再充足，则是使用一个行程的两个快取行来偿付损失，而非一个。红线显示了串列被循序地排列时的数据。我们看到寻常的两步模式：当 L2 快取充足时的大约 17% 的损失、以及当必须用到主记忆体时的大约 27% 的损失。

在随机记忆体存取的情况下，相对的数据看起来有点不同。对于塞得进 L2 的工作集而言的效能衰减介于 25% 到 35% 之间。再往后它下降到了大约 10%。这不是因为损失变小了，而是因为实际的记忆体存取不成比例地变得更昂贵了。这份数据也显示了，在某些情况下，元素之间的距离是很重要的。Random 4 CLs 的曲线显示了较高的损失，因为用到了第一个与第四个快取行。

要查看一个资料结构对比于快取行的佈局，一个简单的方法是使用 pahole 程式（见 \[4\]）。这个程式检验了定义在二进位档案中的资料结构。取一个包含这个定义的程式：

```text
struct foo {
  int a;
  long fill[7];
  int b;
};
```

当在一台 64 位元机器上编译时，pahole 程式的输出（在其它东西之中）包含了显示于图 6.3 的输出。这个输出结果告知了我们很多东西。首先，它显示了这个资料结构使用了超过一个快取行。这个工具假设了当前使用的处理器的快取行大小，但这个值能够使用一个命令列参数来覆写。尤其在结构大小几乎没有超过一个快取行、以及许多这种型别的物件会被分配的情况下，寻求一个压缩这种结构的方式是合理的。或许几个元素能有比较小的型别、又或者某些栏位实际上是能使用独立位元来表示的旗标。

```text
struct foo {
        int                        a;                    /*     0     4 */
    /* XXX 4 bytes hole, try to pack */    long int                   fill[7];              /*     8    56 */    /* --- cacheline 1 boundary (64 bytes) --- */    int                        b;                    /*    64     4 */}; /* size: 72, cachelines: 2 / / sum members: 64, holes: 1, sum holes: 4 / / padding: 4 / / last cacheline: 8 bytes */
```

圖 6.3：pahole 執行的輸出

在这个范例的情况中，压缩是很容易的，而且它也被这支程式所暗示。输出显示了在第一个元素后面有个四位元的洞（hole）。这个洞是由结构的对齐需求以及 fill 元素所造成的。很容易发现元素 b––其大小为四位元组（由那行结尾的 4 所指出的）––完美地与这个间隔（gap）相符。在这个情况下的结果是，间隔不再存在，而这个资料结构塞得进一个快取行中。pahole 工具能自己完成这个最佳化。若是使用了 --reorganize 参数，并将结构的名称加到命令列的结尾，这个工具的输出即是最佳化的结构、以及使用的快取行。除了移动栏位以填补间隔之外，这个工具也能够最佳化位元栏位以及合併填充（padding）与洞。更多细节见 \[4\]。

有个正好大得足以容纳尾端元素的洞当然是个理想的情况。为了让这个最佳化有用，物件本身也必须对齐快取行。我们马上就会开始处理这点。

pahole 输出也能够轻易看出元素是否必须被重新排列，以令那些一起用到的元素也会被储存在一起。使用 pahole 工具，很容易就能够确定哪些元素要在同个快取行，而不是必须在重新排列元素时才能达成。这并不是一个自动的过程，但这个工具能帮助很多。

各个结构元素的位置、以及它们被使用的方式也很重要。如同我们已经在 3.5.2 节看到的，晚到快取行的关键字组的程式效能是很糟的。这表示一位程式设计师应该总是遵循下列两条原则：

1. 总是将最可能为关键字组的结构元素移到结构的开头。
2. 存取资料结构、以及存取顺序不受情况所约束时，以它们定义在结构中的顺序来存取。

以小结构而言，这表示元素应该以它们可能被存取的顺序排列。这必须以灵活的方式处理，以允许其它像是补洞之类的最佳化也能被使用。对于较大的资料结构，每个快取行大小的区块应该遵循这些原则来排列。

不过，若是物件自身不若预期地对齐，就不值得花时间来重新排列它。一个物件的对齐，是由资料型别的对齐需求所决定的。每个基础型别有它自己的对齐需求。对于结构型别，它的任意元素中最大的对齐需求决定了这个结构的对齐。这几乎总是小于快取行大小。这表示即使一个结构的成员被排列成塞得进同一个快取行，一个被分配的物件也可能不具有相符于快取行大小的对齐。有两种方法能确保物件拥有在设计结构佈局时使用的对齐：

* 物件能够以明确的对齐需求分配。对于动态分配（dynamic allocation），呼叫 malloc 仅会以相符于最严格的标准型别（通常是 long double）的对齐来分配物件。不过，使用 posix\_memalign 请求较高的对齐也是可能的。

  ```text
  #include <stdlib.h>
  int posix_memalign(void **memptr,
                     size_t align,
                     size_t size);
  ```

  这个函数将一个指到新分配的记忆体的指标储存到由 memptr 指到的指标变数中。记忆体区块大小为 size 位元组，并在 align 位元组边界上对齐。 对于由编译器分配的物件（在 .data、.bss 等，以及在堆叠中），能够使用一个变数属性（attribute）：

  ```text
  struct strtype variable
     __attribute((aligned(64)));
  ```

  在这个情况下，不管 strtype 结构的对齐需求为何，variable 都会在 64 位元组边界上对齐。这对全域变数与自动变数也行得通。 对于阵列，这个方法并不如你可能预期的那般运作。只有阵列的第一个元素会被对齐，除非每个元素的大小是对齐值的倍数。这也代表了每个单一变数都必须被适当地标注。posix\_memalign 的使用也不是完全不受控制的，因为对齐需求通常会导致碎片与／或更高的记忆体消耗。

* 一个使用者定义型别的对齐需求能够使用一个型别属性来改变：

  ```text
  struct strtype {
      ...members...
  } __attribute((aligned(64)));
  ```

  这会使编译器以合适的对齐来分配所有的物件，包含阵列。不过，程式设计师必须留意针对动态分配物件的合适对齐的请求。这裡必须再一次使用 posix\_memalign。使用 gcc 提供的 alignof 运算子（operator）、并将这个值作为第二个参数传递给 posix\_memalign 是很简单的。

之前在这一节提及的多媒体扩充几乎总是需要对齐记忆体存取。即，对于 16 位元组的记忆体存取而言，位址是被假定以 16 位元组对齐的。x86 与 x86-64 处理器拥有能够处理非对齐存取的记忆体操作的特殊变体，但这些操作比较慢。对于所有记忆体存取都需要完全对齐的大多 RISC 架构而言，这种严格的对齐需求并不新奇。即使一个架构支援非对齐的存取，这有时也比使用合适的对齐还慢，尤其是在不对齐导致一次载入或储存使用了两个快取行、而非一个的情况下。

[![&#x5716; 6.4&#xFF1A;&#x975E;&#x5C0D;&#x9F4A;&#x5B58;&#x53D6;&#x7684;&#x9593;&#x63A5;&#x6210;&#x672C;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.4.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.4.png)

圖 6.4：非對齊存取的間接成本

图 6.4 显示了非对齐记忆体存取的影响。现已熟悉的测试会在（循序或随机）走访记忆体被量测的期间递增一个资料元素，一次使用对齐的串列元素、一次使用刻意不对齐的元素。图表显示了程式因非对齐存取而招致的效能衰减。循序存取情况下的影响比起随机的情况更为显著，因为在后者的情况下，非对齐存取会部分地被一般来说较高的记忆体存取成本所隐藏。在循序的情况下，对于塞得进 L2 快取的工作集大小来说，效能衰减大约是 300%。这能够由 L1 快取的有效性降低来解释。某些递增操作现在会碰到两个快取行，而且现在在一个串列元素上操作经常需要两次快取行的读取。L1 与 L2 之间的连接简直太壅塞了。

对于非常大的工作集大小，非对齐存取的影响仍然是 20% 至 30%––考虑到对于这种大小的对齐存取时间很长，这是非常多的。这张图表应该显示了对齐是必须被严加对待的。即使架构支援非对齐存取，也绝对不要认为「它们跟对齐存取一样好」。

不过，有一些来自这些对齐需求的附带结果。若是一个自动变数拥有一个对齐需求，编译器必须确保它在所有情况下都能够被满足。这并不容易，因为编译器无法控制呼叫点（call site）与它们处理堆叠的方式。这个问题能够以两种方式处理：

1. 产生的程式主动地对齐堆叠，必要时插入间隔。这需要程式检查对齐、建立对齐、并在之后还原对齐。
2. 要求所有的呼叫端都将堆叠对齐。

所有常用的应用程式二进位介面（application binary interface，ABI）都遵循第二条路。如果一个呼叫端违反了规则、并且对齐为被呼叫端所需，程式很可能会失去作用。不过，对齐的完美保持并不会平白得来。

在一个函数中使用的一个堆叠框（frame）的大小不必是对齐的倍数。这表示，若是从这个堆叠框呼叫了其它函数，填充就是必要的。很大的不同是，在大部分情况下，堆叠框的大小对编译器而言是已知的，因此它知道如何调整堆叠指标，以确保任何从这个堆叠框呼叫的函数的对齐。事实上，大多编译器会直接将堆叠框的大小调高，并以它来完成操作。

如果使用了可变长度阵列（variable length array，VLA）或 alloca，这种简单的对齐处理方式就不合适了。在这种情况下，堆叠框的总大小只会在执行期得知。在这种情况下可能会需要主动的对齐控制，使得产生的程式码（略微地）变慢。

在某些架构上，只有多媒体扩充需要严格的对齐；在那些架构上的堆叠总是当作普通的资料型别进行最低限度的对齐，对于 32 与 64 位元架构通常分别是 4 或 8 位元组。在这些系统上，强制对齐会招致不必要的成本。这表示，在这种情况下，我们可能会想要摆脱严格的对齐需求，如果我们知道不会依赖它的话。不进行多媒体操作的尾端函数（tail function）（那些不呼叫其它函数的函数）不必对齐。只呼叫不需对齐的函数的函数也不用。若是能够识别出够大一组函数，一支程式可能会想要放宽对齐需求。对于 x86 的二元档，gcc 拥有宽鬆堆叠对齐需求的支援：

`-mpreferred-stack-boundary=2`

若是这个选项（option）的值为 $$N$$，堆叠对齐需求将会被设为 $$2^{N}$$ 位元组。所以，若是使用了 2 为值，堆叠对齐需求就被从预设值（为 16 位元组）降低成只有 4 位元组。在大多情况下，这表示不需额外的对齐操作，因为普通的堆叠推入（push）与弹出（pop）操作无论如何都是在四位元组边界上操作的。这个机器特定的选项能够帮忙减少程式大小，也能够提升执行速度。但它无法被套用到许多其它的架构上。即使对于 x86-64，一般来说也不适用，因为 x86-64 ABI 要求在 SSE 暂存器中传递浮点数参数，而 SSE 指令需要完整的 16 位元组对齐。然而，只要能够使用这个选项，就能造成明显的差别。

结构元素的有效摆放与对齐并非资料结构影响快取效率的唯一面向。若是使用了一个结构的阵列，整个结构的定义都会影响效能。回想一下图 3.11 的结果：在这个情况中，我们增加了阵列元素中未使用的资料总量。结果是预取越来越没效果，而程式––对于大资料集––变得越来越没效率。

对于大工作集，尽可能地使用可用的快取是很重要的。为了达到如此，可能有必要重新排列资料结构。虽然对程式设计师而言，将所有概念上属于一块儿的资料摆在同个资料结构是比较简单的，但这可能不是最大化效能的最好方法。假设我们有个如下的资料结构：

```text
struct order {
  double price;
  bool paid;
  const char *buyer[5];
  long buyer_id;
};
```

进一步假设这些纪录会被存在一个大阵列中，并且有个经常执行的工作（job）会加总所有帐单的预期付款。在这种情境中，buyer 与 buyer\_id 使用的记忆体是不必被载入到快取中的。根据图 3.11 的资料来判断，程式将会表现得比它能达到的还糟了高达五倍。

将 order 切成两块，前两个栏位储存在一个结构中，而另一个栏位储存在别处要好得多了。这个改变无疑提高了程式的複杂度，但效能提升证明了这个成本的正当性。

最后，让我们考虑一下另一个––虽然也会被应用在其它快取上––主要是影响 L1d 存取的快取使用的最佳化。如同在图 3.8 看到的，增加的快取关联度有利于一般的操作。快取越大，关联度通常也越高。L1d 快取太大，以致于无法为全关联式，但又没有足够大到要拥有跟 L2 快取一样的关联度。若是工作集中的许多物件属于相同的快取集，这可能会是个问题。如果这导致了由于过于使用一组集合而造成逐出，即使大多快取都没被用到，程式还是可能会受到延迟。这些快取错失有时被称为衝突性错失（conflict miss）。由于 L1d 定址使用了虚拟位址，这实际上是能够受程式设计师控制的。如果一起被用到的变数也储存在一块儿，它们属于相同集合的可能性是被最小化的。图 6.5 显示了多快就会碰上这个问题。

[![&#x5716; 6.5&#xFF1A;&#x5FEB;&#x53D6;&#x95DC;&#x806F;&#x5EA6;&#x5F71;&#x97FF;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.5.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.5.png)

圖 6.5：快取關聯度影響

在这张图中，现在熟悉的、使用 NPAD=15 的 Follow 测试是以特殊的配置来量测的。X 轴是两个串列元素之间的距离，以空串列元素为单位量测。换句话说，距离为 2 代表下一个元素的位址是在前一个元素的 128 位元组之后。所有元素都以相同的距离在虚拟记忆体空间中摆放。Y 轴显示了串列的总长度。仅会使用 1 至 16 个元素，代表工作集总大小为 64 至 1024 位元组。Z 轴显示了寻访每个串列元素所需的平均週期数。

图中显示的结果应该不让人吃惊。若是被用到的元素很少，所有的资料都塞得进 L1d，而每个串列元素的存取时间仅有 3 个週期。对于几乎所有串列元素的安排都是如此：虚拟位址以几乎没有衝突的方式，被良好地映射到 L1d 的槽（slot）中。（在这张图中）有两个情况不同的特殊距离值。若是距离为 4096 位元组（即，64 个元素的距离）的倍数、并且串列的长度大于八，每个串列元素的平均週期数便大幅地增加。在这些情况下，所有项目都在相同的集合中，并且––一旦串列长度大于关联度––项目会从 L1d 被冲出，而下一轮必须从 L2 重新读取。这造成了每个串列元素大约 10 个週期的成本。

使用这张图，我们能够确定使用的处理器拥有一个关联度 8、且总大小为 32kB 的 L1d 快取。这表示，这个测试能够––必要的话––用以确定这些值。可以为 L2 快取量测相同的影响，但在这裡更为複杂，因为 L2 快取是使用实体位址来索引的，而且它要大得多了。

但愿程式设计师将这个数据视为值得关注集合关联度的一种暗示。将资料摆放在二的幂次的边界上足够常见于现实世界中，但这正好是容易导致上述影响与效能下降的情况。非对齐存取可能会提高衝突性错失的可能性，因为每次存取都可能需要额外的快取行。

[![&#x5716; 6.6&#xFF1A;Bank Address of L1d on AMD](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.6.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.6.png)

圖 6.6：AMD 上 L1d 的 Bank 位址

如果執行了這種最佳化，另一個相關的最佳化也是可能的。AMD 的處理器––至少––將 L1d 實作為多個獨立的 bank。 The L1d can receive two data words per cycle but only if both words are stored in different banks or in a bank with the same index. bank 位址是以虛擬位址的低位元編碼的，如圖 6.6 所示。 If variables which are used together are also stored together the likelihood that they are in different banks or the same bank with the same index is high.

\[^譯註\]: 這裡可能講得比較抽象。作者的意思是：在一開始三層迴圈的實作中，最內部的每一次 `k` 迴圈疊代同時處理 `res[i][j] += mul1[i][k] * mul2[k][j]` 與 `res[i][j + 1] += mul1[i][k] * mul2[k][j + 1]`。由於才剛存取過 `mul2[k][j]` 與 `res[i][j]`，所以 `mul2[k][j + 1]` 與 `res[i][j + 1]` 還在 L1d 快取中，因而降低了錯失率。後述的方法是這個方法的一般化（generalization）。

\[^29\]: 理論上在 1999 修訂版引入 C 語言的 `restrict` 關鍵字應該解決這個問題。不過編譯器還是不理解。原因主要是存在著太多不正確的程式碼，其會誤導編譯器、並導致它產生不正確的目的碼（object code）。

\[^30\]: 測試是在一台 32 位元機器上執行的，因此 `NPAD`=15 代表每個串列元素一個 64 位元組快取行。

### 6.2.2 最佳化一階指令快取存取

准备有效使用 L1i 的程式码需要与有效使用 L1d 类似的技术。不过，问题是，程式设计师通常不会直接影响 L1i 的使用方式，除非她／他以组合语言来撰写程式。若是使用了编译器，程式设计师能够透过引导编译器建立更好的程式佈局，来间接地决定 L1i 的使用。

程式有跳转（jump）之间为线性的优点。在这些期间，处理器能够有效地预取记忆体。跳转打破了这个美好的想像，因为

* 跳转目标（target）可能不是静态决定的；
* 而且即使它是静态的，若是它错失了所有快取，记忆体获取可能会花上很长一段时间。

这些问题造成了执行中的停顿，可能严重地影响效能。这即是为何现今的处理器在分支预测（branch prediction，BP）上费尽心思的原因了。高度特製化的 BP 单元试著尽可能远在跳转之前确定跳转的目标，使得处理器能够开始将新的位置的指令载入到快取中。它们使用静态与动态规则、而且越来越擅于判定执行中的模式。

对指令快取而言，尽早将资料拿到快取甚至是更为重要的。如同在 3.1 节提过的，指令必须在它们被执行之前解码，而且––为了加速（在 x86 与 x86-64 上很重要）––指令实际上是以被解码的形式、而非从记忆体读取的位元组／字组的形式被快取的。

为了达到最好的 L1i 使用，程式设计师至少应该留意下述的程式码产生的面向：

1. 尽可能地减少程式码量（code footprint）。这必须与像是迴圈展开（loop unrolling）与行内展开（inlining）等最佳化取得平衡。
2. 程式执行应该是没有气泡（bubble）的线性的。[^31](https://github.com/jason2506/cpumemory.zh-tw/blob/master/what-programmers-can-do/cache-access/%E6%B0%A3%E6%B3%A1%E7%94%9F%E5%8B%95%E5%9C%B0%E6%8F%8F%E8%BF%B0%E4%BA%86%E5%9C%A8%E4%B8%80%E5%80%8B%E8%99%95%E7%90%86%E5%99%A8%E7%9A%84%E7%AE%A1%E7%B7%9A%E4%B8%AD%E5%9F%B7%E8%A1%8C%E7%9A%84%E7%A9%BA%E6%B4%9E%EF%BC%8C%E5%85%B6%E6%9C%83%E5%9C%A8%E5%9F%B7%E8%A1%8C%E5%BF%85%E9%A0%88%E7%AD%89%E5%BE%85%E8%B3%87%E6%BA%90%E7%9A%84%E6%99%82%E5%80%99%E7%99%BC%E7%94%9F%E3%80%82%E9%97%9C%E6%96%BC%E6%9B%B4%E5%A4%9A%E7%B4%B0%E7%AF%80%EF%BC%8C%E8%AB%8B%E8%AE%80%E8%80%85%E5%8F%83%E9%96%B1%E8%99%95%E7%90%86%E5%99%A8%E8%A8%AD%E8%A8%88%E7%9A%84%E6%96%87%E7%8D%BB%E3%80%82)
3. 合理的情况下，对齐程式码。

我们现在要看一些根据这些面向、可用于协助最佳化程式的编译器技术。

编译器有启动不同最佳化层级的选项，特定的最佳化也能够个别地启用。在高最佳化层级（gcc 的 -O2 与 -O3）启用的许多最佳化处理了迴圈最佳化与函数行内展开。一般来说，这些是不错的最佳化。如果以这些方式最佳化的程式码佔了程式总执行时间的很重要的一部分，便能够提升整体的效能。尤其是，函数的行内展开允许编译器一次最佳化更大的程式码块（chunk），从而能够产生更好地利用处理器的管线架构的机器码。当程式较大的一部分能被视为一个单一单元时，程式码与资料的处理（透过死码消除〔dead code elimination〕或值域传播〔value range propagation〕、等等）的效果更好。

较大的程式大小意味著 L1i（以及 L2 与更高阶层）快取上的压力更大。这可能导致较差的效能。较小的程式可能比较快。幸运的是，gcc 有一个针对于此的最佳化选项。如果使用了 -Os，编译器将会为程式大小最佳化。已知会增加程式大小的最佳化会被关掉。使用这个选项经常产生惊人的结果。尤其在编译器无法真的获益于迴圈展开与行内展开的情况下，这个选项就是首选了。

行内展开也能被个别处理。编译器拥有引导行内展开的启发法（heuristic）与限制；这些限制能够由程式设计师控制。-finlinelimit 选项指定了对行内展开而言，必须被视为过大的函数有多大。若是一个函数在多处被呼叫，在所有函数中行内展开它便会导致程式大小的剧增。但还有更多细节。假设一个函数 inlcand 在两个函数 f1 与 f2 中被呼叫。函数 f1 与 f2 本身是先后被呼叫的。

|  |  |
| :--- | :--- |


表 6.3：行內展開 Vs 不行內展開

表 6.3 显示了在两个函数中没有行内展开与行内展开的情况下，产生的程式码看起来会怎麽样。若是函数 inlcand 在 f1 与 f2 中被行内展开了，产生的程式码的大小为 size f1 + size f2 + $$2 \times$$ size inlcand。如果没有进行行内展开的话，总大小减少了 size inlcand。这即是在 f1 与 f2 相互在不久后呼叫的话，L1i 与 L2 快取额外所需的量。再加上：若是 inlcand 没被行内展开，程式码可能仍在 L1i 中，而它就不必被再次解码了。再加上：分支预测单元或许能更好地预测跳转，因为它已经看过这段程式了。如果对程式而言，被行内展开的函数大小上限的编译器预设值并不是最好的，它应该要被降低。

不过，有些行内展开总是合理的情况。假如一个函数只会被呼叫一次，它也应该被行内展开。这给了编译器执行更多最佳化的机会（像是值域传播，其会显著地改进程式码）。行内展开也许会受选择限制所阻碍。对于像这样的情况，gcc 有个选项来明确指定一个函数总是要被行内展开。加上 always\_inline 函数属性会命令编译器执行恰如这个名称所指示的操作。

在相同的情境下，若是即便一个函数足够小也不该被行内展开，能够使用 noinline 函数属性。假如它们经常从多处被呼叫，即使对于小函数，使用这个属性也是合理的。若是 L1i 内容能被重複使用、并且整体的程式码量减少了，这往往弥补了额外函数呼叫的附加成本。如今分支预测单元是非常可靠的。若是行内展开能够促成更进一步的最佳化，情况就不同了。这是必须视情况来决定的。

如果行内展开的程式码总是会被用到的话，always\_inline 属性表现得很好。但假如不是这样呢？如果偶尔才会呼叫被行内展开的函数会怎麽样：

```text
void fct(void) {
  ... code block A ...
  if (condition)
    inlfct()
  ... code block C ...
```

为这种程式序列产生的程式码一般来说与原始码的结构相符。这表示首先会是程式区块 A、接著是一个条件式跳转––假如条件式被求值为否（false），就往前跳转。接下来是为行内展开的 inlfct 产生的程式码，最后是程式区块 C。这看起来全都很合理，但它有个问题。

若是 condition 经常为否，执行就不是线性的了。中间有一大块没用到的程式码，不仅因为预取污染了 L1i，它也会造成分支预测的问题。若是分支预测错了，条件表示式可能非常没有效率。

这是个普遍的问题，而且并不专属于函数的行内展开。无论在何时用到了条件执行、而且它是不对称的（即，表示式比起某一种结果还要更常产生另一种结果），就有不正确的静态分支预测、从而有管线中的气泡的可能性。这能够藉由告知编译器，以将较不常执行的程式码移出主要的程式路径来避免。在这种情况下，为一个 if 叙述产生的条件分支将会跳转到一个跳脱顺序的地方，如下图所示。

[![&#x7A0B;&#x5F0F;&#x78BC;&#x5340;&#x584A;&#x7684;&#x91CD;&#x65B0;&#x6392;&#x5217;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/code-block-reordering.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/code-block-reordering.png)

上半部表示单纯的程式佈局。假如区域 B––即，由上面被行内展开的函数 inlfct 所产生的––因为条件 I 跳过它而经常不被执行，处理器的预取会拉进包含鲜少用到的区块 B 的快取行。这能够藉由区块的重新排列来改变，其结果能够在图的下半部看到。经常执行的程式码在记忆体中是线性的，而鲜少执行的程式码被移到不伤及预取与 L1i 效率的某处。

gcc 提供了两个实现这点的方法。首先，编译器能够在重新编译程式码的期间将效能分析（profiling）的输出纳入考量，并根据效能分析摆放程式区块。我们将会在第七节看到这是如何运作的。第二个方法则是藉由明确的分支预测。gcc 认得 \_\_builtin\_expect：

```text
long __builtin_expect(long EXP, long C);
```

这个结构告诉编译器，表示式 EXP 的值非常有可能会是 C。回传值为 EXP。\_\_builtin\_expect 必须被用在条件表示式中。在几乎所有的情况中，它会被用在布林表示式的情境中，在这种情况下定义两个辅助巨集（macro）要更方便一些：

```text
#define unlikely(expr) __builtin_expect(!!(expr), 0)
#define likely(expr) __builtin_expect(!!(expr), 1)
```

然后可以像这样用这些巨集

```text
if (likely(a > 1))
```

若是程式设计师使用了这些巨集、然后使用了 -freorder-blocks 最佳化选项，gcc 会如上图那样重新排列区块。这个选项会随著 -O2 启用，但对于 -Os 会被停用。有另一个重新排列区块的 gcc 选项（-freorder-blocks-and-partition），但它的用途有限，因为它不适用于例外处理。

还有另一个小迴圈的大优点，至少在某些处理器上。Intel Core 2 前端有一个特殊的功能，称作迴圈指令流检测器（Loop Stream Detector，LSD）。若是一个迴圈拥有不多于 18 条指令（没有一个是对子程式〔routine〕的呼叫）、仅要求至多 4 次 16 位元组的解码器撷取、拥有至多 4 条分支指令、并且被执行超过 64 次，那麽这个迴圈有时会被锁在指令伫列中，因而在迴圈被再次用到的时候能够更为快速地使用。举例来说，这适用于会通过一个外部迴圈进入很多次的很小的内部迴圈。即使没有这种特化的硬体，小巧的迴圈也有优点。

就 L1i 而言，行内展开并非最佳化的唯一面向。另一个面向是对齐，就如资料一般。但有些明显的差异：程式大部分是线性的一团，其无法任意地摆在位址空间中，而且它无法直接受程式设计师影响，因为是编译器产生这些程式的。不过，有些程式设计师能够控制的面向。

对齐每条单一指令没有任何意义。目标是令指令流为连续的。所以对齐仅在战略要地上才有意义。为了决定要在何处加上对齐，理解能有什麽好处是必要的。有条在一个快取行开头的指令代表快取行的预取是最大化的。对指令而言，这也代表著解码器是更有效的。很容易看出，若是执行了一条在快取行结尾的指令，处理器就必须准备读取一个新的快取行、并对指令解码。有些事情可能会出错（像是快取行错失），代表平均而言，一条在快取行结尾的指令执行起来并不跟在开头的指令一样有效。

Combine this with the follow-up deduction that the problem is most severe if control was just transferred to the instruction in question \(and hence prefetching is not effective\) and we arrive at our final conclusion where alignment of code is most useful:

* 在函数的开头； 
* 在仅会通过跳转到达的基础区块的开头； 
* 对某些扩充而言，在迴圈的开头。

在前两种情况下，对齐的成本很小。在一个新的位置继续执行，假如决定让它在快取行的开头，我们便最佳化了预取与解码。编译器藉由无操作（no-op）指令的插入，填满因对齐程式产生的间隔，而实现了这种对齐。这种「死码（dead code）」佔用了一些空间，但通常不伤及效能。

第三种情况略有不同：对齐每个迴圈的开头可能会造成效能问题。问题在于，一个迴圈的开头往往是连续地接在其它的程式码之后。若是情况不是非常凑巧，便会有个在前一条指令与被对齐的迴圈开头之间的间隔。不像前两种情况，这个间隔无法完全不造成影响。在前一条指令执行之后，必须执行迴圈中的第一条指令。这表示，在前一条指令之后，要不是非得有若干条无操作指令以填补间隔、要不就是非得有个到迴圈开头的无条件跳转。两种可能性都不是免费的。特别是迴圈本身并不常被执行的话，无操作指令或是跳转的开销可能会比对齐迴圈所省下的还多。

有三种程式设计师能够影响程式对齐的方法。显然地，若是程式是以组合语言撰写，其中的函数与所有的指令都能够被明确地对齐。组合语言为所有架构提供了 .align 假指令（pseudo-op）以做到这点。对高阶语言而言，必须将对齐需求告知编译器。不像资料型别与变数那样，这在原始码中是不可能的。而是要使用一个编译器选项：

`-falign-functions=N`

这个选项命令编译器将所有函数对齐到下一个大于 N 的二的幂次的边界。这表示会产生一个至多 N 位元组的间隔。对小函数而言，使用一个很大的 N 值是个浪费。对只有难得才会执行的程式也相同。在可能同时包含常用与没那麽常用的介面的函式库中，后者可能经常发生。选项值的明智选择可以藉由避免对齐来让工作加速或是节省记忆体。能够藉由使用 1 作为 N 的值、或是使用 -fno-align-functions 选项来关掉所有的对齐。

有关前述的第二种情况的对齐––无法循序达到的基础区块的开头––能够使用一个不同的选项来控制：

`-falign-jumps=N`

所有其它的细节都相同，关于浪费记忆体的警告也同样适用。

第三種情況也有它自己的選項：

`-falign-loops=N`

再一次，同样的细节与警告都适用。除了在这裡，如同先前解释过的，对齐会造成执行期的成本，因为在对齐的位址会被循序地抵达的情况下，要不是必须执行无操作指令就是必须执行跳转指令。

gcc 还知道一个用来控制对齐的选项，在这裡提起它仅是为了完整起见。-falign-labels 对齐了程式中的每个单一标籤（label）（基本上是每个基础区块的开头）。除了一些例外状况之外，这都会让程式变慢，因而不该被使用。

### 6.2.3 最佳化二階與更高階快取存取

关于一阶快取的最佳化所说的一切也适用于二阶与更高阶快取存取。有两个最后一层快取的额外面向：

* 快取错失一直都非常昂贵。L1 错失（希望）频繁地命中 L2 与更高阶快取，于是限制了其损失，但最后一层快取显然没有后盾了。
* L2 快取与更高阶快取经常由多颗核心与／或超执行绪所共享。每个执行单元可用的有效快取大小因而经常小于总快取大小。

为了避免快取错失的高成本，工作集大小应该配合快取大小。若是资料只需要一次，这显然不是必要的，因为快取无论如何都没有效果。我们要讨论的是被需要不只一次的资料集的工作负载。在这种情况下，使用一个太大而不能塞得进快取的工作集将会产生大量的快取错失，即使预取成功地执行了，也会拖慢程式。

即使资料集太大，一支程式也必须完成它的职责。以最小化快取错失的方式完成工作是程式设计师的职责。对于末层快取，是可能––如同 L1 快取––以较小的部分来执行工作的。这与表 6.2 最佳化的矩阵乘法非常雷同。不过，有一点不同在于，对于最后一层快取，要处理的资料区块可能比较大。如果也需要 L1 最佳化，程式会变得更加複杂。想像一个矩阵乘法，其资料集––两个输入矩阵与输出矩阵––无法同时塞进最后一层快取。在这种情况下，或许适合同时最佳化 L1 与最后一层快取存取。

众多处理器世代中的 L1 快取行大小经常是固定的；即使不同，差异也很小。假设为较大的大小是没什麽大问题的。在有著较小快取大小的处理器中，会用到两个或更多快取行、而非一个。在任何情况下，写死快取行大小、并为此最佳化程式都是合理的。

对于较高层级的快取，若程式是假定为一般化的话，就不是这样了。那些快取的大小可能有很大的差异。八倍或更多倍并不罕见。将较大的快取大小假定为预设大小是不可能的，因为这可能表示，除了那些有著最大快取的机器之外，程式在所有机器上都会表现得很差。相反的选择也很糟：假定为最小的快取，代表浪费掉 87% 或者更多的快取。这很糟；如同我们能从图 3.14 看到的，使用大快取对程式的速度有著巨大的影响。

这表示程式必须动态地将自身调整为快取行大小。这是一种程式特有的最佳化。我们这裡能说的是，程式设计师应该正确地计算程式的需求。不仅资料集本身需要，更高层级的快取也会被用于其它目的；举例来说，所有执行的指令都是从快取载入的。若是使用了函式库函式，这种快取的使用可能会加总为一个可观的量。那些函式库函式也可能需要它们自己的资料，进一步减少了可用的记忆体。

一旦我们有一个记忆体需求的公式，我们就能够将它与快取大小作比较。如同先前所述，快取可能会被许多其它核心所共享。当前^34，在没有写死知识的情况下，取得正确资讯的唯一方法是透过 /sys 档案系统。在表 5.2，我们已经看过系统核心发布的有关于硬体的资讯。程式必须在目录：

`/sys/devices/system/cpu/cpu*/cache`

找到最后一层快取。这能够由在这个目录裡的层级档案中的最高数值来辨别出来。当目录被识别出来时，程式应该读取在这个目录中的 size 档案的内容，并将数值除以 shared\_cpu\_map 档案中的位元遮罩中设置的数字。

以这种方式计算的值是个安全的下限。有时一支程式会知道多一些有关其它执行绪或行程的行为。若是那些执行绪被排程在共享这个快取的核心或超执行绪上、并且已知快取的使用不会耗尽它在总快取大小中所佔的那份，那麽计算出的限制可能会太小，而不是最佳的。是否要比公平共享应该使用的还多，真的要视情况而定。程式设计师必须做出抉择，或者必须让使用者做个决定。

### 6.2.4 最佳化 TLB 使用

有两种 TLB 使用的最佳化。第一种最佳化是减少一支程式必须使用的分页数。这会自动导致较少的 TLB 错失。第二种最佳化是藉由减少必须被分配的较高层目录表的数量，以令 TLB 查询便宜一些。较少的目录表代表使用的记忆体较少，这可能使得目录查询有较高的快取命中率。

第一种最佳化与分页错误的最小化密切相关。我们将会在 7.5 节仔细地涵盖这个主题。分页错误经常是个一次性的成本，但由于 TLB 快取通常很小而且会被频繁地冲出，因此 TLB 错失是个长期的损失。分页错误比起 TLB 错失还贵了数个数量级，但若是一支程式跑得足够久、而且程式的某些部分会被足够频繁地执行，TLB 错失甚至可能超过分页错误的成本。因此重要的是，不仅要从分页错误的角度、也要从 TLB 错失的角度来考虑分页最佳化。差异在于，分页错误的最佳化只要求分页范围内的程式码与资料分组，而 TLB 最佳化则要求––在任何时间点––尽可能少的 TLB 项目。

第二种 TLB 最佳化甚至更难控制。必须使用的分页目录数量是视行程的虚拟位址空间中使用的位址范围分佈而定的。位址空间中广泛多样的位置代表著更多的目录。

一个难题是，位址空间佈局随机化（Address Space Layout Randomization，ASLR）恰好造成了这种状况。堆叠、DSO、堆积、与可能的可执行档的载入位址会在执行期随机化，以防止机器的攻击者猜出函数或变数的位址。

只有在最大效能至关重要的情况下，才应该关掉 ASLR。额外目录的成本低到足以令这步是不必要的，除了一些极端的状况之外。系统核心能随时执行的一个可能的最佳化是，确保一个单一的映射不会横跨两个目录之间的记忆体空间边界。这会以最小的方式限制 ASLR，但不足以大幅地削弱它。

程式設計師直接受此影響的唯一方式是在明確請求一個位址空間區域的時候。這會在以 `MAP_FIXED` 使用 `mmap` 的時候發生。 Allocating new a address space region this way is very dangerous and hardly ever done. It is possible, though, and, if it is used and the addresses can be freely chosen, the programmer should know about the boundaries of the last level page directory and select the requested address appropriately.

## 6.3 预取

预取的目的是隐藏记忆体存取的等待时间。现今处理器的命令管道与无序（out-of-order，OOO）执行的功能能够隐藏一些等待时间，但最多也只是对命中快取的存取而言。要掩盖主记忆体存取的等待时间，命令伫列可能得要非常地长。某些没有 OOO 的处理器试著藉由提高核心数来补偿，但除非所有使用的程式码都被平行化，否则这是个不太好的交易。

预取能进一步帮助隐藏等待时间。处理器靠它自己执行预取，由某些事件触发（硬体预取）或是由程式明确地请求（软体预取）。

### 6.3.1 硬體預取

CPU 启动硬体预取的触发，通常是二或多个快取错失的某种模式的序列。这些快取错失可能在快取行之前或之后。在旧的实作中，只有邻近快取行的快取错失会被识别出来。使用当代硬体，步伐也会被识别出来，代表跳过固定数量的快取行会被识别为一种模式并被适当地处理。

若每次单一的快取错失都会触发一次硬体预取，对于效能来说大概很糟。随机记忆体存取模式––例如存取全域变数––是非常常见的，而产生的预取会大大地浪费 FSB 频宽。这即是为何启动预取需要至少两次快取错失。处理器现今全都预期有多于一条记忆体存取的串流。处理器试著自动将每个快取错失指派给这样的一条串流，并且在达到门槛时启动硬体预取。CPU 现今能追踪更高阶快取的八到十六条单独的串流。

负责模式识别的单元与各自的快取相关联。可以有一个 L1d 与 L1i 快取的预取单元。很可能有一个 L2 与更高阶快取的预取单元。L2 与更高阶快取的预取单元是被所有使用相同快取的其它核心与超执行绪所共享。 The number of eight to sixteen separate streams therefore is quickly reduced.

预取有个大弱点：它无法跨越分页边界。理解到 CPU 支援需求分页（demand paging）时，原因应该很明显。若是预取被允许横跨分页边界，存取可能会触发一个事件，以令分页能够被取得。这本身可能很糟，尤其是对效能而言。更糟的是预取器并不知道程式或 OS 本身的语义（semantic）。它可能因此预取了实际上永远不会被请求的分页。 That means the prefetcher would run past the end of the memory region the processor accessed in a recognizable pattern before. 这不只可能，而且非常有可能。若是处理器––作为一次预取的一个副作用––触发了对这样的分页的请求，OS 甚至可能会在这种请求永远也不会发生时完全扔掉它的追踪纪录。

因此重要的是认识到，无论预取器在预测模式上有多厉害，程式也会在分页边界上历经快取错失，除非它明确地从新的分页预取或是读取。这是如 6.2 节描述的最佳化资料佈局、以藉由将不相关的资料排除在外来最小化快取污染的另一个理由。

由于这个分页限制，处理器现今并没有非常複杂的逻辑来识别预取模式。以仍佔主导地位的 4k 分页大小而言，有意义的也就这麽多了。这些年来已经提高了识别步伐的位址范围，但超过现今经常使用的 512 位元组窗格（window）可能没太大意义。当前的预取单元并不认得非线性的存取模式。这种模式较有可能是真的随机、或者至少足够不重複到令试著识别它们不具意义。

若是硬体预取被意外地触发，能做的只有这麽多。一个可能是试著找出这个问题，并稍微改变资料与／或程式佈局。这大概满困难的。可能有特殊的在地化（localized）解法，像是在 x86 与 x86-64 处理器上使用 ud2 指令。这个无法自己执行的指令是在一条间接的跳转指令后被使用；它被作为指令获取器（fetcher）的一个信号使用，表示处理器不应浪费精力解码接下来的记忆体，因为执行将会在一个不同的位置继续。不过，这是个非常特殊的情况。在大部分情况下，必须要忍受这个问题。

能够完全或部分地停用整个处理器的硬体预取。在 Intel 处理器上，一个特定模型暂存器（Model Specific Register，MSR）便用于此（IA32\_MISC\_ENABLE，在许多处理器上为位元 9；位元 19 只停用邻近快取行预取）。这在大多情况下必须发生在系统核心中，因为它是个特权操作。若是数据分析显示，执行于系统上的一个重要的应用程式因硬体快取而遭受了频宽耗竭与过早的快取逐出，使用这个 MSR 是一种可能性。

\[^35\]: Or non-instruction. It is the recommended undefined opcode.

### 6.3.2 軟體預取

硬体预取的优势在于不必调整程式。缺点如同方才描述的，存取模式必须很直观，而且预取无法横跨分页边界进行。因为这些原因，我们现在有了更多可能性，软体预取它们之中最重要的。软体预取不需藉由插入特殊的指令来修改原始码。某些编译器支援编译指示（pragma）以或多或少地自动插入预取指令。 On x86 and x86-64 Intel’s convention for compiler intrinsics to insert these special instructions is generally used:

```text
#include <xmmintrin.h>
enum _mm_hint
{
  _MM_HINT_T0 = 3,
  _MM_HINT_T1 = 2,
  _MM_HINT_T2 = 1,
  _MM_HINT_NTA = 0
};
void _mm_prefetch(void *p,
                  enum _mm_hint h);
```

程式能够在程式中的任何指标上使用 \_mm\_prefetch 内部函数。许多处理器（当然包含所有 x86 与 x86-64 处理器）都会忽略无效指标产生的错误，这令程式设计师的生活好过非常多。若是被传递的指标指向合法的记忆体，会命令预取单元将资料载入到快取中，并且––必要的话––逐出其它资料。不必要的预取应该被确实地避免，因为这会降低快取的有效性，而且它会耗费记忆体频宽（在被逐出的快取行是髒的情况下，可能需要两个快取行的频宽）。

要与 \_mm\_prefetch 一起使用的不同提示（hint）是由实作定义的。这表示每个处理器版本能够（稍微）不同地实作它们。一般能说的是，\_MM\_HINT\_T0 会为包含式快取将资料获取到所有快取层级，并为独占式快取获取到最低层级的快取。若是资料项目在较高层级的快取中，它会被载入到 L1d 中。\_MM\_HINT\_T1 提示将资料拉进 L2 而非 L1d。若是有个 L3 快取，\_MM\_HINT\_T2 能做到类似于此的事情。不过，这些是没怎麽被明确指定的细节，需要对所使用的实际处理器进行验证。一般来说，若是资料在使用 \_MM\_HINT\_T0 之后立刻被用到就没错了。当然这要求 L1d 快取大小要大得足以容纳所有被预取的资料。若是立即被使用的工作集大小太大，将所有东西预取到 L1d 就是个坏点子，而应该使用其它两种提示。

第四种提示，\_MM\_HINT\_NTA 能够吩咐处理器特殊地对待预取的快取行。NTA 代表非暂存对齐（non-temporal aligned），我们已经在 6.1 节解释过了。程式告诉处理器应该尽可能地避免以这个资料污染快取，因为资料只在一段很短的期间内会被使用。对于包含式快取实作，处理器因而能够在载入时避免将资料读取进较低层级的快取。当资料从 L1d 逐出时，资料不必被推进 L2 或更高层级的快取中，但能够直接写到记忆体中。可能有其它处理器设计师在给定这个提示时能够部属的其它手法。程式设计师必须谨慎地使用这个提示：若是当前的工作集大小太大，并强制逐出以 NTA 提示载入的快取行，就要重新从记忆体载入。

[![&#x5716; 6.7&#xFF1A;&#x4F7F;&#x7528;&#x9810;&#x53D6;&#x7684;&#x5E73;&#x5747;&#xFF0C;NPAD=31](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.7.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.7.png)

圖 6.7：使用預取的平均，NPAD=31

图 6.7 显示了使用现已熟悉的指标追逐框架（pointer chasing framework）的测试结果。串列是随机地被摆放在记忆体中的。与先前测试的不同之处在于，程式真的会在每个串列节点上花一些时间（大约 160 週期）。如同我们从图 3.15 的数据中学到的，一旦工作集大小大于最末层快取，程式的效能就会受到严重的影响。

我们现在能够试著在计算之前发出预取请求来改善这种状况。即，我们在迴圈的每一轮预取一个新元素。串列中被预取的节点与正在处理的节点之间的距离必须被谨慎地选择。假定每个节点在 160 週期内被处理、并且我们必须预取两个快取行（NPAD=31），五个串列元素的距离便足够了。

图 6.7 的结果显示了预取确实有帮助。只要工作集大小不超过最后一层快取的大小（这台机器拥有 512kB = 219B 的 L2），数字就是相同的。预取指令并不会增加能量测出来的额外负担。一旦超过 L2 大小，预取省下了 50 到 60 週期之间，高达 8%。预取的使用无法隐藏任何损失，但它稍微有点帮助。

AMD 在它们 Opteron 产品线的 10h 家族实作了另一个指令：prefetchw。在 Intel 这边迄今仍没有这个指令的等价物，也不能透过内部函数使用。prefetchw 指令命令 CPU 将快取行预取到 L1 中，就如同其它预取指令一样。差异在于快取行会立即变成「M」状态。若是之后没有接著对快取行的写入，这将会是个不利之处。但若是有一或多次写入，它们将会被加速，因为写入操作不必改变快取状态––其在快取行被预取时就被设好了。这对于竞争的快取行尤为重要，其中在另一个处理器的快取中的快取行的一次普通的读取操作会先在两个快取中将状态改成「S」。

预取可能有比我们这裡达到的微薄的 8% 还要更大的优势。但它是众所皆知地难以做得正确，尤其是在预期相同的二元档在各种各样的机器上都表现良好的情况。由 CPU 提供的效能计数器能够帮助程式设计师分析预取。能够被计数并取样的事件包含硬体预取、软体预取、有用的／使用的软体预取、在不同层级的快取错失、等等。在 7.1 节，我们将会介绍这些事件。这所有的计数器都是机器特有的。

在分析程式时，应该要先看看快取错失。找出大量快取错失来源的所在时，应该试著针对碰上问题的记忆体存取加上预取指令。这应该一次处理一个地方。每次修改的结果应该藉由观察量测有用预取指令的效能计数器来检验。若是那些计数器没有提升，那麽预取可能是错的，它并没有给予足够的时间来从记忆体载入，或者预取从快取逐出了仍然需要的记忆体。

gcc 现今能够在唯一一种情况下自动发出预取指令。若是一个迴圈叠代在一个阵列上，能够使用下面的选项：

`-fprefetch-loop-arrays`

编译器会计算出预取是否合理，以及––如果是的话––它应该往前看多远。对小阵列而言，这可能是个不利之处，而且若是在编译期不知道阵列的大小的话，结果可能更糟。gcc 手册提醒道，这个好处极为仰赖于程式码的形式，而在某些情况下，程式可能真的会跑得比较慢。程式设计师必须谨慎地使用这个选项。

### 6.3.3 特殊的預取類型：猜測

一个现代处理器的 OOO 执行能力允许在不与彼此衝突的情况下搬移指令。举例来说（这次使用 IA-64 为例）：

```text
st8        [r4] = 12
add        r5 = r6, r7;;
st8        [r18] = r5
```

这段程式序列将 12 储存至由暂存器 r4 指定的位址、将 r6 与 r7 暂存器的内容相加、并将它储存在暂存器 r5 中。最后，它将总和储存至由暂存器 r18 指定的位址。这裡的重点在于，加法指令能够在第一个 st8 指令之前––或者同时––执行，因为并没有资料的依赖关係。但假如必须载入其中一个加数会怎麽样呢？

```text
st8        [r4] = 12
ld8        r6 = [r8];;
add        r5 = r6, r7;;
st8        [r18] = r5
```

额外的 ld8 指令将值载入到由 r8 指令的位址。在这个载入指令与接下来的 add 指令之间有个明确的资料依赖关係（这便是指令后面的 ;; 的理由了，感谢提问）。这裡的关键在于，新的 ld8 指令––不若 add 指令––无法被移到第一个 st8 前面。处理器无法在指令解码的期间足够快速地决定储存与载入是否衝突––即，r4 与 r8 是否可能有相同的值。假如它们有相同的值，st8 指令会决定载入到 r6 的值。更糟的是，在载入错失快取的情况下，ld8 可能也会随之带来漫长的等待时间。IA 64 架构针对这种情况支援了猜测式载入（speculative load）：

```text
ld8.a      r6 = [r8];;
[... other instructions ...]
st8        [r4] = 12
ld8.c.clr  r6 = [r8];;
add        r5 = r6, r7;;
st8        [r18] = r5
```

新的 ld8.a 与 ld8.c.clr 指令是一对的，并取代了前一段程式序列的 ld8 指令。ld8.a 为猜测式载入。这个值无法被直接使用，但处理器能开始运作。这时，当到达 ld8.c.clr 指令的时候，这个内容可能已经被载入了（假定这个间隔中有足够数量的指令）。这个指令的引数（argument）必须与 ld8.a 指令相符。若是前面的 st8 指令没有覆写这个值（即 r4 与 r8 相同），就什麽也不必做。猜测式载入做了它的工作，而载入的等待时间被隐藏了。若是载入与储存衝突了，ld8.c.clr 会重新从记忆体载入值，而我们最终会得到一个正常的 ld8 指令的语义。

猜测式载入（仍？）没有被广泛使用。但如同这个例子所显示的，它是个非常简单而有效的隐藏等待时间的方法。预取基本上是等同的东西，并且对有著少量暂存器的处理器而言，猜测式载入可能没多大意义。猜测式载入有直接将值载入到暂存器中，而不载入到可能会被再次逐出的快取行（举例来说，当执行绪被移出排程〔deschedule〕的时候）这个（有时很大的）优点。如果能够使用猜测的话，应该要使用它。

\[^譯註\]: `r4` 與 `r8` 相同指的是「值會被覆寫的情況」。

### 6.3.4 輔助執行緒

在尝试使用软体预取时，往往会碰到程式複杂度的问题。若是程式必须叠代于一个资料结构上（在我们的情况中是个串列），必须在同个迴圈中实作两个独立的叠代：执行作业的普通叠代、与往前看以使用预取的第二个叠代。这轻易地变得足够複杂到容易产生失误。

此外，决定要往前看多远是必要的。太短的话，记忆体将无法及时被载入。太远的话，刚载入的资料可能会被再一次逐出。另一个问题是，虽然它不会阻挡或等待记忆体载入，但预取指令很花时间。指令必须被解码，假如解码器太忙碌的话––举例来说，由于良好撰写／产生的程式码––这可能很明显。最后，迴圈的程式大小会增加。这降低了 L1i 的效率。若藉由一次发出多个预取指令来试著避免部分成本，则会碰到显著的预取请求数的问题。

一个替代方法是完全独立地执行一般的操作与预取。这能使用两条普通的执行绪来进行。执行绪显然必须被排程，以令预取执行绪填充一个被两条执行绪存取的快取。有两个值得一提的特殊解法：

* 在相同的核心上使用超执行绪（见 3.3.4 节，超执行绪）。在这种情况下，预取能够进入 L2（或者甚至是 L1d）。
* 使用比 SMT 执行绪「更愚笨的（dumber）」执行绪，其除了预取与其它简单的操作之外什麽也不做。这是个处理器厂商可能会探究的选项。

超执行绪的使用是尤其令人感兴趣的。如同我们已经在 3.3.4 节看到的，假如超执行绪执行独立的程式码的话，快取的共享是个问题。反而，在一条执行绪被用作一条预取辅助执行绪（helper thread）时，这并不是个问题。与此相反，这是个令人渴望的结果，因为最低层级的快取被预载了。此外，由于预取执行绪大多是空閒或者在等待记忆体，所以假如不必自己存取主记忆体的话，其馀超执行绪的一般操作并不会太受干扰。后者正好是预取辅助执行绪所预防的。

唯一棘手的部分是确保辅助执行绪不会往前跑得太远。它不能完全污染快取，以致最早被预取的值被再次逐出。在 Linux 上，使用 futex 系统呼叫 \[7\] 或是––以稍微高一些的成本––使用 POSIX 执行绪同步基本指令（primitive），是很容易做到同步的。

[![&#x5716; 6.8&#xFF1A;&#x4F7F;&#x7528;&#x8F14;&#x52A9;&#x57F7;&#x884C;&#x7DD2;&#x7684;&#x5E73;&#x5747;&#xFF0C;NPAD=31](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.8.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.8.png)

圖 6.8：使用輔助執行緒的平均，NPAD=31

这个方法的好处能够在图 6.8 中看到。这是与图 6.7 中相同的测试，只不过加上了额外的结果。新的测试建立了一条额外的辅助执行绪，往前执行大约 100 个串列项目，并读取（不只预取）每个串列元素的所有快取行。在这种情况下，我们每个串列元素有两个快取行（在一台有著 64 位元组快取行大小的 32 位元机器上，NPAD=31）。

两条执行绪被排程在相同核心的两条超执行绪上。测试机仅有一颗核心，但结果应该与多于一颗核心的结果大致相同。亲和性函数––我们将会在 6.4.3 节介绍––被用来将执行绪绑到合适的超执行绪上。

要确定 OS 知道哪两个（或更多）处理器为超执行绪，可以使用来自 libNUMA 的 NUMA\_cpu\_level\_mask 介面（见附录 D）。

```text
#include <libNUMA.h>
ssize_t NUMA_cpu_level_mask(size_t destsize,
                            cpu_set_t *dest,
                            size_t srcsize,
                            const cpu_set_t*src,
                            unsigned int level);
```

这个介面能用来决定透过快取与记忆体连结的 CPU 阶层架构。这裡感兴趣的是对应于超执行绪的一阶快取。为了在两条超执行绪上排程两条执行绪，能够使用 libNUMA 函数（为了简洁起见，省略了错误处理）：

```text
cpu_set_t self;
NUMA_cpu_self_current_mask(sizeof(self),
                           &self);
cpu_set_t hts;
NUMA_cpu_level_mask(sizeof(hts), &hts,
                    sizeof(self), &self, 1);
CPU_XOR(&hts, &hts, &self);
```

在执行这段程式之后，我们有两个 CPU 位元集。self 能用来设定当前执行绪的亲和性，而 hts 中的遮罩能被用来设定辅助执行绪的亲和性。这在理想上应该在执行绪被建立前发生。在 6.4.3 节，我们会介绍设定亲和性的介面。若是没有可用的超执行绪，NUMA\_cpu\_level\_mask 函数会回传 1。这能够用以作为避免这个最佳化的徵兆。

这个基准测试的结果可能出乎意料（也可能不会）。若是工作集塞得进 L2，辅助执行绪的间接成本将效能降低了 10% 到 60% 之间（主要在比较低的那端，再次忽略最小的工作集大小，杂讯太多了）。这应该在预料之中，因为若是所有资料都已经在 L2 快取中，预取辅助执行绪仅仅使用了系统资源，却没有对执行有所贡献。

不过，一旦不再足够的 L2 大小耗尽，情况就改变了。预取辅助执行绪协助将执行时间降低了大约 25%。我们仍旧看到一条上升的曲线，只不过是因为无法足够快速地处理预取。不过，主执行绪执行的算术操作与辅助执行绪的记忆体载入操作彼此互补。资源衝突是最小的，其导致了这种相辅相成的结果。

这个测试的结果应该能够被转移到更多其它的情境。由于快取污染而经常无用的超执行绪，在这些情境中表现出众，并且应该被善用。附录 D 介绍的 NUMA 函式库令执行绪兄弟的找寻非常容易（见这个附录中的范例）。若是函式库不可用，sys 档案系统令一支程式能够找出执行绪的兄弟（见表 5.3 的 thread\_siblings 栏位）。一旦能够取得这个资讯，程式就必须定义执行绪的亲和性，然后以两种模式执行迴圈：普通的操作与预取。被预取的记忆体总量应该视共享的快取大小而定。在这个例子中，L2 大小是有关的，程式能够使用

`sysconf(_SC_LEVEL2_CACHE_SIZE)`

来查询大小。辅助执行绪的进度是否必须被限制取决于程式。一般来说，最好确定有一些同步，因为排程细节可能会导致显著的效能降低。

### 6.3.5 直接快取存取

在现代 OS 中，快取错失的一个来源是到来的资料流量的处理。像网路介面卡（Network Interface Card，NIC）与硬碟控制器等现代硬体，能够在不涉及 CPU 的情况下，直接将接收或读取的资料写入到记忆体中。这对于我们现今拥有的装置的效能而言至关重要，但它也造成了问题。假使有个从网路传入的封包：OS 必须检查封包的标头（header）以决定要如何处理它。NIC 将封包摆进记忆体，然后通知处理器它的到来。处理器没有机会去预取资料，因为它并不知道资料将何时抵达，甚至可能不会确切知道它将会被存在哪。结果是在读取标头时的一次快取错失。

Intel 已经在它们的晶片组与 CPU 中加上了技术以缓解这个问题 \[14\]。构想是将封包的资料填入将会被通知到来的封包的 CPU 的快取。封包的承载内容在这裡并不重要，这个资料一般将会由更高阶的函数––要不是在系统核心中、就是在使用者层级––处理。封包标头被用来决定封包必须以什麽方式处理，因此这个资料是立即所需的。

网路 I/O 硬体已有 DMA 以写入封包。这表示它直接地与潜在整合在北桥中的记忆体控制器进行沟通。记忆体控制器的另一边是通过 FSB 到处理器的介面（假设记忆体控制器没有被整合到 CPU 自身）。

[![\(a\) &#x555F;&#x52D5; DMA](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.9a.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.9a.png)\(a\) 啟動 DMA

[![\(b\) &#x57F7;&#x884C; DMA &#x8207; DCA](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.9b.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.9b.png)\(b\) 執行 DMA 與 DCA 圖 6.9：直接快取存取

直接快取存取（Direct Cache Access，DCA）背后的想法是，扩充 NIC 与记忆体控制器之间的通讯协定。在图 6.9 中，第一张图显示了在一台有著南北桥的正规机器上的 DMA 传输的起始。NIC 被连接到南桥上（或作为其一部分）。它启动了 DMA 存取，但提供了关于封包标头的新资讯，其应该被推进处理器的快取中。

在第二步中，传统的行为仅会是以连结到记忆体的连线完成 DMA 传输。对于被设置 DCA 旗标的 DMA 传输，北桥会以特殊的、新的 DCA 旗标在 FSB 上同时送出资料。处理器一直窥探著 FSB，并且若是它认出了 DCA 旗标，它会试著将寄给处理器的资料载入到最低阶快取中。事实上，DCA 旗标是个提示；处理器能够自由地忽略它。在 DMA 传输完成之后，会以信号通知处理器。

在处理封包时，OS 必须先确定是哪种封包。若是 DCA 提示没有被忽略的话，OS 必须执行、以识别封包的载入操作很有可能会命中快取。将每个封包数以百计个循环的节约，乘上每秒能处理的成千上万个封包，节省的加总量是个非常可观的数字，尤其在谈到等待时间的时候。

少了 I/O 硬体（在这个例子中为 NIC）、晶片组与 CPU 的整合，这种最佳化是不可能的。因此，假如需要这个技术的话，确保明智地挑选平台是必要的。

## 6.4 多執行緒最佳化

关于多执行绪，有三个快取使用的面向是很重要的：

* 并行（Concurrency）
* 原子性（Atomicity）
* 带宽

这些面向也适用于多行程的情况，但因为多行程（大多数）是独立的，因此为它们最佳化并没有那麽容易。可能的多行程最佳化是那些可用于多执行绪情况的子集。所以这裡我们会专门讨论后者。

在这种前后文下，并行指的是在一次执行多于一条执行绪时，一个行程所历经的记忆体影响。执行绪的一个特性是它们全都共享相同的位址空间，因此全都能够存取相同的记忆体。在理想的情况下，执行绪所使用的记忆体区域在多数时候都是不同的。在这种情况下，那些执行绪仅稍许耦合（couple）（举例来说，共有的输入与／或输出）。若是多于一条执行绪使用了相同的资料，就需要协调了：这即是原子性发挥作用的时候。最后，视机器架构而定，可用的记忆体与可用于处理器的处理器之间的汇流排频宽是有限的。我们将会在接下来的章节分别论及这三个面向––虽然它们是紧密相连的。

### 6.4.1 並行最佳化

一开始，我们将会在本节讨论两个个别的议题，其实际上需要对立的最佳化。一个多执行绪应用程式在一些它的执行绪中使用共有的资料。一般的快取最佳化要求将资料保存在一起，使得应用程式的记忆体使用量很小，从而最大化在任意时间塞得进快取的记忆体总量。[^譯註1](https://github.com/jason2506/cpumemory.zh-tw/blob/master/what-programmers-can-do/multi-thread-optimizations/%E5%9B%A0%E7%82%BA%E5%BF%AB%E5%8F%96%E7%9A%84%E6%9C%80%E5%B0%8F%E5%96%AE%E4%BD%8D%E7%82%BA%E5%BF%AB%E5%8F%96%E8%A1%8C%E3%80%82%E5%9B%A0%E6%AD%A4%E8%8B%A5%E6%98%AF%E8%B3%87%E6%96%99%E6%93%BA%E5%9C%A8%E4%B8%80%E8%B5%B7%EF%BC%8C%E4%BB%A3%E8%A1%A8%E5%AE%83%E5%80%91%E6%89%80%E4%BD%94%E7%94%A8%E7%9A%84%E5%BF%AB%E5%8F%96%E8%A1%8C%E6%95%B8%E9%87%8F%E8%BC%83%E5%B0%91%EF%BC%8C%E5%9B%A0%E6%AD%A4%E4%B8%80%E6%AC%A1%E8%83%BD%E5%BF%AB%E5%8F%96%E7%9A%84%E8%B3%87%E6%96%99%E9%87%8F%E5%B0%B1%E8%AE%8A%E5%A4%9A%E4%BA%86%E3%80%82)

不过，使用这个方法有个问题：若是多条执行绪写入到一个记忆体位置，每个相对应核心的 L1d 中的快取行必须处于「E」（独占）状态。这表示会送出许多的 RFO 讯息。在最糟的情况下，每次写入存取都会送出一个讯息。所以一个普通的写入将会突然变得非常昂贵。若是使用了相同的记忆体位置，同步就是必须的（可能透过原子操作的使用，其会在下个章节讨论到）。不过，当所有执行绪都使用了不同的记忆体位置、并且可能是独立的时候，问题也显而易见。

[![&#x5716; 6.10&#xFF1A;&#x4E26;&#x884C;&#x5FEB;&#x53D6;&#x884C;&#x5B58;&#x53D6;&#x7684;&#x9593;&#x63A5;&#x6210;&#x672C;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.10.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.10.png)圖 6.10：並行快取行存取的間接成本

图 6.10 显示了这种「假共享（false sharing）」的结果。测试程式（显示于 A.3 节）建立了若干执行绪，其除了递增一个记忆体位置（5 亿次）外什麽也不做。量测的时间是从程式启动、直到程式等待最后一条执行绪结束之后。执行绪被钉在独立的处理器上。机器拥有四个 P4 处理器。蓝色值表示被指派到每条执行绪的记忆体分配位在个别快取行上的执行时间。红色部分为执行绪的位置被移到仅一个快取行时出现的损失。

蓝色的量测（使用独立的快取行时所需的时间）与预期的相符。程式在无损失的情况下延展至多条执行绪。每个处理器都将它的快取行保存在它拥有的 L1d 中，而且没有频宽问题，因为不必读取太多程式码或资料（事实上，它们全都被快取了）。量测的些微提升其实是系统的杂讯、和可能的一些预取影响（执行绪使用连续的快取行）。

使用唯一一个快取行所需的时间、以及每条执行绪一个个别的快取行所需的时间相除所计算出的量测的间接成本分别是 390%、734%、以及 1,147%。乍看之下，这些很大的数字可能很令人吃惊，但考虑到需要的快取交互影响，这应该很显而易见。已经完成写入到快取行之后，就从一个处理器的快取拉出快取行。^译注2在任何给定的时刻，除了拥有快取行的处理器以外，所有处理器都会被延迟，无法做任何事。每个额外的处理器都会导致更多的延迟。

[![&#x5716; 6.11&#xFF1A;&#x56DB;&#x6838;&#x7684;&#x9593;&#x63A5;&#x6210;&#x672C;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.11.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.11.png)圖 6.11：四核的間接成本

由于这些量测，清楚的是这种情况必须在程式中避免。考虑到巨大的损失，在许多情况下，这个问题是很显而易见的（至少，效能分析会显示程式位置），但有个使用现代硬体的陷阱。图 6.11 显示了当程式执行在一台单处理器、四核心的机器上（Intel Core 2 QX 6700）的等价量测。即使使用这个处理器的两个个别的 L2，测试案例也没有显示出任何可延展性的问题。当相同的快取行被使用超过一次时有些许的间接成本，但它并没有随著核心的数量增加。^36若是用了多于一个这种处理器，我们自然会看到类似于那些在图 6.10 中的结果。儘管越来越多多核处理器的使用，许多机器还是会继续使用多处理器。因此，正确的处理这种状况是很重要的，这可能意味著要在真实的 SMP 机器上测试程式。

有个针对这个问题的非常简单的「修正」：将每个变数摆在它们自己的快取行。这是与先前提到的发挥作用的最佳化的衝突之处，具体来说就是应用程式的记忆体使用量会增加许多。这是不能忍受的；因此有必要想出一个更聪明的解法。

需要确定哪些变数一次只会被唯一一条执行绪使用到，始终只有一条执行绪使用的那些变数、也可能是那些不时会被争夺的变数。针对这些情况的每一个的不同解法是可能而且有用的。以变数的区分来说，最基本的标准是：它们是否曾被写入过、以及这有多常发生。

不曾被写入、以及那些仅会被初始化一次的变数基本上是常数（constant）。由于仅有写入操作需要 RFO 讯息，因此能够被在快取中共享常数（「S」状态）。所以，不必特别处理这些变数；将它们归在一起很好。若是程式设计师正确地以 const 标记这些变数，工具链将会把这些变数从普通的变数移出到 .rodata（唯读资料）或 .data.rel.ro（重定位〔relocation〕后唯读） 资料段（section）。不需其他特别的行为。若是出于某些理由，变数无法正确地以 const 标记，程式设计师能够藉由将它们指派到一个特殊的资料段来影响它们的摆放。

当连结器构造出最后的二元档时，它首先会附加来自所有输入档、具有相同名称的资料段；那些资料段接著会以连结器脚本所决定的顺序排列。这表示，藉由将所有基本上为常数、但没被这样标记的变数移到一个特殊的资料段，程式设计师便能够将那些变数全部群组在一起。它们之中不会有个经常被写入的变数。藉由适当地对齐在这个资料段中的第一个变数，就可能保证不会发生假共享。假定这个小例子：

```text
int foo = 1;
int bar __attribute__((section(".data.ro"))) = 2;
int baz = 3;
int xyzzy __attribute__((section(".data.ro"))) = 4;
```

假如被编译的话，这个输入档定义了四个变数。有趣的部分是，变数 foo 与 baz、以及 bar 与 xyzzy 被各自群组在一起。少了那个属性定义，编译器就会以原始码中定义的顺序将四个变数全都分配在一个叫做 .data 的资料段中。使用现有这段程式，变数 bar 与 xyzzy 会被放置在一个叫做 .data.ro 的资料段中。将这个资料段叫做 .data.ro 或多或少有些随意。一个 .data. 的前缀保证 GNU 连结器会将这个资料段与其它资料段摆在一起。

相同的技术能被用于分离出主要是读取、但偶尔也会被写入的变数。只要选择一个不同的资料段名称就可以了。在某些像是 Linux 系统核心的情况中，这种分离看起来很合理。

若是一个变数永远仅会被一条执行绪用到的话，有另一个指定变数的方式。在这种情况下，使用执行绪区域变数（thread-local variable）是可能而且有用的（见 \[8\]）。gcc 中的 C 与 C++ 语言允许使用 \_\_thread 关键字将变数定义为各条执行绪的。

```text
int foo = 1;
__thread int bar = 2;
int baz = 3;
__thread int xyzzy = 4;
```

变数 bar 与 xyzzy 并非被分配在普通的资料段中；而是每条执行绪拥有它自己的、储存这种变数的分离区域。这些变数能够拥有静态初始子（static initializer）。所有执行绪区域变数都能够被所有其它的执行绪定址，但除非一条执行绪将执行绪区域变数的指标传递给那些其它的执行绪，其它执行绪也没法找到这个变数。由于变数为执行绪区域的，假共享就不是个问题––除非程式人为地造成问题。这个解法很容易设置（编译器与连结器做了所有的事），但它有它的成本。当建立执行绪时，它必须花上一些时间来设置执行绪区域变数，这需要时间与记忆体。此外，定址执行绪区域变数通常比使用全域或自动变数更昂贵（如何自动地将成本最小化––如果可能的话––的解释见 \[8\]）。

另一个使用执行绪区域储存区（thread-local storage，TLS）的缺点是，假如变数的使用转移给另一条执行绪，在旧执行绪的当前值是无法被新执行绪取得的。每条执行绪的变数副本都是不同的。通常这根本不是问题，但假如是的话，转移到新的执行绪就需要协调，能够在这个时刻複製当前值。

一个更大的问题是可能浪费资源。假如在任何时候都仅有一条执行绪会使用这个变数，所有执行绪都必须付出记忆体的代价。若是一条执行绪不使用任何 TLS 变数的话，TLS 记忆体区域的惰性分配（lazy allocation）会防止它成为问题（除了在应用程式本身的 TLS）。若是一条执行绪仅在 DSO 中使用了一个 TLS 变数，所有在这个物件中的其它 TLS 变数也都会被分配记忆体。假如大规模地使用 TLS，这可能会潜在地累加。

一般来说，可以给出的最好的建议是

1. 至少分离唯读（初始化之后）与读写变数。可能将这种分离扩展到，以主要是读取的变数作为第三种类别。
2. 将一起用到的读写变数一起群组在一个结构中。使用结构，是确保在某种程度上，被所有 gcc 版本一致翻译成，所有那些变数的记忆体区域都紧靠在一起的唯一方法。
3. 将经常被不同执行绪写入的读写变数移到它们自己的快取行。这可能代表要在末端加上填充，以填满快取行的剩馀部分。若是结合步骤 2，这经常不是真的浪费。扩展上面的例子，我们可能会产生下列程式（假定 bar 与 xyzzy 要一起使用）：

   ```text
   int foo = 1;
   int baz = 3;
   struct {
     struct al1 {
       int bar;
       int xyzzy;
     };
     char pad[CLSIZE sizeof(struct al1)];
   } rwstruct __attribute__((aligned(CLSIZE))) =
     { { .bar = 2, .xyzzy = 4 } };
   ```

   某些程式的改变是必要的（bar 的参考必须被取代为 rwstruct.bar，xyzzy 亦同），但就这样了。编译器与连结器会做完剩下的事情。

4. 若是一个变数被多条执行绪用到，但每次使用都是独立的，则将变数移入 TLS。

\[^37\]: 資料段，由它們的名字所識別，為一個 ELF 檔案中包含程式與資料的原子單元。

\[^38\]: 這並不受 ISO C 標準保證，但 gcc 是這麼做的。

\[^39\]: 到目前為止，這段程式都必須在命令列以 `-fms-extensions` 編譯。

### 6.4.2 原子性最佳化

假如多条执行绪同时修改了相同的记忆体位置，处理器并不保证任何具体的结果。这是个为了避免在所有情况的 99.999% 中的不必要成本而做出的慎重决定。举例来说，若有个在「S」状态的记忆体位置、并且有两条执行绪同时必须增加它的值的时候，在从快取读出旧值以执行加法之前，执行管线不必等待快取行变为「E」状态。而是会读取当前快取中的值，并且一旦快取行变为「E」状态，新的值便会被写回去。若是在两条执行绪中的两次快取读取同时发生，结果并不如预期；其中一个加法会没有效果。

对于可能发生并行操作的情况，处理器提供了原子操作。举例来说，这些原子操作可能在直到能以像是原子地对记忆体位置进行加法的方式执行加法之前，不会读取旧值。除了等待其它核心与处理器之外，某些处理器甚至会将特定位址的原子操作发给在主机板上的其它装置。这全都会令原子操作变慢。

处理器厂商决定提供不同的一组原子操作。早期的 RISC 处理器，与代表简化（reduced）的「R」相符，提供了非常少的原子操作，有时仅有一个原子的位元设置与测试。在光谱的另一端，我们有提供了大量原子操作的 x86 与 x86-64。普遍来说可用的原子操作能够归纳成四类：位元测试这些操作原子地设置或者清除一个位元，并回传一个代表位元先前是否被设置的状态。载入锁定／条件储存（Load Lock/Store Conditional，LL/SC）LL/SC 操作成对使用，其中特殊的载入指令用以开始一个事务（transaction），而最后的储存仅会在这个位置没有在这段期间内被修改的情况才会成功。储存操作指出了成功或失败，所以程式能够在必要时重複它的工作。比较并交换（Compare-and-Swap，CAS）这是个三元（ternary）操作，仅在当前值与第三个参数值相同的时候，将一个以参数提供的值写入到一个位址中（第二个参数）；原子算术这些操作仅在 x86 与 x86-64 可用，其能够在记忆体位置上执行算术与逻辑操作。这些处理器拥有对这些操作的非原子版本的支援，但 RISC 架构则否。所以，怪不得它们的可用性是有限的。

一个架构要不支援 LL/SC 指令、要不支援 CAS 指令，并不会两者都支援。两种方法基本上相同；它们能提供一样好的原子算术操作实作，但看起来 CAS 是近来偏好的方法。其它所有的操作都能够间接地以它来实作。例如，一个原子加法：

```text
int curval;
int newval;
do {
  curval = var;
  newval = curval + addend;
} while (CAS(&var, curval, newval));
```

呼叫 CAS 的结果指出了操作是否成功。若是它回传失败（非零的值），迴圈会再次执行、执行加法、并且再次尝试呼叫 CAS。这会重複到成功为止。这段程式值得注意的是，记忆体位置的位址必须以两个独立的指令来计算。对于 LL/SC，程式看起来大致相同：

```text
int curval;
int newval;
do {
  curval = LL(var);
  newval = curval + addend;
} while (SC(var, newval));
```

这裡我们必须使用一个特殊的载入指令（LL），而且我们不必将记忆体位置的当前值传递给 SC，因为处理器知道记忆体位置是否曾在这期间被修改过。

|  |  |  |
| :--- | :--- | :--- |
| 1. 做加法並讀取結果 | 2. 做加法並回傳舊值 | 3. 原子地以新值替換 |

圖 6.12：在一個迴圈中原子遞增

The big differentiators are x86 and x86-64, where we have the atomic operations and, here, it is important to select the proper atomic operation to achieve the best result. 圖 6.12 顯示了實作一個原子遞增操作的三種方法。在 x86 與 x86-64 上，三種方法全都會產生不同的程式，而在其它的架構上，程式則可能完全相同。效能的差異很大。下面的表格顯示了由四條並行的執行緒進行 1 百萬次遞增的執行時間。程式使用了 gcc 的內建函數（`__sync_*`）

| 1. Exchange Add | 2. Add Fetch | 3. CAS |
| :--- | :--- | :--- |
| 0.23s | 0.21s | 0.73s |

前两个数字很相近；我们看到回传旧值稍微快了一点点。重要的资讯在被强调的那一栏，使用 CAS 时的成本。毫不意外，它要昂贵许多。对此有诸多理由：1. 有两个记忆体操作、2. CAS 操作本身比较複杂，甚至需要条件操作、以及 3. 整个操作必须在一个迴圈中完成，以防两个同时的存取造成一次 CAS 呼叫失败。

现在读者可能会问个问题：为什麽有人会使用这种利用 CAS 的複杂、而且较长的程式？对此的回答是：複杂性通常会被隐藏。如同先前提过的，CAS 是横跨所有有趣架构的统一原子操作。所以有些人认为，以 CAS 定义所有的原子操作就足够了。这令程式更为简单。但就如数字所显示的，这绝对不是最好的结果。CAS 解法的记忆体管理的间接成本很大。下面示意了仅有两条执行绪的执行，每条在它们自己的核心上。

| 執行緒 \#1 | 執行緒 \#2 | `var` 快取狀態 |
| :--- | :--- | :--- |
| `v = var` |  | 在 Proc 1 上為「E」 |
| `n = v + 1` | `v = var` | 在 Proc 1+2 上為「S」 |
| CAS\(`var`\) | `n = v + 1` | 在 Proc 1 上為「E」 |
|  | CAS\(`var`\) | 在 Proc 2 上為「E」 |

我们看到，在这段很短的执行期间内，快取行状态至少改变了三次；两次改变为 RFO。再加上，因为第二个 CAS 会失败，所以这条执行绪必须重複整个操作。在这个操作的期间，相同的情况可能会再度发生。

相比之下，在使用原子算术操作时，处理器能够将执行加法（或者其它什麽的）所需的载入与储存操作保持在一起。能够确保同时发出的快取行请求直到原子操作完成前都会被阻挡。

因此，在范例中的每次迴圈叠代至多会产生一次 RFO 快取请求，就没有别的了。

这所有的一切都意味著，在一个能够使用原子算术与逻辑操作的层级定义机器抽象是很重要的。CAS 不该被普遍地用作统一的机制。

对于大多数处理器而言，原子操作本身一直是原子的。对于不需要原子性的情况，只有藉由提供完全独立的程式路径时，才能够避免这点。 This means more code, a conditional, and further jumps to direct execution appropriately.

对于 x86 与 x86-64，情况就不同了：相同的指令能够以原子与非原子的方式使用。为了令它们原子化，便对指令用上一个特殊的前缀：lock 前缀。假如在一个给定的情况下，原子性需求是不必要的，这为原子操作提供了避免高成本的机会。例如，在函式库中，在需要时必须一直是执行绪安全（thread-safe）的程式就能够受益于此。没有撰写程式时所需的资讯，决策能够在执行期进行。技巧是跳过 lock 前缀。这个技巧适用于 x86 与 x86-64 允许以 lock 前缀的所有指令。

```text
    cmpl $0, multiple_threads
    je   1f
    lock
1:  add  $1, some_var
```

如果这段组语程式看起来很神秘，别担心，它很简单的。第一个指令检查一个变数是否为零。非零在这个情况中表示有多于一条执行中的执行绪。若是这个值为零，第二个指令就会跳到标籤 1。否则，就执行下一个指令。这就是狡猾的部分了。若是 je 没有跳转，add 指令便会以 lock 前缀执行。否则，它会在没有 lock 前缀的情况下执行。

增加像是条件跳转这样一个潜在昂贵的操作（在分支预测错误的情况下是很昂贵的）看似事与愿违。确实可能如此：若是大多时候都有多条执行绪在执行中，效能会进一步降低，尤其在分支预测不正确的情况。但若是有许多仅有一条执行绪在使用中的情况，程式是明显比较快的。使用一个 if-then-else 构造的替代方法在两种情况下都会引入额外的非条件跳转，这可能更慢。假定一次原子操作花费大约 200 个週期，使用这个技巧（或是 if-then-else 区块）的交叉点是相当低的。这肯定是个要记在心上的技术。不幸的是，这表示无法使用 gcc 的 _\_sync_\* 内部函式。

\[^40\]: HP Parisc 仍然沒有提供更多的操作...

\[^42\]: x86 與 x86-64 上的 `CAS` 操作碼（opcode）能夠避免第二次與後續疊代中的值的載入，但在這個平台上，我們能夠用一個單一的加法操作碼、以一個較簡單的方式來撰寫原子加法。

### 6.4.3 頻寬考量

当使用多条执行绪、并且它们不会因为在不同的核心上使用相同的快取行而造成快取争夺时，仍然会有潜在的问题。每个处理器拥有连接到与这个处理器上所有核心与超执行绪共享的记忆体的最大频宽。取决于机器架构（如，图 2.1 中的那个），多个核心可能会共享连结到记忆体或北桥的相同的汇流排。

处理器核心本身即便在完美的情况下全速运转，到记忆体的连线也无法在不等待的前提下满足所有载入与储存的请求。现在，将可用的频宽进一步以核心、超执行绪、以及共享一条到北桥的连线的处理器的数量划分，平行突然变成一个大问题。有效率程式的效能可能会受限于可用的记忆体频宽。

图 3.32 显示了增加处理器的 FSB 速度能帮上大忙。这就是为什麽随著处理器核心数量的成长，我们也会看到 FSB 速度上的提升。儘管如此，若是程式使用了很大的工作集，并且被充分最佳化过了，这也永远不会足够。程式设计师必须准备好识别由有限频宽所致的问题。

现代处理器的效能量测计数器能够观察到 FSB 的争夺。在 Core 2 处理器上，NUS\_BNR\_DRV 事件计算了一颗核心因为汇流排尚未准备就绪而必须等待的週期数。这指出汇流排被重度使用，而且载入或储存至主记忆体要花费比平常还要更长的时间。Core 2 处理器支援更多事件，能够计算像 RFO 或一般的 FSB 使用率等特定的汇流排行为。在开发期间研究一个应用程式的可延展性的可能性的时候，后者可能会派上用场。若是汇流排使用率已经接近 1.0 了，可延展性的机会是最小的。

若是识别出一个频宽问题，有几件能够做到的事情。它们有时候是矛盾的，所以某些实验可能是必要的。一个解法是去买更快的电脑，假如有什麽可买的话。取得更多的 FSB 速度、更快的 RAM 模组、或许还有处理器本地的记忆体，大概––而且很可能会––有帮助。不过，这可能成本高昂。若是程式仅在一台机器（或少数几台机器）上需要，硬体的一次性开销可能会比重写程式的成本还低。不过一般来说，最好是对程式下手。

在最佳化程式码本身以避免快取错失之后，达到更好频宽使用率的唯一剩馀选项是将执行绪更妥善地放在可用的核心上。预设情况下，系统核心中的排程器会根据它自己的策略，将一条执行绪指派给一个处理器。将一条执行绪从一颗核心移到另一颗是被尽可能避免的。不过，排程器并不真的知道关于工作负载的任何事情。它能够从快取错失等收集资讯，但这在许多情况下并不是非常有帮助。

[![&#x5716; 6.13&#xFF1A;&#x6C92;&#x6548;&#x7387;&#x7684;&#x6392;&#x7A0B;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.13.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.13.png)圖 6.13：沒效率的排程

一个可能导致很大的记忆体汇流排使用率的情况，是在两条执行绪被排程在不同的处理器（或是在不同快取区域的核心）上、而且它们使用相同的资料集的时候。图 6.13 显示了这种状况。核心 1 与 3 存取了相同的资料（以相同颜色的存取指示与记忆体区域表示）。同样地，核心 2 与 4 存取了相同的资料。但执行绪被排程在不同的处理器上。这表示每次资料集都必须要从记忆体读取两次。这种状况能够被更好地处理。

[![&#x5716; 6.14&#xFF1A;&#x6709;&#x6548;&#x7387;&#x7684;&#x6392;&#x7A0B;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.14.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.14.png)圖 6.14：有效率的排程

在图 6.14 中，我们看到理想上来看应该要是怎麽样。现在被使用的总快取大小减少了，因为现在核心 1 与 2 以及 3 与 4 都在相同的资料上运作。资料集只需从记忆体读取一次。

这是个简单的例子，但透过扩充，它适用于许多情况。如同先前提过的，系统核心中的排程器对资料的使用并没有深刻的理解，所以程式设计师必须确保排程是被有效率地完成的。没有很多系统核心的介面可用于传达这个需求。事实上，只有一个：定义执行绪亲和性。

执行绪亲和性表示，将一条执行绪指派给一颗或多颗核心。排程器接著将会在决定在哪执行这条执行绪的时候，（只）在那些核心中选择。即使有其它閒置的核心，它们也不会被考虑。这听来可能像是个缺陷，但这是必须偿付的代价。假如太多执行绪排外地执行在一组核心上，剩馀的核心可能大多数都是閒置的，而除了改变亲和性之外就没什麽能做的了。预设情况下，执行绪能够执行在任一核心上。

有一些查詢與改變一條執行緒的親和性的介面：

```text
#define _GNU_SOURCE
#include <sched.h>
int sched_setaffinity(pid_t pid, size_t size,
                      const cpu_set_t *cpuset);
int sched_getaffinity(pid_t pid, size_t size,
                      cpu_set_t *cpuset);
```

这两个介面必须要被用在单执行绪的程式上。pid 引数指定了哪个行程的亲和性应该要被改变或测定。呼叫者显然需要适当的权限来做这件事。第二与第三个参数指定了核心的位元遮罩。第一个函数需要填入位元遮罩，使得它能够设定亲和性。第二个函数以选择的执行绪的排程资讯来填充位元遮罩。这些介面都被宣告在  中。

`cpu_set_t` 型別也和一些操作與使用這個型別物件的巨集一同被定義在這個標頭檔中。

```text
#define _GNU_SOURCE
#include <sched.h>
#define CPU_SETSIZE
#define CPU_SET(cpu, cpusetp)
#define CPU_CLR(cpu, cpusetp)
#define CPU_ZERO(cpusetp)
#define CPU_ISSET(cpu, cpusetp)
#define CPU_COUNT(cpusetp)
```

`CPU_SETSIZE` 指定有多少 CPU 能够以这个资料结构表示。其它三个巨集运用了 cpu\_set\_t 物件。要初始化一个物件，应该使用 CPU\_ZERO；其它两个巨集应该用以选择或取消选择个别的核心。CPU\_ISSET 测试一个指定的处理器是否为集合的一部分。CPU\_COUNT 回传集合中被选择的核心数量。cpu\_set\_t 型别为 CPU 数量的上限提供了一个合理的预设值。随著时间推移，肯定会证实它太小了；在这个时间点，这个型别将会被调整。这表示程式必须一直将这个大小放在心上。上述的便利巨集根据 cpu\_set\_t 的定义，隐式地处理了这个大小。若是需要更动态的大小管理，应该使用一组扩充的巨集：

```text
#define _GNU_SOURCE
#include <sched.h>
#define CPU_SET_S(cpu, setsize, cpusetp)
#define CPU_CLR_S(cpu, setsize, cpusetp)
#define CPU_ZERO_S(setsize, cpusetp)
#define CPU_ISSET_S(cpu, setsize, cpusetp)
#define CPU_COUNT_S(setsize, cpusetp)
```

這些介面接收一個對應於這個大小的額外參數。為了能夠分配動態大小的 CPU 集，提供了三個巨集：

```text
#define _GNU_SOURCE
#include <sched.h>
#define CPU_ALLOC_SIZE(count)
#define CPU_ALLOC(count)
#define CPU_FREE(cpuset)
```

CPU\_ALLOC\_SIZE 巨集的回传值为，必须为一个能够处理 CPU 计数的 cpu\_set\_t 结构而分配的位元组数量。为了分配这种区块，能够使用 CPU\_ALLOC 巨集。以这种方式分配的记忆体必须使用一次 CPU\_FREE 的呼叫来释放。这些巨集可能会在背后使用 malloc 与 free，但并不是非得要维持这种方式。

最後，定義了一些對 CPU 集物件的操作：

```text
#define _GNU_SOURCE
#include <sched.h>
#define CPU_EQUAL(cpuset1, cpuset2)
#define CPU_AND(destset, cpuset1, cpuset2)
#define CPU_OR(destset, cpuset1, cpuset2)
#define CPU_XOR(destset, cpuset1, cpuset2)
#define CPU_EQUAL_S(setsize, cpuset1, cpuset2)
#define CPU_AND_S(setsize, destset, cpuset1,
                  cpuset2)
#define CPU_OR_S(setsize, destset, cpuset1,
                 cpuset2)
#define CPU_XOR_S(setsize, destset, cpuset1,
                  cpuset2)
```

这两组四个巨集的集合能够检查两个集合的相等性，以及对集合执行逻辑 AND、OR、与 XOR 操作。这些操作在使用一些 libNUMA 函数（见附录 D）的时候会派上用场。

一个行程能够使用 sched\_getcpu 介面来确定它当前跑在哪个处理器上：

```text
#define _GNU_SOURCE
#include <sched.h>
int sched_getcpu(void);
```

结果为 CPU 在 CPU 集中的索引。由于排程的本质，这个数字并不总是 100% 正确。在回传结果的时间、与执行绪回到使用者层级的时间之间，执行绪可能已经被移到一个不同的 CPU 上。程式必须总是将这种不正确的可能性纳入考量。在任何情况下，更为重要的是被允许执行执行绪的那组 CPU。这个集合能够使用 sched\_getaffinity 来查询。集合会被子执行绪与行程继承。执行绪不能指望集合在生命週期中是稳定的。亲和性遮罩能够从外界设置（见上面原型中的 pid 参数）；Linux 也支援 CPU 热插拔（hot-plugging），这意味著 CPU 能够从系统中消失––因此，也能从亲和 CPU 集消失。

在多执行绪程式中，个别的执行绪并没有如 POSIX 定义的正式行程 ID，因此无法使用上面两个函数。作为替代， 宣告了四个不同的介面：

```text
#define _GNU_SOURCE
#include <pthread.h>
int pthread_setaffinity_np(pthread_t th,
                           size_t size,
                           const cpu_set_t *cpuset);
int pthread_getaffinity_np(pthread_t th,
                           size_t size,
                           cpu_set_t *cpuset);
int pthread_attr_setaffinity_np(
                           pthread_attr_t *at,
                           size_t size,
                           const cpu_set_t *cpuset);
int pthread_attr_getaffinity_np(
                           pthread_attr_t *at,
                           size_t size,
                           cpu_set_t *cpuset);
```

前两个介面基本上与我们已经看过的那两个相同，除了它们在第一个参数拿的是一个执行绪的控制柄（handle），而非一个行程 ID。这能够定址在一个行程中的个别执行绪。这也代表这些介面无法在另一个行程使用，它们完全是为了行程内部使用的。第三与第四个介面使用了一个执行绪属性。这些属性是在建立一条新的执行绪的时候使用的。藉由设置属性，一条执行绪能够在开始时就被排程在一个特定的 CPU 集合上。这麽早选择目标处理器––而非在执行绪已经启动之后––能够在许多不同层面上受益，包含（而且尤其是）记忆体分配（见 6.5 节的 NUMA）。

说到 NUMA，亲和性介面在 NUMA 程式设计中也扮演著一个重要的角色。我们不久后就会回到这个案例。

目前为止，我们已经谈过两条执行绪的工作集重叠、使得在相同核心上拥有两条执行绪是合理的情况。反之亦然。假如两条执行绪在个别的资料集上运作，将它们排程在相同的核心上可能是个问题。两条执行绪为相同的快取斗争，因而相互减少了快取的有效使用。其次，两个资料集都得被载入到相同的快取中；实际上，这增加了必须载入的资料总量，因此可用的频宽被砍半了。

在这种情况中的解法是，设置执行绪的亲和性，使得它们无法被排程在相同的核心上。这与先前的情况相反，所以在做出任何改变之前，理解试著要最佳化的情况是很重要的。

针对快取共享最佳化以最佳化频宽，实际上是将会在下一节涵盖的 NUMA 程式设计的一个面相。只要将「记忆体」的概念扩充至快取。一旦快取的层级数增加，这会变得越来越重要。基于这个理由，NUMA 支援函式库中提供了一个多核排程的解决方法。在不写死系统细节、或是鑽入 /sys 档案系统的深度的前提下，决定亲和性遮罩的方法请参阅附录 D 中的程式例子。

## 6.5 NUMA 程式設計

以 NUMA 程式设计而言，目前为止说过的有关快取最佳化的所有东西也都适用。差异仅在这个层级以下才开始。NUMA 在存取位址空间的不同部分时引入了不同的成本。使用均匀记忆体存取的话，我们能够最佳化以最小化分页错误（见 7.5 节），但这是对它而言。所有建立的分页都是均等的。

NUMA 改变了这点。存取成本可能取决于被存取的分页。存取成本的差异也增加了针对记忆体分页的局部性进行最佳化的重要性。NUMA 对于大多 SMP 机器而言都是无可避免的，因为有著 CSI 的 Intel（for x86, x86-64, and IA-64）与 AMD（for Opteron）都会使用它。随著每个处理器的核心数量增加，我们很可能会看到被使用的 SMP 系统急遽减少（至少除了资料中心与有著非常高 CPU 使用率需求的人们的办公室之外）。大多家用机器仅有一个处理器就很好了，因此没有 NUMA 的问题。但这 a\) 不代表程式设计师能够忽略 NUMA，以及 b\) 不代表没有相关的问题。

假如理解 NUMA 的一般化的话，也能快速意识到拓展至处理器快取的概念。在使用相同快取的核心上的两条执行绪，会合作得比不共享快取的核心上的执行绪还快。这不是个杜撰的状况：

* 早期的双核处理器没有 L2 共享。
* 举例来说，Intel 的 Core 2 QX 6700 与 QX 6800 四核晶片拥有两个独立的 L2 快取。
* 正如早先猜测的，由于一片晶片上的更多核心、以及统一快取的渴望，我们将会有更多层的快取。

快取形成它们自己的阶层结构；执行绪在核心上的摆放，对于许多快取的共享（或者没有）来说变得很重要。这与 NUMA 面对的问题并没有很大的不同，因此能够统一这两个概念。即使是只对非 SMP 机器感兴趣的人也该读一读本节。

在 5.3 节中，我们已经看到 Linux 系统核心提供了许多在 NUMA 程式设计中有用––而且需要––的资讯。不过，收集这些资讯并没有这麽简单。以这个目的而言，当前在 Linux 上可用的 NUMA 函式库是完全不足的。一个更为合适的版本正由本作者建造中。

现有的 NUMA 函式库，libnuma––numactl 套件（package）的一部分––并不提供对系统架构资讯的存取。它仅是一个对可用系统呼叫的包装（wrapper）、与针对常用操作的一些方便的介面。现今在 Linux 上可用的系统呼叫为：mbind选择指定记忆体分页的连结（binding）。

```text
<dt><code>set_mempolicy</code></dt>
<dd>設定預設的記憶體連結策略。</dd>

<dt><code>get_mempolicy</code></dt>
<dd>取得預設的記憶體連結策略。</dd>

<dt><code>migrate_pages</code></dt>
<dd>將一組給定節點上的一個行程的所有分頁遷移到一組不同的節點上。</dd>

<dt><code>move_pages</code></dt>
<dd>將選擇的分頁移到給定的節點、或是請求關於分頁的節點資訊。</dd>
```

这些介面被宣告在与 libnuma 函式库一起出现的  标头档中。在我们深入更多细节之前，我们必须理解记忆体策略的概念。

### 6.5.1 記憶體策略

定义一个记忆体策略背后的构想是，令现有的程式在不大幅度修改的情况下，能够在一个 NUMA 环境中适度良好地运作。策略由子行程继承，这使得我们能够使用 numactl 工具。这个工具的用途之一是能够用来以给定的策略来启动一支程式。

Linux 系统核心支援下列策略：MPOL\_BIND记忆体只会从一组给定的节点分配。假如不能做到，则分配失败。

```text
<dt><code>MPOL_PREFERRED</code></dt>
<dd>記憶體最好是從一組給定的節點分配。若是這失敗了，才考慮來自其它節點的記憶體。</dd>

<dt><code>MPOL_INTERLEAVE</code></dt>
<dd>記憶體是平等地從指定的節點分配。節點要不是針對基於 VMA 的策略，以虛擬記憶體區域中的偏移量來選擇、就是針對基於任務（task）的策略，透過自由執行的計數器來選擇。</dd>

<dt><code>MPOL_DEFAULT</code></dt>
<dd>根據記憶體區域的預設值來選擇分配方式。</dd>
```

[![&#x5716; 6.15&#xFF1A;&#x8A18;&#x61B6;&#x9AD4;&#x7B56;&#x7565;&#x968E;&#x5C64;&#x7D50;&#x69CB;](https://github.com/jason2506/cpumemory.zh-tw/raw/master/assets/figure-6.15.png)](https://github.com/jason2506/cpumemory.zh-tw/blob/master/assets/figure-6.15.png)圖 6.15：記憶體策略階層結構

这份清单似乎递迴地定义了策略。这对了一半。事实上，记忆体策略形成了一个阶层结构（见图 6.15）。若是一个位址被一个 VMA 策略所涵盖，就会使用这个策略。一种特殊的策略被用在共享的记忆体区段上。假如没有针对特定位址的策略，就会使用任务的策略。若是连这也没有，便使用系统的预设策略。

系统预设是分配请求记忆体的那条执行绪本地的记忆体。预设不会提供任务与 VMA 策略。对于一个有著多条执行绪的行程，本地节点为首先执行行程的「家」节点。上面提到的系统呼叫能够用来选择不同的策略。

### 6.5.2 指定策略

set\_mempolicy 呼叫能够用以为当前的执行绪（对系统核心来说的任务）设定任务策略。仅有当前的执行绪会受影响，而非整个行程。

```text
#include <numaif.h>
long set_mempolicy(int mode,
                   unsigned long *nodemask,
                   unsigned long maxnode);
```

mode 参数必须为前一节介绍过的其中一个 MPOL\_\* 常数。nodemask 参数指定了未来分配要使用的记忆体节点，而 maxnode 为 nodemask 中的节点（即位元）数量。若是 mode 为 MPOL\_DEFAULT，就不需要指定记忆体节点，并且会忽略 nodemask 参数。若是为 MPOL\_PREFERRED 传递了一个空指标作为 nodemask，则会选择本地节点。否则，MPOL\_PREFERRED 会使用 nodemask 中设置的位元所对应的最低的节点编号。

对于已经分配的记忆体，设定策略并没有任何影响。分页不会被自动迁移；只有未来的分配会受影响。注意到记忆体分配与预留位址空间之间的不同：一个使用 mmap 建立的位址空间区域通常不会被自动分配。在记忆体区域上首次的读取或写入操作会分配合适的分页。若是策略在存取相同位址空间区域的不同分页之间改变了，或者策略允许记忆体的分配来自不同节点，那麽一个看似均匀的位址空间区域可能会被分散在许多记忆体节点之中。

### 6.5.3 置換與策略

若是实体记忆体耗尽了，系统就必须丢弃乾淨的分页，并将髒的分页储存到置换空间（swap）中。Linux 的置换实作会在将分页写入置换空间的时候丢弃节点资讯。这表示当分页被重複使用并载入分页（page in）时，将会从头开始选择被使用的节点。执行绪的策略很可能会导致一个靠近执行中处理器的节点被选到，但这个节点可能跟先前使用的节点不同。

这种变换的关联（association）意味著节点关联无法藉由一支程式被储存为分页的一个属性。关联可能会随著时间改变。对于与其它行程共享的分页，这也可能会因为一个行程的请求而发生（见下面 mbind 的讨论）。系统核心本身能够在一个节点耗尽空间、而其它节点仍有閒置空间的时候迁移分页。 任何使用者层级程式得知的节点关联因而只能在一段很短的时间内为真。它比起纯粹的资讯，更像是一个提示。每当需要精确的消息时，应该使用 get\_mempolicy 介面（见 6.5.5 节）。

### 6.5.4 VMA 策略

要为一个位址范围设定 VMA 策略，必须使用一个不同的介面：

```text
#include <numaif.h>
long mbind(void *start, unsigned long len,
           int mode,
           unsigned long *nodemask,
           unsigned long maxnode,
           unsigned flags);
```

这个介面为位址范围 \[start, start + len\) 注册了一个新的 VMA 策略。由于记忆体管理是在分页上操作，所以起始位址必须是对齐分页的。len 值会被无条件进位至下一个分页大小。

`mode` 參數再次指定了策略；這個值必須從 6.5.1 節的清單中挑選。與使用 `set_mempolicy` 相同，`nodemask` 參數只會用在某些策略上。它的處理是一樣的。

mbind 介面的语义取决于 flags 参数的值。预设情况下，若是 flags 为零，系统呼叫会为这个位址范围设定 VMA 策略。现有的对映不受影响。假如这还不够，目前有三种修改这种行为的旗标；它们能够被单独或者一起被选择：MPOL\_MF\_STRICT假如并非所有分页都在由 nodemask 指定的节点上，对 mbind 的呼叫将会失败。在这个旗标与 MPOL\_MF\_MOVE 和／或 MPOL\_MF\_MOVEALL 一起使用的情况下，呼叫会在任何分页无法被移动的时候失败。

```text
<dt><code>MPOL_MF_MOVE</code></dt>
<dd>系統核心將會試著移動位址範圍中、任何分配在一個不在由 <code>nodemask</code> 指定的集合中的節點上的分頁。預設情況下，僅有被當前行程的分頁表專用的分頁會被移動。</dd>

<dt><code>MPOL_MF_MOVEALL</code></dt>
<dd>如同 <code>MPOL_MF_MOVE</code>，但系統核心會試著移動所有分頁，而非僅有那些獨自被當前行程的分頁表使用的分頁。這個操作具有系統層面的影響，因為它也影響了其它––可能不是由相同使用者所擁有的––行程的記憶體存取。因此 <code>MPOL_MF_MOVEALL</code> 是個特權操作（需要 <code>CAP_NICE</code> 的能力）。</dd>
```

注意到针对 MPOL\_MF\_MOVE 与 MPOL\_MF\_MOVEALL 的支援仅在 2.6.16 Linux 系统核心中才被加入。

在没有任何旗标的情况下呼叫 mbind，在任何分页真的被分配之前必须为一个新预留的位址范围指定策略的时候是最有用的。

```text
void *p = mmap(NULL, len,
               PROT_READ|PROT_WRITE,
               MAP_ANON, -1, 0);
if (p != MAP_FAILED)
  mbind(p, len, mode, nodemask, maxnode,
        0);
```

这段程式序列保留了一段 len 位元组的位址空间范围，并指定应该使用指涉了 nodemask 中的记忆体节点的策略 mode。除非 MAP\_POPULATE 旗标与 mmap 一起使用，否则在 mbind 呼叫的时候并不会分配任何记忆体，因此新的策略会套用在这个位址空间区域中的所有分页。

单独的 MPOL\_MF\_STRICT 旗标能用来确定，传给 mbind 的 start 与 len 参数所描述的位址范围中的任何分页，是否都被分配在那些由 nodemask 指定的那些节点以外的节点上。已分配的分页不会被改变。若是所有分页都被分配在指定的节点上，那麽位址空间区域的 VMA 策略会根据 mode 改变。

有时候是需要记忆体的重新平衡的。在这种情况下，可能必须将一个节点上分配的分页移到另一个节点上。以设置 MPOL\_MF\_MOVE 呼叫的 mbind 会尽最大努力来达成这点。仅有单独被行程的分页表树指涉到的分页会被考虑是否移动。可能有多个以执行绪或其他行程的形式共享部分分页表树的使用者。不可能影响碰巧映射相同资料的其它行程。这些分页并不共享分页表项目。

若是传递给 mbind 的 flags 参数中设置了 MPOL\_MF\_STRICT 与 MPOL\_MF\_MOVE 位元，系统核心会试著移动并非分配在指定节点上的所有分页。假如这无法做到，这个呼叫将会失败。这种呼叫可能有助于确定是否有个节点（或是一组节点）能够容纳所有的分页。可以连续尝试多种组合，直到找到一个合适的节点。

除非执行当前的行程是这台电脑的主要目的，否则 MPOL\_MF\_MOVEALL 的使用是较难以证明为正当的。理由是，即使是出现在多张分页表的分页也会被移动。这会轻易地以负面的方式影响其它行程。因而应该要谨慎地使用这个操作。

### 6.5.5 查詢節點資訊

`get_mempolicy` 介面能用以查詢關於一個給定位址的 NUMA 狀態的各種事實。

```text
#include <numaif.h>
long get_mempolicy(int *policy,
             const unsigned long *nmask,
             unsigned long maxnode,
             void *addr, int flags);
```

当 get\_mempolicy 以零作为 flags 参数呼叫时，关于位址 addr 的策略资讯会被储存在由 policy 指到的字组、以及由 nmask 指到的节点的位元遮罩中。若是 addr 落在一段已经被指定一个 VMA 策略的位址空间范围中，就回传关于这个策略的资讯。否则，将会回传关于任务策略或者––必要的话––系统预设策略的资讯。

若是 flags 中设置了 MPOL\_F\_NODE、并且管理 addr 的策略为 MPOL\_INTERLEAVE，那麽 policy 所指到的字组为要进行下一次分配的节点索引。这个资讯能够潜在地用来设定打算要在新分配的记忆体上运作的一条执行绪的亲和性。这可能是实现逼近的一个比较不昂贵的方法，尤其是在执行绪仍未被建立的情况。

MPOL\_F\_ADDR 旗标能用来检索另一个完全不同的资料项目。假如使用这个旗标，policy 所指到的字组为已经为包含 addr 的分页分配了记忆体的记忆体节点索引。这个资讯能用来决定可能的分页迁移、决定哪条执行绪能够最有效率地运作在记忆体位置上、还有更多更多的事情。

一条执行绪正在使用的 CPU––以及因此用到的记忆体节点––比起它的记忆体分配还要更加变化无常。在没有明确请求的情况下，记忆体分页只会在极端的情况下被移动。一条执行绪能被指派给另一个 CPU，作为重新平衡 CPU 负载的结果。关于当前 CPU 与节点的资讯可能因而仅在短期内有效。排程器会试著将执行绪维持在同一个 CPU 上，甚至可能在相同的核心上，以最小化由于冷快取（cold cache）造成的效能损失。这表示，查看当前 CPU 与节点的资讯是有用的；只要避免假设关联性不会改变。

libNUMA 提供了两个介面，以查询一段给定虚拟位址空间范围的节点资讯：

```text
#include <libNUMA.h>
int NUMA_mem_get_node_idx(void *addr);
int NUMA_mem_get_node_mask(void *addr,
                           size_t size,
                           size_t __destsize,
                           memnode_set_t *dest);
```

NUMA\_mem\_get\_node\_mask 根据管理策略，在 dest 中设置代表所有分配（或者可能分配）范围 \[addr, addr+size\) 中的分页的记忆体节点的位元。NUMA\_mem\_get\_node 只看位址 addr，并回传分配（或者可能分配）这个位址的记忆体节点的索引。这些介面比 get\_mempolicy 还容易使用，而且应该是首选。

当前正由一条执行绪使用的 CPU 能够使用 sched\_getcpu 来查询（见 6.4.3 节）。使用这个资讯，一支程式能够使用来自 libNUMA 的 NUMA\_cpu\_to\_memnode 介面来确定 CPU 本地的记忆体节点（们）：

```text
#include <libNUMA.h>
int NUMA_cpu_to_memnode(size_t cpusetsize,
                        const cpu_set_t *cpuset,
                        size_t memnodesize,
                        memnode_set_t *
                        memnodeset);
```

对这个函数的一次呼叫会设置所有对应于任何在第二个参数指到的集合中的 CPU 本地的记忆体节点的位元。就如同 CPU 资讯本身，这个资讯直到机器的配置改变（例如，CPU 被移除或新增）时才会是正确的。

memnode\_set\_t 物件中的位元能被用在像 get\_mempolicy 这种低阶函数的呼叫上。使用 libNUMA 中的其它函数会更加方便。反向映射能透过下述函数取得：

```text
#include <libNUMA.h>
int NUMA_memnode_to_cpu(size_t memnodesize,
                        const memnode_set_t *
                        memnodeset,
                        size_t cpusetsize,
                        cpu_set_t *cpuset)
```

在产生的 cpuset 中设置的位元为任何在 memnodeset 中设置的位元所对应的记忆体节点本地的那些 CPU。对于这两个介面，程式设计师都必须意识到，资讯可能随著时间改变（尤其是使用 CPU 热插拔的情况）。在许多情境中，在输入的位元集中只会设置单一个位元，但举例来说，将 sched\_getaffinity 呼叫检索到的整个 CPU 集合传递到 NUMA\_cpu\_to\_memnode，以确定哪些记忆体节点能够被执行绪直接存取到，也是有意义的。

### 6.5.6 CPU 與節點集合

藉由将程式改变为使用目前为止所描述的介面来为 SMP 与 NUMA 环境调整程式，在来源无法取得的情况下可能会极为昂贵（或者不可能）。此外，系统管理员可能想要对使用者和／或行程能够使用的资源施加限制。对于这些情境，Linux 系统核心支援了所谓的 CPU 集。这个名称有一点误导，因为记忆体节点也被涵盖其中。它们也与 cpu\_set\_t 资料型别无关。

此刻，CPU 集的介面为一个特殊的档案系统。它通常没有被挂载（mount）（至少到目前为止）。这能够使用

`mount -t cpuset none /dev/cpuset`

改变。挂载点 /dev/cpuset 在这个时间点当然必须存在。这个目录的内容为预设（根）CPU 集的描述。它起初由所有的 CPU 与所有的记忆体节点所构成。这个目录中的 cpus 档案显示了在 CPU 集中的 CPU、mems 档案显示了记忆体节点、tasks 档案显示了行程。

为了建立一个新的 CPU 集，只要在阶层结构中的某个地方建立一个新的目录。新的 CPU 集会继承来自父集合的所有设定。接著，新的 CPU 集的 CPU 与记忆体节点能够藉由将新值写到在新目录中的 cpus 与 mems 虚拟档案来更改。

若是一个行程属于一个 CPU 集，CPU 与记忆体节点的设定会被用作亲和性与记忆体策略位元遮罩的遮罩。这表示，程式无法在亲和性遮罩裡选择不在行程正在使用的 CPU 集（即，它在 tasks 档案中列出的位置）的 cpus 档案中的任何 CPU。对于记忆体策略的节点遮罩与 mems 档案也是类似的。

除非位元遮罩在遮罩后为空，否则程式不会经历任何错误，因此 CPU 集是一种控制程式执行的近乎无形的手段。这种方法在有著大量 CPU 与／或记忆体节点时是尤其有效率的。将一个行程移到一个新的 CPU 集，就跟将行程 ID 写到合适 CPU 集的 tasks 档案一样简单。

CPU 集的目录包含许多其它档案，能用来指定像是记忆体压力下、以及独占存取 CPU 与记忆体节点时的行为。感兴趣的读者请参阅系统核心原始码树中的档案 Documentation/cpusets.txt。

\[^譯註\]: 根據前後文猜測，這裡的「來源」指的應該是程式使用的 CPU 與記憶體節點。

### 6.5.7 明確的 NUMA 最佳化

假如所有节点上的所有执行绪都需要存取相同的记忆体区域时，所有的本地记忆体与亲和性规则都无法帮上忙。当然，简单地将执行绪的数量限制为直接连接到记忆体节点的处理器所能支援的数量是可能的。不过，这并没有善用 SMP NUMA 机器，因此并不是个实际的选项。

若是所需的资料是唯读的，有个简单的解法：複製（replication）。每个节点能够得到它自己的资料副本，这样就不必进行节点之间的存取了。这样做的程式码看起来可能像这样：

```text
void *local_data(void) {
  static void *data[NNODES];
  int node =
    NUMA_memnode_self_current_idx();
  if (node == -1)
    /* Cannot get node, pick one. */
    node = 0;
  if (data[node] == NULL)
    data[node] = allocate_data();
  return data[node];
}
void worker(void) {
  void *data = local_data();
  for (...)
    compute using data
}
```

在这段程式中，函数 worker 藉由一次 local\_data 的呼叫来取得一个资料的本地副本的指标来进行准备。接著它继续执行使用了这个指标的迴圈。local\_data 函数保存一个已经被分配的资料副本的列表。每个系统拥有有限的记忆体节点，所以带有各节点记忆体副本的指标的阵列大小是受限的。来自 libNUMA 的 NUMA\_memnode\_system\_count 函数回传了这个数字。若是给定节点的记忆体还没被分配给当前节点（由 data 在 NUMA\_memnode\_self\_current\_idx 呼叫所回传的索引位置的空指标来识别），就分配一个新的副本。

重要的是要意识到，如果在 getcpu 系统呼叫之后，执行绪被排程在另一个连接到不同记忆体节点的 CPU 上时，是不会发生什麽可怕的事情的。它只代表在 worker 中使用 data 变数的存取，存取了另一个记忆体节点上的记忆体。这直到 data 被重新计算为止会拖慢程式，但就这样了。系统核心总是会避免不必要的、每颗 CPU 执行伫列的重新平衡。若是这种转移发生了，通常是为了一个很好的理由，并且在不久的未来不会再次发生。

当处理中的记忆体区域是可写入的，事情会更为複杂。在这种情况下，单纯的複製是行不通的。根据具体情况，或许有一些可能的解法。

举例来说，若是可写入的记忆体区域是用来累积（accumulate）结果的，首先为结果被累积的每个记忆体节点建立一个分离的区域。接著，当这项工作完成时，所有的每节点的记忆体区域会被结合以得到全体的结果。即使运作从不真的停止，这项技术也能行得通，但中介结果是必要的。这个方法的必要条件是，结果的累积是无状态的（stateless）。即，它不会依赖先前收集起来的结果。

不过，拥有对可写入记忆体区域的直接存取总是比较好的。若是对记忆体区域的存取数量很可观，那麽强迫系统核心将处理中的记忆体分页迁移到本地节点可能是个好点子。若是存取的数量非常高，并且在不同节点上的写入并未同时发生，这可能有帮助。但要留意，系统核心无法产生奇蹟：分页迁移是一个複製操作，并且以此而论并不便宜。这个成本必须被分期偿还。

\[^43\]: 使用者層級的 `sched_getcpu` 介面是使用 `getcpu` 系統呼叫來實作的。後者不該被直接使用，並且有一個不同的介面。

### 6.5.8 利用所有頻寬

在图 5.4 中的数据显示了，当快取无效时，对远端记忆体的存取并不显著慢于对本地记忆体的存取。这表示，一支程式也许能藉著将它不必再次读取的资料写入到附属于另一个处理器的记忆体中来节省频宽。到 DRAM 模组的连线频宽与交互连线的频宽大多数是独立的，所以平行使用能提升整体效能。

这是否真的可能，取决于许多因素。必须确保快取无效，否则与远端存取相关的减慢是很显著的。另一个大问题是，远端节点是否有任何它所拥有的记忆体频宽的需求。在採用这个方法之前，必须详加检验这种可能性。理论上，使用一个处理器可用的所有频宽可能有正面影响。一个 10h Opteron 家族的处理器能够直接连接到高达四个其它的处理器。假如系统的其馀部分合作的话，利用所有这种额外频宽，也许结合合适的预取（尤其是 prefetchw），可能致使改进。

